%!TEX root = ../thesis.tex
\chapter{The existing literature \& mathematical preliminaries\label{ch:math}} 
% intro to intro
% contextual background
	% Fluid queues 
		% definition
		% applications 
		% existing results 
	% Fluid-fluid queues
		% definition 
		% basic analysis in terms of generators
		% the need for approximation and how the old work doesn't fit here
	% The DG method
		% pertition into cells
		% project on to a basis
		% problems: oscillations
		% soutions and why they dont fit
		% observation about constant-basis methods
		% this thesis asks the question: motivated by the constant basis being a probability model, can we dream up a more accurate probability model to approximate a fluid queue?
	% On the structure of the constant basis/uniformisation method (QBD)
	% least variable is PH, so what about MEs?
% structure of the thesis
	% rest of chapter 1 gives mathematical preliminaries
	% chapter 2 explains the DG method, problems/oscillations, slope limiting, and appllication to SFFMs
	% inspired by the structure of the order-1 scheme, and to solve the negativity problem, chapter 3 introduces a new approximation method; The QBD-RAP. In this chapter we derive the intuitively of the model and explain it's dynamics.
	% we then move to convergence, with chapter 4 proving a convergence of the QBD-RAP up to the first hitting time of the fluid queue on the edges of an interval. technical results and extensions are left to the appendix: certain properties of closing vectors, convergence without ephemeral phases, and certain matrix algebraic manipulations. 
	% chapter 5 stitches together the results of chapter 4 to prove a global result about convergence
	% chapter 6 investigate, numerically, the performance of the DG, DG with limiter, uniformisation and QBD-RAP schemes. 
	% chapter 7  makes concluding remarks.
% mathematical preliminaries 
	% CTMCs
	% fluid queues
	% fluid-fluid queues and operator-analytic expressions (all of them)
		% the equations of bo2014
		% meets the partition of the DG/QBD-RAP, and reconstructing the Fil sets 
    % why we cant solve the equations, what we need to approximate to solve it, i.e. B, R, then D, Psi
	% projections
	% phase-type distributions 
		% definition
		% least variable property
	% matrix exponentials
		% definition 
		% properties 
		% verification that parameters give an ME
		% CMEs
	% QBD-RAPs, then QBDs as a special case
		% orbit processes, their interpretation for PH and how they differ for MEs
	% convergence theorems 
		% DCT 
		% Continuity of Laplace transforms

		
% THE ROUGH IDEA & CONTEXT
% 	THE CONTEXT: FLUID-FLUID QUEUES
%	PDEs 
%	NON-NEGATIVITY & CONSERVATION 
% MATHEMATICAL PRELIMINARIES
\section{Stochastic processes}
% \subsubsection{Random variables}
% Let \(\mathcal S\) be a set. 

% Roughly following \cite{billingsleyconvergence}, a random variable \(Z\), is a mapping between a \emph{sample space}, \(\mathcal S\), say, to the real numbers. 


% An \emph{event} is a set \(\{\omega \in \mathcal S \mid Z(\omega)\in E\}\equiv \{\omega\in Z^{-1}(E)\}\) where \(E\subseteq \mathcal S\) and \(Z^{-1}\) is the inverse image of the mapping \(Z\). Associated with a \(Z\) is a \emph{probability measure}, \(P\), which maps events to \([0,1]\) and has the properties \(P(\emptyset)=0\), \(P(S)=1\) and if . The notation \(P(\{\omega\in Z^{-1}(E)\})\) is cumbersome, so we write \(\mathbb P(Z\in E)\) in its place. 

% The sample space, \(\mathcal S\), is the set of all possible outcomes. Associated with the random variable \(Z\) is a \emph{probability measure} which assigns 


A stochastic process is a sequence, \(\{X(t)\}_{t\in\mathcal T}\), of random variables (or random vectors) indexed by some index set \(\mathcal T\). In this thesis, in the case that the index set is \(\mathcal T=\{0,1,2,...\}\) we say that the process \(\{X(n)\}_{n\in \mathcal T}\) is a discrete-time process, and we will typically use the dummy variable \(n\) for the `time' index. When the index set is \(\mathcal T=[0,\infty)\) we say \(\{X(t)\}_{t\in\mathcal T}\) is a continuous-time process, and we will typically use the dummy variable \(t\) for the `time' index. We may omit the index set and write \(\{X(t)\}\) in place of \(\{X(t)\}_{t\in\mathcal T}\) when it is not explicitly needed, or we may write \(\{X(t)\}_{t\geq t_0}\) to mean \(\{X(t)\}_{t\in[t_0,\infty)}\). The \emph{sample space} of \(X(t)\) is the set of possible values that \(X(t)\) can take at any time \(t\in\mathcal T\).

The \emph{initial distribution}, \(\mu\), of a stochastic process is the distribution of \(X(0)\). More generally, for a random variable, \(Z\), we write \(Z\sim \nu\) when \(Z\) has the distribution given by the probability measure \(\nu\). For the probability that \(X(t)\) lies in some set \(E\subset \mathcal S\) given \(X(0)\sim \mu\), we write \(\mathbb P(X(t)\in E \mid X(0)\sim \mu)\). When \(\mu\) assigns probability 1 to a single point, \(x\in\calS\), say, we write \(\mathbb P(X(t)\in E \mid X(0)=x).\) 

A stochastic process is \emph{stationary} if
\begin{align}\label{eqn: stationary}
	&\nonumber\mathbb P(X(t_n)\in E_n,X(t_{n-1})\in E_{n-1},...,X(t_0)\in E_0) 
	\\&= \mathbb P(X(t_n+t)=i,X(t_{n-1}+t)\in E_{n-1},...,X(t_0+t)\in E_0),
\end{align}
for any \(t,t_0,...,t_n\in\mathcal T\) and any \(E_0,...,E_n\subseteq \mathcal S\), i.e.~the joint distribution~(\ref{eqn: stationary}) does not depend on \(t\).
A random variable, \(\tau\), is a \emph{stopping time} for the stochastic process \(\{X(t)\}_{t\in\mathcal T}\) if \(\tau\in\sigma(X(s), s\leq t),\) where \(\sigma(X(s), s\leq t)\) denotes the \(\sigma\)-algebra generated by \(\{X(s), s\leq t\}\).

We call \(\{X(n)\}_{n\in\{0,1,2,\dots\}}\) a \emph{discrete-time Markov chain} if \(\calS\) is countable and
\begin{align}\label{eqn: Markov property}
	\nonumber &\mathbb P(X(n+1)=j \mid X(n)=i,X(n-1)=i_{n-1},...,X(0)=i_0) 
	\\&= \mathbb P(X(n+1)=j \mid X(n)=i) 
\end{align}
for all \(n\in\{0,1,2,...\}\) and \(i,i_0,...,i_{n-1},j\in\calS,\) and refer to (\ref{eqn: Markov property}) as the Markov property. The process \(\{X(n)\}_{n\in\{0,1,2,...\}}\) is said to be time-homogeneous if 
\(\mathbb P(X(n+1)=j \mid X(n)=i)\) does not depend on \(n\). The \emph{strong Markov property} states that for each stopping time \(\tau\) of the Markov chain \(\{X(n)\}_{n\in\{0,1,2,...\}}\), on the event that \(\{\tau<\infty\}\), then 
\begin{align}\label{eqn: strong Markov property}
	\mathbb P(X(\tau+1)=j \mid \sigma(X(0),X(1),...,X(\tau))) = \mathbb P(X(\tau+1)=j \mid X(\tau)).
\end{align}

We call \(\{X(t)\}_{t\geq 0}\) a \emph{continuous-time Markov chain} if \(\calS\) is countable and for all \(t_{n+1} > t_n > t_{n-1} > ... > t_0 > 0\) and \(i,i_0,...,i_{n-1},j\in\calS,\)
\begin{align}\label{eqn: Markov property cmtc}
	\mathbb P(X(t_{n+1})=j \mid X(n)=i, X(n-1)=i_{n-1},...,X(0)=i_0) = \mathbb P(X(t_{n+1})=j \mid X(t_n)=i),
\end{align}
and refer to (\ref{eqn: Markov property cmtc}) as the \emph{Markov property}.
The process \(\{X(t)\}_{t\geq 0}\) is said to be time-homogeneous if 
\(\mathbb P(X(t+s)=j \mid X(s)=i)\) does not depend on \(s\). The \emph{strong Markov property} states that for each stopping time \(\tau\) of the Markov chain \(\{X(t)\}_{t\geq 0}\), on the event that \(\{\tau<\infty\}\), then 
\begin{align}\label{eqn: strong Markov property ctmc}
	\mathbb P(X(\tau + t)=j \mid \sigma(X(s),s\leq \tau)) = \mathbb P(X(\tau+t)=j \mid X(\tau)).
\end{align}

Most generally, we call \(\{X(t)\}_{t\geq 0}\) a \emph{Markov process} if for all \(t_n > t_{n-1} > ... > t_0 > 0,\, t_k\in\mathcal T,\, k=0,...,n+1,\) and \(E_0,...,E_n\subseteq \mathcal S\)
\begin{align}\label{eqn: Markov process}
	\mathbb P(X(t_n)\in E_n,X(t_{n-1})\in E_{n-1},...,X(t_0)\in E_0) 
	= \mathbb P(X(t_n)\in E_n\mid X(t_{n-1})),
\end{align}
and is time homogeneous if \(\mathbb P(X(t+s)\in E_n\mid X(s))\) does not depend on \(s\). 

The Chapman-Kolmogorov equations state that 
\begin{align}\label{eqn: CK}
	&\nonumber \mathbb P(X(t+s)\in E\mid X(0)\sim \mu) 
	\\&= \int_{x\in\calS}\mathbb P(X(t+s)\in E\mid X(t)=x)\mathbb P(X(t)\in \wrt x\mid X(0)\sim \mu).
\end{align}
% Markov processes, time-homogeneity, strong Markov prperty, partitioning and LOTP

\section{Some semigroup theory}
The evolution of Markov processes can be described by an \emph{operator semigroup}. Semigroups also arise in other contexts related to the analysis of fluid queues and fluid-fluid queues (see, for example, Sections~\ref{sec: ffq intro} and~\ref{sec: transient ffq intro}). Semigroups (and therefore Markov processes) can be characterised by their \emph{infinitesimal generator}, or generator for short. One of the aims of this thesis is to approximate the generator of a fluid queue. Here we very briefly introduce semigroups and their \emph{infinitesimal generators}. 

\subsubsection{Operators}
Let \(L\) be a Banach space (a vector space with a metric and where Cauchy sequences converge to a point within the space). An operator is a mapping which takes elements of a vector space and sends them to elements of a vector space (not necessarily the same space). That is, let \(V\) and \(W\) be normed vector spaces, then \(\mathbb B:\mathcal D(\mathbb B)\subset V\to W\) is an operator. The set \(\mathcal D(\mathbb B)\) is the domain of the operator, and is not necessarily all of \(V\). When specification of the domain is unnecessary we sometimes write \(\mathbb B:V\to W\). Let \(c\in \mathbb R\) and \(v\in V\), \(w\in W\). Then \(\mathbb B\) is a \emph{linear} operator if \(\mathbb B(cv) = c(\mathbb Bv)\) and \(\mathbb B(v+w)=\mathbb Bv+\mathbb Bw\). We define the norm of an operator as \(||\mathbb B||=\sup_{v\in V}\cfrac{||\mathbb Bv||_W}{||v||_V}\), where \(||\cdot||_V\) and \(||\cdot||_W\) are the norms attached to \(V\) and \(W\), respectively.  An operator is said to be bounded if \(||\mathbb B||<M\) for some \(M<\infty, M\in \mathbb R\), for every \(v\in V\). 

% For example, let \(L\) be the space of real, smooth functions for which \(f(x)\to 0\) as \(x\to \infty\), and consider the sup norm, \(||f||=\sup\limits_{x\in\mathbb R}f(x)\). Then differentiation is an operator which maps \(L\) to it self. That is \(B=\cfrac{\wrt}{\wrt x}:L\to L\). In addition \(B\) is linear since for any \(f,g\in L\), \(B(f+g) = \cfrac{\wrt}{\wrt x}(f(x)+g(x)) =  \cfrac{\wrt}{\wrt x}f(x) +  \cfrac{\wrt}{\wrt x}g(x)\) and \(B(cf)= \cfrac{\wrt}{\wrt x}cf(x)=c \cfrac{\wrt}{\wrt x}f(x)\). The operator \(B\) is, however, not bounded.

% The \emph{graph} of \(\mathbb B\) is defined as the linear subspace of \(L\times L\), \(\mathcal G(B)=\{(f,Bf):f\in \mathcal D(B)\}\subset V\times W\}\). The operator \(B\) is said to be closed if \(\mathcal G(B)\) is a closed subset of \(V\times W\). That is, for every sequence \(\{f_n\}\) in \(\mathcal D(B)\) such that \(f_n\to f\) and \(Bf_n=g_n\to g \), then \(f\in\mathcal D(B)\) and \(Bf = g\). Note that we must have both \(\{f_n\}\) and \(\{g_n\}\) converge. 

% For example, let \(B=\cfrac{\wrt}{\wrt x}:C^1[0,1]\subset C^0[0,1]\to C^0[0,1]\), where \(C^n[0,1]\) is the set of all \(n\)-times continuously differentiable functions on the interval \([0,1]\). The graph of \(B\) is the set \(G(B)=\{(f,Bf); f\in C^1[0,1]\}\). The operator \(B\) is closed since for any \(f_n\in C^1[0,1]\) with \(f_n\to f\) and \(Bf_n = \cfrac{\wrt f_n}{\wrt x}=g_n\to g\), then \(f\in C^1[0,1]=\mathcal D(B)\) and \(Bf=g\). Notice that we can construct \(\{f_n\}\) such that \(f_n\to f\notin \mathcal D(B)\) (e.g.~from the Weierstrass Approximation Theorem there is a sequence of polynomials in \(C^1[0,1]\) converging to \(f(x)=|x-1/2|\notin C^1[0,1]\)) but in these cases \(Bf_n = \cfrac{\wrt f_n}{\wrt x}=g_n\) must not converge. 

\subsubsection{Semigroups}  A family of linear operators is a collection of indexed linear operators \(\{\mathbb B(i)\}_{i\in I}\) where \(I\) is some index set. That is, for each \(i\in I\), \(\mathbb B(i)\) is a linear operator. A family of operators \(\{\mathbb T(t)\}_{t\geq 0}\), \(\mathbb T(t):L\to L\), is said to have the \emph{semigroup} property if \(\mathbb T(t+s)=\mathbb T(t)\mathbb T(s)\). If \(\mathbb T(0)=I\), the identity operator, then the family \(\{\mathbb T(t)_{t\geq 0}\) is known as a semigroup. If \(\mathbb T(t)f\to f\) as \(t\to 0^+\), then \(\{\mathbb T(t)\}_{t\geq 0}\) is said to be strongly continuous. If \(||\mathbb T(t)||\leq 1\) for all \(t\geq 0\) then \(\{\mathbb T(t)\}_{t\geq 0}\) is known as a contraction semigroup.

For example, for a bounded linear operator \(\mathbb B\), define the operator exponential as \(e^{\mathbb Bt}:=\sum_{k=0}^\infty\cfrac{1}{k!}t^k\mathbb B^k\), then it can be shown that \(e^{\mathbb B(t+s)}=e^{\mathbb Bt}e^{\mathbb Bs}\), and from the definition \(e^{\mathbb B0}=I\), hence \(\{e^{\mathbb Bt}:t\geq 0\}\) is a semigroup. As another example, let \(\mathbb B=\cfrac{d}{dx}:C^\omega \to C^\omega(\mathbb R)\), where \(C^\omega(\mathbb R)\) is the class of analytic functions on the whole of \(\mathbb R\). Then \(\sum_{k=0}^\infty \cfrac{1}{k!}t^k\cfrac{\wrt^k}{\wrt x^k}f(x)=f(x-t)\), and the sum converges since \(f\in C^\omega(\mathbb R)\); the series representation is just the Taylor series of \(f\) about the point \(x\). 

\subsubsection{Infinitesimal generators} The \emph{infinitesimal generator} (or just \emph{generator} for short) of a semigroup \(\{\mathbb T(t)\}\) is the linear operator defined by 
\[\mathbb Bf = \lim\limits_{t\to 0^+}\frac{1}{t}(\mathbb T(t)f-I),\]
and the domain, \(\mathcal D(\mathbb B)\), is the subspace of \(L\) for which this limit exists. Note that an essential part of the definition of the generator is its domain. For this reason it is common to denote the generator as the pair \((\mathbb B,\mathcal D(\mathbb B))\). 

To determine the generator \(\mathbb B\) we can proceed by either differentiation of the semigroup, 
\[\mathbb Bf = \lim\limits_{t\to 0^+}\frac{1}{t}(\mathbb T(t)f-I),\]
or integration;
\[\int_{t=0}^\infty e^{-st}\mathbb T(t)\wrt t = (sI-\mathbb B)^{-1}=:R_s.\]
The operator \(R_s\) is known as the resolvent. 

One of the fundamental results of semigroup theory is the Hille-Yosida Theorem which essentially states that a semigroup is entirely characterised by its generator. This is not immediately obvious as the generator is defined on a subset of \(L\) only, whereas \(\{\mathbb T(t)\}\) is defined on all of \(L\).

% \section{Markov processes and semigroups}
% Semigroups arise naturally in Markov processes. Let \(\{X(t):t\geq 0\}\) be a Markov process with state space \(\mathcal S\). The Markov property states that for \(s,t\geq 0\), \(X(t+s)\) is independent of \(X(u),\, 0\leq u<t\) given \(X(t)\). That is, \(P(X(t+s)\in\mathcal{E} \mid X(t), X(u), 0\leq u<t)=P(X(t+s)\in\mathcal{E} \mid X(t))\). If \(P(X(t+s)\in\mathcal{E} \mid X(t))=P(X(s)\in\mathcal{E} \mid X(0))\), that is the distributions are invariant under translation by \(t\), then \(X(t)\) is said to be time-homogeneous. Where necessary, we will use the notation \(X_\mu(t)\) to denote the Markov process with initial distribution \(\mu\), i.e.~\(P(X(0)\in\cdot)=\mu(\cdot)\), and \(X_x(t)\) to be the Markov process with initial distribution \(\delta(x)\). Markov processes can be characterised by \emph{probability kernels}.
 
% Let \((U,\mathcal U)\) and \((V,\mathcal V)\) be two measure spaces, then a probability kernel \(\nu\), is a mapping \(\nu:U\times \mathcal V\to \mathbb R_+\) such that \(\nu(u,\mathcal{E})\) is \(\mathcal U\)-measurable in \(u\in U\) for fixed \(\mathcal{E}\in \mathcal V\), and a probability measure in \(\mathcal{E}\in \mathcal V\) for fixed \(u\in U\). We say that \(\nu\) is a probability kernel from \(U\) to \(V\). 

% With a probability kernel comes associated integral operators. For a measure \(\mu\) on \((U,\mathcal U)\), then 
% \[\mu \nu (\mathcal{E})=\int_{U} \mu(\wrt x) \nu(x,\mathcal{E})\]
% is also a measure on \((U,\mathcal U)\). For a function \(f:V\to \mathbb R\) then 
% \[\nu f (x)=\int_{V}  \nu(x,\wrt y)f(y).\] 
% We can also compose two transition kernels. Let \(\nu\) be a transition kernel from \(U\) to \(V\) and \(\eta\) be a transition kernel from \(V\) to \(W\). Then \(\nu\eta\) is also a probability kernel from \(U\) to \(W\);
% \[\nu\eta(x,\mathcal{E}) = \int_{V}\nu(x,\wrt y)\eta(y,\mathcal{E}).\]
 
% Let \(\mu_{t}(\cdot)=P(X(t)\in \cdot)\) for \(t\geq 0\) be a one-parameter family of measures on \((\mathcal S,\mathcal F)\), and \(\nu_{t}(x,\cdot)=P(X(t)\in\cdot \mid X(0)=x)\) for \(t\geq 0\) be a one-parameter family of probability kernels from \(\mathcal S\) to \(\mathcal S\). Then \(\mu_t\) and \(\nu_{t}\) characterise the time-homogeneous Markov process \(X(t)\). The probability kernel \(\nu_t\) is referred to as a transition kernel. The kernels \(\nu_t\) form a semigroup since, using the law of total probability and time-homogeneity, 
% \begin{align*}
% \nu_{t+s}(x,\mathcal{E})&=P(X(t+s)\in\mathcal{E}\mid X(0)=x)
% \\&=\int_{y\in\mathcal S}P(X(t)\in\wrt y\mid X(0)=x)P(X(t+s)\in\mathcal{E}\mid X(t)=y)
% \\&=\int_{y\in\mathcal S}P(X(t)\in\wrt y\mid X(0)=x)P(X(s)\in\mathcal{E}\mid X(0)=y)
% \\&=\int_{y\in\mathcal S}\nu_t(x,\wrt y)\nu_s(y,\mathcal{E})=\nu_t\nu_s(x,\mathcal{E}).
% \end{align*}

% We can also construct other semigroups from \(\nu_t\). First we need dome more definitions. Let \((\mathcal S,\mathcal F)\) be a measure space. A \emph{Markov operator on measures} is an operator \(M\) which takes finite measures on \((\mathcal S,\mathcal F)\) to finite measures on \((\mathcal S,\mathcal F)\) such that \(M\) is linear and for a finite measure \(\mu\), \(\mu M(\mathcal S)=\mu (\mathcal S)\). We also have the following. Let \((\mathcal S,\mathcal F, \mu)\) be a measure space and let \(L_1\) be the class of all \(\mu\)-integrable generalised functions on \(\mathcal S\). A linear operator \(P:L_1\to L_1\) is a \emph{Markov operator on densities} when; if \(f\geq 0\) almost everywhere with respect to \(\mu\), then \(fP \geq 0\) almost everywhere with respect to \(\mu\); and if \(f\geq 0\) almost everywhere with respect to \(\mu\), then \(||fP|| = ||f||\). 
% \begin{lem}
% For a Markov operator \(M\) and measure \(\mu <<\lambda\) (absolutely continuous with respect to Lebesgue measure), if \(\mu M <<\lambda\), then \(M\) induces an almost-unique Markov operator on densities \(P\) with respect to \(\mu\). 
% \end{lem}
% Let \((\mathcal S,\mathcal F)\) be a measure space and let \(B(\mathcal S)\) be the class of bounded measurable functions. An operator \(K:B(\mathcal S)\to B(\mathcal S)\) is a \emph{transition operator} when \(K\) is linear and for \(f\), \(\{f_n\}\in B(\mathcal S)\), if \(f\geq 0\) almost everywhere with respect to a measure \(\mu\), \(Kf\geq 0\) almost everywhere with respect to \(\mu\), \(K1(\mathcal S)=1(\mathcal S)\) and if \(f_n\to 0^+\), then \(Kf_n\to 0^+\). 
% \begin{lem}
% Every transition probability kernel \(\nu\) induces a Markov operator on measures, \(\mu M(\mathcal{E}) = \mu \nu(\mathcal{E})\), and a transition operator on functions, \(Kf(x)=\nu f(x)\). In the other direction, every Markov operator induces a probability kernel \(\delta(x)M(\mathcal{E})=\nu(x,\mathcal{E})\), and every transition operator induces a transition kernel \(\nu(x,\mathcal{E})=K1(\mathcal{E})(x)\).
% \end{lem}

% We can now use Markov operators and transition operators to construct semigroups from the transition kernel semigroup. A transition semigroup is the semigroup \(K(t)f(x) = \nu_tf(x)\). Similarly we can construct the Markov semigroup, \(\mu M(t)(\mathcal{E}) = \mu \nu_t (\mathcal{E})\) for measures and \(fM(t)(\wrt y) = f\nu_t (\wrt y)\) for densities. 

% \subsubsection{Feller processes}
% Feller processes are Markov processes with particularly nice transition semigroups. A continuous-time homogeneous Markov process \(\{X(t):t\geq 0\}\) is a Feller process when, for all \(x\in\mathcal S\) 
% \begin{align}
% y\to x&\implies X_y(t)\xrightarrow[]{d} X_x(t),\, \forall t\geq 0,
% \\t \to 0 &\implies X_x(t)\xrightarrow[]{P}x, 
% \end{align}
% where \(\xrightarrow[]{d}\) denotes convergence in distribution and \(\xrightarrow[]{P}\) denotes convergence in probability. It can be shown that these probabilistic properties are equivalent to conditions on the transition semigroup. A semigroup of linear, positive (maps positive functions to positive functions), conservative (\(K(t)1(\mathcal S) = 1(\mathcal S)\)), contractions, is a Feller semigroup if for every \(f\in C_0\) (continuous function for which \(|x|\to \infty  \implies f(x)\to 0\)), and every \(x\in \mathcal S\), 
% \begin{align*}
% K(t)f&\in C_0,\\
% \lim_{t\to 0^+}K(t)f(x) &= f(x).
% \end{align*}
% \begin{thm}
% A Markov process is a Feller process if and only if its evolution operators form a Feller semigroup. 
% \end{thm}

% It is a fact that transition operators and Markov operators on densities are \emph{adjoint} operators. Therefore in many cases we can work with transition operators and carry the results over to Markov operators on densities, or \emph{vice-versa}.

% \subsubsection{A long story short}
% A time homogeneous Markov processes can be defined by a transition kernel, \(\nu_t(x,\mathcal{E})=P(X(t)\in\mathcal{E}\mid X(0)=x)\) and an initial distribution \(\mu(\mathcal{E})=P(X(0)\in\mathcal{E})\). The operators \(\nu_t\) form a semigroup due to the Markov property and time homogeneity. We can construct other semigroups from \(\nu_t\) by either integrating measures on the left, \(\mu\nu_t(\mathcal{E})=:\mu M(t)(\mathcal{E})\) (which we will call Markov semigroups), or by integrating functions on the right, \(\nu_tf(x) =: K(t)f(x)\). The latter is sometimes known as the transition semigroup, \(K(t):L\to L\) where \(L\) is a Banach space. These semigroup operators are adjoint, so results from transition semigroups may be able to be translated to results for Markov semigroups. 

% Feller processes are Markov processes with nice properties. It can be shown that a process is a Feller process if and only if its transition semigroup is a Feller semigroup. A Feller semigroup is a semigroup which maps continuous functions to continuous functions and is strongly continuous, \(\lim\limits_{t\to 0^+}K(t)f(x)=f(x)\).

% Under certain conditions, a semigroup, (and hence also a Feller process), can be characterised by its generator, \(B\) defined by;
% \[Bf:=\lim_{t\to 0^+} \frac{1}{t}(K(t)-I)f,\]
% on the set of functions \(f\) for which this limit exists. This set is known as the domain of the generator \(\mathcal D(B)\), which is not necessarily all of \(L\). The domain of \(B\) is a very important part of its definition, and some authors highlight this by writing the generator as the pair \((B,\mathcal D(B))\). 

% On \(\mathcal D\left(\bigcup\limits_{n=0}^\infty B^n\right)\) the semigroup with generator \(B\) is given by the operator exponential 
% \[e^{Bt}=\sum_{k=0}^\infty \frac{t^kB^k}{k!} \mbox{ for }t\geq 0.\]

% It may be surprising that the semigroup \(K(t)\), which is defined on all of \(L\), can be characterised entirely in term of its generator \(B\), which is only defined on \(\mathcal D(B)\) and furthermore, only defines \(K(t)\) in terms of an operator exponential on \(\mathcal D\left(\bigcup\limits_{n=0}^\infty B^n\right)\). A step in the direction of characterisation of \(K(t)\) in terms of its generator is the fact that, when \(\{K(t)\}\) is strongly continuous, \(\mathcal D(B)\) is dense in \(L\).

% The Hille-Yosida Theorem gives necessary and sufficient conditions for \((B,\mathcal D(B))\) to be the generator of a strongly continuous semigroup. A key element in the proof of the Hille-Yosida Theorem is the Yosida approximation given by 
% \[B_\lambda = \lambda B(\lambda - B)^{-1},\]
% where \((\lambda - B)^{-1}\) is the resolvent of \(B\). It can be shown that the domain of \((\lambda - B)^{-1}=L\) for every \(\lambda >0\), that \(\{e^{tB_\lambda}\}\) is a strongly continuous semigroup of \(L\), and that \(B_\lambda \to B\) as \(\lambda \to \infty\). Hille and Yosida were able to use these facts to determine necessary and sufficient conditions in term of \(B\), for \((B,\mathcal D(B))\) to generate a strongly continuous semigroup. Furthermore, since \(R_\lambda\) is one-to-one with domain \(L\) and range \(\mathcal D(B)\), then there is a correspondence between elements in \(\mathcal D(B)\) and \(L\). That is, we can represent any element of \(\mathcal D(B)\) as \(R_\lambda f\) for some \(f\in L\). This can be useful to characterise the domain, \(\mathcal D(B)\).

\section{Fluid queues}\label{sec: fqs}
An unbounded fluid queue is a two-dimensional stochastic process which we denote by \(\{\ddot{\bs X}(t)\} = \{(\ddot X(t),\varphi(t))\}_{t\geq0}\) where \(\{\varphi(t)\}_{t\geq0}\) is known as the phase or driving process, and \(\{\ddot X(t)\}_{t\geq0}\) is known as the level process or buffer. The phase process \(\{\varphi(t)\}_{t\geq0}\), is an irreducible continuous-time Markov chain (CTMC) with finite state space, which we assume to be \(\mathcal S=\{1,2,\dots,N\}\), and infinitesimal generator \(\bs T= [T_{ij}]_{i,j\in\mathcal S}\). We assume that \(\bs T\) is \emph{conservative}. Associated with states \(i\in\mathcal S\) are real-valued \emph{rates} \(c_i\in\mathbb R\) which determine the rate at which \(\{X(t)\}\) moves. 

Partition the state space \(\calS\) into \(\calS_+ = \{i\in\calS\mid c_i>0\}\), \(\calS_- = \{i\in\calS\mid c_i<0\}\), \(\calS_0 = \{i\in\calS\mid c_i=0\}\), \(\mathcal S_{-1}=\{i\in\calS\mid c_i\leq0\},\, \mathcal S_{K+1}=\{i\in\calS\mid c_i\geq0\}\)\footnote{The notation \(\mathcal S_{-1}\) and \(\mathcal S_{K+1}\) will make sense later when we introduce the partition into cells for the approximation schemes into \(K\) cells, plus a lower and upper boundary which we represent with indices \(-1\) and \(K+1\) respectively.}. We assume, without loss of generality, that the generator \(\bs T\) is partitioned into sub-matrices
\[\bs T = \left[\begin{array}{ccc}\bs T_{++} & \bs T_{+-} & \bs T_{+0} \\ \bs T_{-+} & \bs T_{--} & \bs T_{-0} \\ \bs T_{0+} & \bs T_{0-} & \bs T_{00}  \end{array}\right],\]
where \(\bs T_{mn} = [T_{ij}]_{i\in\mathcal S_m, j\in\mathcal S_n}\), \(m,n\in\{+,-,0\}\).\footnote{Let's clarify some notation. We use the notation \(\bs u = (u_h)_{h\in \mathcal H}\) to denote a row-vector, \(\bs u\), defined by its elements, \(u_h\), indexed by \(h\in\mathcal H\), where \(\mathcal H\) is some index set. Similarly, \(\bs u = (\bs u_h)_{h\in\mathcal H}\), is a row-vector defined by a collection of row-vectors \(\bs u_h\). The notation \(\bs u_m=(u_h)_{h\in\mathcal H_m}\) refers to the vector containing the subset of elements corresponding to \(\mathcal H_m\subseteq \mathcal H\). When the index set is empty, the resulting vector \(\bs u_m\) is a vector of dimension 0. In cases when there are two indices, we order the elements of the vector according to the first index, then the second; i.e.~\(\bs u = (u_{g}^h)_{g\in\mathcal G,h\in\mathcal H} = ((u_g^h)_{g\in\mathcal G})_{h\in\mathcal H}\). Here we use the convention that for a vector \(\bs u=(u)_{h\in \mathcal H}\) where the elements \(u\) do not depend on the index \(h\) and \(H\) is some index set, then we repeat \(u\) \(h\)-times; i.e.~\(\bs u = (u)_{h\in \mathcal H} =\underbrace{(u,\dots,u)}_{h-\mbox{times}}\). The notation \(\bs U = [u_{gh}]_{g\in\mathcal G, h\in\mathcal H}\) (square brackets) is used to denote a matrix defined by its elements, or sub-blocks, \(u_{gh}\).} Also define the diagonal matrices 
\begin{align*}
	\bs C &= \left[\begin{array}{ccc} \bs C_+ && \\ &\bs C_-& \\ && \bs 0\end{array}\right], && \bs C_+ = diag(c_i,i\in\calS_+), && \bs C_- = diag(|c_i|,i\in\calS_-),
\end{align*}
and 
\(
	\widehat{\bs C} = diag(c_i,i\in\calS),
\)
where \(diag(a_i,i\in\mathcal I)\) denotes a diagonal matrix with entries \(a_i\) down the diagonal. 

The level process is given by 
\[\ddot X(t) = \ddot X(0) + \int_{s=0}^t c_{\varphi(s)}\wrt s.\]
Sample paths of \(\{\ddot X (t)\}\) are continuous and piecewise linear, with \(\cfrac{\wrt }{\wrt t} \ddot X(t) = c_\varphi(t)\), when \(\ddot X(t)\) is differentiable. Given sample paths of \(\{\varphi(t)\}\), then \(\{\ddot X(t)\}\) is deterministic, and in this sense, \(\{\varphi(t)\}\) is the only stochastic element of the fluid queue. 

Often, boundary conditions are imposed. We denote a fluid queue bounded below at \(0\) and unbounded above by \(\{\dot{\bs X}(t)\} = \{(\dot X(t),\varphi(t))\}_{t\geq0}\), and a fluid queue bounded below at \(0\) and above by \(b<\infty\) by \(\{{\bs X}(t)\} = \{( X(t),\varphi(t))\}_{t\geq0}\). Here, we consider a mixture of \emph{regulated} and \emph{reflecting} boundary conditions. Upon hitting a boundary we suppose that, with probability \(p_{ij},\,i,j\in\mathcal S\), the phase process instantaneously transitions from phase \(i\) to phase \(j\) (note that we might have \(i=j\) i.e.~no transition) and if \(sign(c_i)=sign(c_j)\) or \(sign(c_j)=0\) then the process is absorbed in the boundary, otherwise it is reflected. At a lower boundary, if \(j\in\calS_0\cup\calS_-\), then \(\cfrac{\wrt}{\wrt t} X(t) = 0\), and the phase process continues to evolve according to the sub-generator 
\[\left[\begin{array}{cc} \bs T_{--} & \bs T_{-0} \\ \bs T_{0-} & \bs T_{00}  \end{array}\right],\]
until such a time that \(\{\varphi(t)\}\) transitions to a phase \(k\in\calS_+\), at which time \(\{X(t)\}\) leaves the boundary. Similarly, at an upper boundary if \(j\in\calS_0\cup\calS_+\), then \(\cfrac{\wrt}{\wrt t} X(t) = 0\) and the phase process continues to evolve according to the sub-generator 
\[\left[\begin{array}{cc} \bs T_{++} & \bs T_{+0} \\ \bs T_{0+} & \bs T_{00}  \end{array}\right],\]
until such a time that \(\{\varphi(t)\}\) transitions to a phase \(k\in\calS_-\) at which time \(\{X(t)\}\) leaves the boundary. Without loss of generality, we assume the lower and upper boundaries (when present) are at \(x=0\) and \(x=b>0\), respectively.

In summary, the evolution of the level can be expressed as 
\[\cfrac{\wrt}{\wrt t} X(t) = \begin{cases} c_{\varphi(t)}, & \mbox{ if } X(t)>0, \\ \max\{0,c_{\varphi(t)}\}, & \mbox{ if } X(t)=0, \\ \min\{0,c_{\varphi(t)}\}, & \mbox{ if } X(t)=b.  \end{cases}\]

Let \(\bs f(x,t) = (f_i(x,t))_{i\in\calS}\) be a row-vector function where \(f_i(x,t)\) is the density of \(\mathbb P(X(t)\leq x, \varphi(t) = i\mid \bs X(0)\sim \bs \mu )\), assuming it exists. When a differentiable density exists, the system of partial differential equation which describes the evolution of the densities \(\bs f(x,t)\) is 
\begin{align}
	\cfrac{\partial}{\partial t} \bs f(x,t) = \bs f(x,t)\bs T - \cfrac{\partial}{\partial x}\bs f(x,t)\widehat{\bs C},\label{eqn: pde}
\end{align}
on the interior \(x\in(0,b)\), with appropriate boundary conditions (see Section~\ref{subsec: boundary DG}). The initial condition is the initial distribution of the fluid queue which, when it has a density, we write as \(f_i(x,0)\). Often a differentiable density function does not exist and therefore the partial differential equation~(\ref{eqn: pde}) is not well-defined. For example, for a fluid queue with no upper boundary, if the initial distribution of the fluid queue is a point mass at any point \(x_0\geq 0\) and in phase \(i\in\calS_+\cup\calS_0\), then a density function \(f_i(x,t)\) will not exist for any finite \(t\). Specifically, a point mass will persist along the ray \(x_0+c_it\), \(t\geq 0\). In such situations, it is the \emph{weak solution} to (\ref{eqn: pde}) that we seek. A weak solution satisfies
\begin{align}
	-\int_{x=0}^b\int_{t=0}^\infty \bs f(x,t)\cfrac{\partial}{\partial t} \bs \psi(x,t)\wrt t \wrt x= &\int_{x=0}^b\int_{t=0}^\infty\bs f(x,t)\bs T\bs \psi(x,t)\wrt t \wrt x \nonumber 
	\\&{} + \int_{x=0}^b \int_{t=0}^\infty\bs f(x,t)\widehat{\bs C} \cfrac{\partial}{\partial x} \bs \psi(x,t)\wrt t \wrt x,\label{eqn: weak pde}
\end{align}
for every row-vector of test functions, \(\bs \psi(x,t) = ( \psi_i(x,t))_{i\in\calS}\), which are smooth, have compact support and \(\bs \psi(x,0)=\bs \psi(0,t) = \bs \psi(b,t)=\bs 0\). 

\subsection{Transient analysis of fluid queues}\label{sec: transient ffq intro}
The transient analysis of fluid queues has been relatively well studied (see, for example, \cite{ar2004,bean2005,dasilva2005,bean2009} which are matrix-analytics-methods-based, and also \cite{rs2003} which is differential-equations-based and treats a slightly more complex model than the ones considered here, but is none-the-less relevant). Given the interest in discontinuous and point-mass initial conditions, transient analysis of fluid queues rarely relies on solving governing differential equations numerically, although it is quite possible to do so\footnote{We have already alluded to some difficulties with differential-equation-based approaches to problems with discontinuous solutions such as ill-defined PDEs, the need for weak solutions, and oscillatory approximations with possibly infeasible approximate solutions, for example negative probability approximations.}. Instead, expressions for transient distributions of fluid queues are derived in terms of Laplace transforms with respect to time, and/or moments of the transient distributions are derived \citep{ar2004,bean2005,bean2009}. Moreover, these techniques often lead to quantities which have direct probabilistic interpretations which further aids in their analysis and also applications to other problems \citep{ar2003,dasilva2005,bnp2018} (we also leverage these stochastic interpretations in Chapter~\ref{sec: conv} to derive expressions for certain the Laplace transforms of the fluid queue). In some context, the actual transient distributions are not required, and Laplace transforms or moments are all that are required. In other cases, we can invert the Laplace transforms using known methods (\cite{aw2006}, or in the case of the discontinuities we might prefer \cite{hhat2020}, which uses the same concentrated matrix exponential distribution that we do).

One such analysis of fluid queues derives the Laplace transform of the time taken for the level of an unbounded fluid queue to return to its initial level in a certain phase, given it started in a phase with positive rate \citep{bean2005}. The principles underlying the derivation of this first return operator are the same as those applied in \cite{bo2014} to derive the first return operator for fluid-fluid queues. Moreover, certain matrices appearing in the analysis have a stochastic interpretation which we leverage to write down certain Laplace transforms in Chapter~\ref{sec: conv} in terms of these matrices. Given its relevance we briefly recount the analysis of \cite{bean2005} here. 

Consider an unbounded fluid queue \(\{(\ddot X(t), \varphi(t))\}\). Let \(\zeta_X(E)\) be the random variable which is the first hitting time of \(\{\ddot X(t)\}\) on the set \(E\) and define the matrix \(\bs \Psi_X(s)\) with elements \(\left[\bs \Psi_X(s)\right]_{ij}\), \(i\in\calS_+\), \(j\in\calS_-\), given by
\begin{align}
	\left[\bs \Psi_X(\lambda)\right]_{ij} = \mathbb E\left[ e^{-\zeta_X(z)\lambda} 1(\zeta_X(z)<\infty, \varphi(\zeta_X(z)=j)\mid \ddot X(0)=z, \varphi(0)=i\right].
\end{align}
\(\left[\bs \Psi_X(\lambda)\right]_{ij}\) is the Laplace-Stieltjes transform of the time taken for the fluid queue to first return to level \(z\) and do so in phase \(j\in\calS_-\), given it started at level \(z\) in phase \(i\in\calS_+\). Define the in-out fluid level by $\beta_X(t) := \int_0^t \left| c_{\varphi(z)} \right|  \wrt z$, which is the total amount of fluid to flow in to or out of the buffer \(\{\ddot X(t)\}\) by time \(t\), and also define $\eta_X(y) := \inf \{t > 0: \beta_X(t) = y\}$ as the first hitting time of the in-out process \(\{\beta_X(t)\}\) on level \(y\geq 0\). Further, let \(\bs H(\lambda,y)\) be the matrix with elements \(h_{ij}(\lambda,y)\), \(i,j\in\calS_+\cup\calS_-\), given by
\begin{align}
	\mathbb E\left[e^{-\lambda \eta_X(y)}1(\eta_X(y)<\infty,\varphi(\eta_X(y))=j)\mid \ddot X(0)=0, \varphi(0)=i\right],
\end{align}
which is the Laplace-Stieltjes transform of the time taken for \(y\) amount of fluid to flow in or out of the fluid queue and to be in phase \(j\) at this time, given the initial level of the fluid queue was \(0\) and the initial phase was \(i\). 

It turns out that, for fixed \(\lambda \geq 0\), \(\bs H (\lambda,y)\) is a semigroup (with variable \(y\)) \citep{bean2005}. \cite{bean2005} find the infinitesimal generator, \(\bs Q(\lambda)\) of \(\bs H(\lambda,y)\) and, since the generator is bounded, we can write \(\bs H(\lambda,y)=e^{\bs Q(\lambda)y}\). The derivation of the generator \(\bs Q(\lambda)=[Q_{ij}(\lambda)]_{ij\in\calS_+\cup\calS_-}\) is based on a direct analysis of sample paths of the fluid queue over an infinitesimal time, \(u\) say, and by leveraging the fact that complex sample paths occur with probability \(\mathcal O(u^2)\). As a result, there are only three types of sample paths to consider. Further, \cite{bean2005} similarly argue that the generator \(\bs Q(\lambda)\) can be partitioned into blocks \(\bs Q_{mn}(\lambda) = \left[Q_{ij}(\lambda)\right]_{i\in\calS_m,j\in\calS_n}\), \(m\in\{+,-\},\,n\in\{+,-,0\}\) where 
\begin{align*}
	\bs Q_{+0}(\lambda) &= \bs C_+^{-1}\bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1},
	%
	\\\bs Q_{-0}(\lambda) &= \bs C_-^{-1}\bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1},
	%
	\\\bs Q_{++}(\lambda) &= \bs C_+^{-1} \left(\bs T_{++} - \lambda \bs I + \bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0+}\right),
	%
	\\\bs Q_{+-}(\lambda) &= \bs C_+^{-1} \left(\bs T_{+-} + \bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0-} \right) ,
	%
	\\\bs Q_{--}(\lambda) &= \bs C_-^{-1} \left(\bs T_{--}  - \lambda \bs I + \bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0-}\right),
	%
	\\\bs Q_{-+}(\lambda) &=\bs C_-^{-1} \left(\bs T_{-+}+ \bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0+}\right).
\end{align*}
\cite{bean2005} also define the functions 
\begin{align}
	\bs H^{++}(\lambda,y)&= \left[h_{ij}^{++}(\lambda,y)\right]_{i\in \mathcal S_+,j\in\mathcal S_{+}\cup\calS_{+0}} := e^{\bs{Q}_{++}(\lambda)y}\vligne{\bs C_+^{-1} & \bs Q_{+0}(\lambda)},  \label{eqn: lst 1a}
	\\\bs H^{--}(\lambda,y) &= \left[h_{ij}^{--}(\lambda,y)\right]_{i\in\mathcal S_-,j\in\mathcal S_{-}\cup\calS_{-0}}:= e^{\bs{Q}_{--}(\lambda)y}\vligne{\bs C_-^{-1} & \bs Q_{-0}(\lambda)},
	\\\bs H^{+-}(\lambda,y)  &= \left[h_{ij}^{+-}(\lambda,y)\right]_{i\in\mathcal S_+,\,j\in\mathcal S_-}:= e^{\bs{Q}_{++}(\lambda)y}\bs{Q}_{+-}(\lambda), 
	\\\bs H^{-+}(\lambda,y)&= \left[h_{ij}^{-+}(\lambda,y)\right]_{i\in\mathcal S_-,\, j\in\mathcal S_+} := e^{\bs{Q}_{--}(\lambda)y}\bs{Q}_{-+}(\lambda), \label{eqn: lst 4a}
\end{align}
for \(y,\lambda\geq 0,\) which have the following stochastic interpretations. The function \(h_{ij}^{++}(\lambda,y)\) (\(h_{ij}^{--}(\lambda,y)\)) is the Laplace transform (with respect to time) of the time taken for the fluid level to shift by an amount \(y\) whilst remaining in phases in \(\mathcal S_+\cup\calS_{+0}\) (\(\mathcal S_-\cup\calS_{-0}\)), given the phase was initially \(i\in\mathcal S_+\) (\(i\in\mathcal S_-\)). The function \(h_{ij}^{+-}(\lambda,y)\) (\(h_{ij}^{-+}(\lambda,y)\)) is the Laplace transform (with respect to time) of the time taken for the fluid level, \(\{Y(t)\}\) to shift by an amount \(y\) whilst remaining in phases in \(\mathcal S_+\cup\calS_{+0}\) (\(\mathcal S_-\cup\calS_{-0}\)), after which time the phase instantaneously changes to \(j\in\mathcal S_-\) (\(\mathcal S_+\)), given the phase was initially \(i\in\mathcal S_+\) (\(\mathcal S_-\)) \citep{bean2005}.

Thus, \cite{bean2005} are able to characterise, by the above matrix expressions, segments of sample paths of the fluid queue where the fluid level is non-decreasing and non-increasing. \cite{bean2005} then partition the sample paths which contribute to \(\bs \Psi_X\) as those which either, (a) have a single transition from \(\calS_+\) to \(\calS_-\) (perhaps via \(\calS_0\)), and, (b) those which have more than one transition from \(\calS_+\) to \(\calS_-\) (perhaps via \(\calS_0\)). An expression for Laplace-Stieltjes transform of (a) is 
\[\int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y}\bs Q_{+-}(\lambda)e^{\bs Q_{--}y}\wrt y.\]
For the sample paths (b), there must be at least one point at which the phase process transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) before the first return time. Further, one of the transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) must occur lower than all others. By considering the lowest level, \(y\), at which a phase transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) occurs, \cite{bean2005} characterise the Laplace transform of the paths (b) by the expression 
\begin{align}
	\int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y} \bs \Psi_x(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) e^{\bs Q_{--}(\lambda)y}\wrt y.
\end{align}

Adding the expressions for (a) and (b) together, \cite{bean2005} state 
\begin{align}
	\bs \Psi_X(\lambda) = \int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y}\bs Q_{+-}(\lambda)e^{\bs Q_{--}y} + e^{\bs Q_{++}(\lambda)y} \bs \Psi_x(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) e^{\bs Q_{--}(\lambda)y}\wrt y,
\end{align}
which can be shown to be equivalent to \citep[Lemma~3 and Theorem~9.2]{br1997}
\begin{align}
	\bs Q_{++}(\lambda)\bs \Psi_X(\lambda) + \bs \Psi_X(\lambda)\bs Q_{--}(\lambda)\bs \Psi_X(\lambda) + \bs Q_{+-}(\lambda) + \bs \Psi_X(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) = \bs 0.
\end{align}

The key concepts of this argument are; (1) to partition the state space of the driving process into sets on which the fluid level is increasing, decreasing, or constant; (2) to characterise the sections of sample paths of the fluid level which are non-decreasing and non-increasing as semigroups and derive their generators; (3) to partition the sample paths which comprise \(\bs \Psi_X(\lambda)\) such that we can write down an expression for \(\bs \Psi_X(\lambda)\) in terms of the expressions derived in (2) and \(\bs \Psi_X(\lambda)\) itself and then to solve the resulting expression. 


% DO THE \(\bs Q(s)\) equation for \(\Psi(s)\); this is all possible because we have \(\bs T\) and the process can be partitioned in to up down sections. For FFQs the same can be done. The first step towards this is finding the generator of the fluid queue and partitioning it appropriately, which is what we do next. 


% fluid queues (with or without boundaries) are Feller processes, hence their associated semigroups are Feller semigroups. Let \(\boldsymbol f(x,t)\) be the density of a fluid queue at time \(t\). Denote the Markov semigroup associated with the fluid queue as \(V(t)\). Therefore, \(\boldsymbol f(x,t)\wrt x=\boldsymbol \mu V(t)(\wrt x)\), where \(\boldsymbol \mu(\wrt x)=\boldsymbol f(x,0)\wrt x\) is the initial probability density.

% Let \(L=C(\{1,2,...,N\}\times[-\infty,\infty])\) be the space of bounded continuous functions on \(\{1,2,...,N\}\times[-\infty,\infty]\) equipped with the sup norm, \(||\boldsymbol f||_\infty = \sup\limits_{x\in[-\infty,\infty],i\in\mathcal S}|f_i(x)|\). Continuity at \(\infty\) means that \(\lim\limits_{x\to \infty} f(x)=\lim\limits_{x\to -\infty} f(x)\) and the limit exists and is finite. That is, for \(\boldsymbol f\in L\), \(\boldsymbol f(x)=(f_1(x),\dots,f_N(x))\), where each \(f_i(x)\) is a bounded continuous function on the extended reals. Denote by \(V^*(t):L\to L\) the transition semigroup of a SFM: \(V^*(t)\boldsymbol f(x) = \int_{y}P(X(t)\in\wrt y, \varphi(t)=k\mid X(0)=x,\varphi(0)=i)f_j(y)\). The superscript \(^*\) alludes to the fact that \(V^*(t)\) is the adjoint of \(V(t)\). 
% \begin{lem}\label{lem: adjoint generator}
% For an unbounded SFM the generator of the semigroup \(V^*(t)\) is 
% \[A = T+\cfrac{\wrt}{\wrt x}C.\]
% \end{lem}
%The weak form, (\ref{eqn: weak pde}) is derived by multiplying (\ref{eqn: pde}) by \(\bs \psi(x,t)\), then integrating the last term by parts and assuming \( \psi(x,t)

%%
%%
% On the analysis fluid queues, MAM and why the methods dont apply here. 

\section{Fluid-fluid Queues}\label{sec: ffq intro}
	\label{sec:prelim}
	\begin{center}
		\begin{minipage}{0.8\textwidth}
			\textit{This subsection was largely taken from Sections~2 and 3 of \cite{blnos2022} with changes, such as notations, so that this chapter is consistent with the rest of the thesis. I am a co-author of the paper \cite{blnos2022}. %The conceptualisation of \cite{blnos2022} was originally by Vikram Sunkara, Nigel Bean and Giang Nguyen, and the original coding was done by Vikram Sunkara. I made significant contributions to Section~3 of the paper, expressing the operator-theoretic expressions to use the same partition as the approximation scheme. I contributed Sections~4.4 and 5.1. I extended the numerical experiments in Section~6 to higher orders and made all the plots in Section~6. Appendix~A is also my original work. I did a significant proportion of the writing of the manuscript and addressed the reviewers comments and also developed code for the numerical experiments.
			}
		\end{minipage}
		\end{center}

A stochastic fluid-fluid queue \citep{bo2013} is a Markov process with three elements, $\{( \ddot{Y}(t),{X}(t), \varphi(t))\}_{t \geq 0},$ where $\{(X(t),\varphi(t))\}_{t\geq 0}$ is a classical fluid queue\footnote{We assume \(X(t)\) is doubly-bounded as this results in a finite-dimensional approximation, but this is not necessary} and $\ddot{Y}(t)$ is the second fluid, which varies at rate $r_{\varphi(t)}(\dot{X}(t))$:
% 
\begin{align*} 
	\ddot{Y}(t) := \ddot Y(0)) + \int_0^t r_{\varphi(s)}({X}(s)) \wrt s.
\end{align*} 
% 
Regulated boundaries may also be included for the second fluid level. In this thesis we consider the second fluid to have a regulated boundary at \(y=0\) and unbounded above. To distinguish between unbounded and bounded processes, we use the notation \(\ddot Y(t)\) to denote the unbounded process and \(\dot Y(t)\) to denote the second fluid level process with a regulated lower boundary at 0.

% As classic fluid processes, $\{(\widehat X(t), \varphi(t)),t \geq 0\}$, or unbounded analogues, are used extensively in many areas, such as insurance and environmental modelling, it is clear that stochastic fluid-fluid queues have an even wider range of applicability. 

In the following, we assume that $\dot Y(t) \in [0,\infty)$ and that there is a boundary at level $0$ for both the first and second fluid levels and the first fluid level is also bounded above at \(b\). Thus, the rates of change of the fluid levels can be summarised as:
% 
	\begin{align*} 
		\frac{\wrt}{\wrt t}  X(t) & := \max\{0, c_i\} \quad \mbox{if }  X(t) = 0 \mbox{ and } \varphi(t) = i, \\
		\frac{\wrt}{\wrt t}  X(t) & := \min\{0, c_i\} \quad \mbox{if }  X(t) = b \mbox{ and } \varphi(t) = i, \\
          	\frac{\wrt}{\wrt t} \dot Y(t) & := \max\{0, r_i(x)\} \quad \mbox{if } \dot Y(t) = 0,\, X(t) = x \mbox{ and } \varphi(t) = i, 	
	\end{align*} 
	% 
for $i \in \mathcal{S} = \{1,...,N\}$. Let $\bs R(x) := \diag(r_i(x))_{i \in \mathcal{S}}$ be the diagonal fluid-rate matrix of functions for $\{\dot Y(t)\}$. 

For the remainder of this section, we summarise the findings of~\cite{bo2014} on the joint stationary distribution of $\{( \dot Y(t), X(t), \varphi(t))\}_{t \geq 0}$. The derivation of the stationary distribution relies on obtaining the operator \(\mathbb \Psi\) which gives the distribution of the process \(\{(X(t),\varphi(t))\}\) at the time when \(\{\dot Y(t)\}\) first returns to the level 0, given \(\dot Y(0)=0\).

The concepts leading to the derivation of \( {\mathbb \Psi}\) for the fluid-fluid queue are much the same as the derivation of the analogous quantity, \(\bs\Psi_X\), for a classical fluid queue. First, we need the infinitesimal generator of the driving process, \(\{(X(t),\varphi(t))\}\), which we denote by \(\mathbb B\). Then we need to partition \(\mathbb B\) on sets for which \(\{\dot Y(t)\}\) is increasing, decreasing or constant -- these are the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), below. We can then derive an operator-Riccati equation for the first return operator for the fluid-fluid queue, \(\mathbb \Psi\), in terms of the partitioned generator \(\mathbb B\), the rates \(r_i(x)\). The solution to the operator-Riccati equation gives the distribution of the driving process \(\{(X(t),\varphi(t))\}\) at the time when \(\{\dot Y(t)\}\) first returns to \(0\). 

The problem we have is to solve the operator-Riccati equation -- something which is only possibly in the very simplest of cases. This is where the methods in this thesis come in. Here, we approximate \(\mathbb B\) by a finite-dimensional matrix which we then partition according to the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\). We then substitute the resulting matrices into the operator-Riccati equation and this gives us a matrix-Riccati equation which we can then solve using known methods \cite{bot08}. 

A key element of the cell-based approximation schemes is the partition of the state space of \(\{(X(t),\varphi(t))\}\) into sets of the form \((\calD_{k,i},i)\), where \(\calD_{k,i}\) are intervals are known as cells. We must choose the cells wisely so that the partition into the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\) can be recovered from the partition into cells. This is not terribly difficult to do, but does require some notation and explanation. We now proceed to introduce the work of \cite{bo2014} and explain how we can recover the partition into the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\) from the cells as determined by the cell-based approximations.

For each Markovian state $i \in \mathcal{S}$, we partition the state space of \( X(t)\), \([0,b]\), according to the rates of change $r_i(\cdot)$ for the second fluid $\{\dot Y(t)\}$: $[0,b] := \mathcal{F}^{+}_i \cup \mathcal{F}^{-}_i \cup \mathcal{F}^{0}_i,$  
where 
% 
		\begin{align} 
%				\label{eqn:Fi1}
			\mathcal{F}^{+}_i & := \{u \in [0,b] : r_i(u) > 0\},  \; \nonumber
%				\label{eqn:Fi2}
			\\\mathcal{F}^{-}_i &:=  \{u \in [0,b]:  r_i(u) < 0\}, \; \nonumber
%				\label{eqn:Fi3}
			\\\mathcal{F}^{0}_i &:= \{u \in [0,b]: r_i(u) = 0\}.\label{eqn:fil}
		\end{align} 
% 
For all $i \in \mathcal{S}$, the functions $r_i(\cdot)$ are assumed to be sufficiently well-behaved so that $\mathcal{F}^{m}_i$, $m \in \{+, -, 0\}$, is a finite union of intervals and isolated points. 

We assume that the process $\{(\dot Y(t), X(t), \varphi(t))\}_{t\geq 0}$ is positive recurrent, in order to guarantee the existence of the joint stationary distribution. Define stationary operators 
\begin{align} 
		\label{eqn:jointpi} 
		\bbpi_i(y)(\mathcal{E}) & := \lim_{t \rightarrow \infty} \frac{\partial}{\partial y} \mathbb{P}\left( \dot Y(t) \leq y,  X(t) \in \mathcal{E}, \varphi(t) = i\right), \quad y>0,\\
% 
		\label{eqn:jointmass}
		{\mathbb p}_i(\mathcal{E}) & := \lim_{t \rightarrow \infty}  \mathbb{P}( \dot Y(t) = 0,  X(t) \in \mathcal{E}, \varphi(t) = i]),
	\end{align} 
	where $\mathcal{E} \subset [0,b]$.
Then let ${{\bbpi}}(y) = ( \bbpi_i(y))_{i \in \mathcal{S}}$ be a vector containing the joint stationary density operators and ${\mathbb{p}} = ( {\mathbb p}_i)_{i \in \mathcal{S}}$ be a vector containing the joint stationary mass operators. The determination of ${{\bbpi}}(y)$ involves two important matrices of operators, $ {\mathbb D}$ and $ {\mathbb \Psi}$ which we now introduce. %The operator \( {\mathbb B}\) is the \textit{infinitesimal generator} of the process \(\{(X(t),\varphi(t))\}\). %Intuitively, for a set $\mathcal{E} \subset \mathcal{F}$ and a measure vector $\boldsymbol{\mu} = (\mu_i)_{i \in \mathcal{S}}$, $\bs{\mu}V(t)(\mathcal{E})$, where \(V(t)\) is a semigroup with infinitesimal generator \(B\), gives the conditional probability of $X(t) \in \mathcal{E}$, 
% The operator \( {\mathbb \Psi}\) is such that ${\boldsymbol{\mu}}^{+} {\mathbb \Psi} (\mathcal{E})$ gives the conditional probability of $\{\dot Y(t)\}$ returning to level zero and doing so when $X(t) \in \mathcal{E}$, given that the initial distribution of \((X(0),\varphi(0))\) in the set \(\bigcup_{i\in\calS}(\mathcal F_i^+,i)\), is ${\boldsymbol{\mu}}^{+}$. 

\subsection{The infinitesimal generator, \(\mathbb B\), of the driving process}
Given the discussion above, if we are to replicate the arguments of \cite{bean2005} to derive the Laplace-Stieltjes transform of the first return operator of a fluid-fluid queue, we first need an expression for the generator of the driving process. We also need to partition it according to whether the second fluid is increasing, decreasing or constant. 

The generator of a fluid queue is a differential operator and to enable computation of the first-return operator, approximation method are needed. The approximation schemes which we discuss in this thesis are all cell-based methods which discretise the level of the fluid queue into cells. Thus, one complexity in approximation is to reconcile the partition according to whether the second fluid is increasing, decreasing or constant and the partition which the approximation method uses. As long as the rates of the fluid-fluid queue are not `too-badly' behaved, this is not terribly difficult to do, but it does require some discussion and notation. 

\label{subsec:B_operators}
\subsubsection{The transition semigroup of a fluid queue}
Since \(\{( X(t),\varphi(t))\}_{t\geq 0}\) is a Markov process, the evolution of probability can be described by a semigroup. Let $\mathcal{M}(\mathcal{S} \times [0,b])$ be the set of integrable complex-valued Borel measures on the Borel $\sigma$-algebra $\mathcal{B}_{\mathcal{S} \times [0,b]}$. For $\overline{\bs{\mu}} \in \mathcal{M}(\mathcal{S} \times [0,b])$, we can write \(\overline{\bs{\mu}} = (\overline{\mu_i})_{i \in \mathcal{S}}\). The measures \(\overline{\mu_i}(\cdot)\) represent an initial distribution, \(\overline{\mu_i}(\cdot) = \mathbb P(X(0)\in\cdot, \varphi(0) = i)\). 
Let \(\{\overline{\mathbb V}(t)\}_{t\geq 0},\, \overline{\mathbb V}(t):\mathcal{M}(\mathcal{S} \times [0,b]\mapsto \mathcal{M}(\mathcal{S} \times [0,b]\) be the semigroup describing the evolution of probability for \(\{(X(t),\varphi(t))\}_{t\geq 0}\) structured as a matrix of operators, \(\left[ \overline{\mathbb V}(t)\right]_{ij}= \overline{\mathbb V}_{ij}(t)\) where, 
\[\overline\mu_i \overline{ \mathbb V}_{ij}(t)(\mathcal{E}) = \int_{x\in[0,b]} \wrt \overline\mu_i(x)\mathbb P (X(t) \in\mathcal{E},\varphi(t) = j \mid X(0) = x, \varphi(0) = i).\]
Intuitively, the operator \( \overline{\mathbb V}(t)\) maps an initial measure \(\overline{\boldsymbol \mu}\) on \((X(0),\varphi(0))\) to the measure \(\mathbb P(X(t)\in \mathcal{E}, \varphi(t)=j)=:\overline\mu_j(t)(\mathcal{E})\). 
The matrix of operators \( \overline{\mathbb B}:=[\overline{\mathbb B}_{ij}]_{i,j\in\mathcal S}\) is the \textit{infinitesimal generator} of the semigroup \(\{ \overline{\mathbb V}(t)\}\) defined by 
\[ \overline{\mathbb B} =\left.\cfrac{\wrt}{\wrt t} \overline{\mathbb V}(t)\right|_{t=0},\]
with domain the set of measures for which this limit exists. Specifically, the domain of \(\overline{\mathbb B}\) is the set of measures, \(\overline{\bs \mu} = (\overline\mu_i)_{i\in\calS}\), for which each \(\overline\mu_i\) admits an absolutely continuous density on \((0,b)\), and can have a point mass at \(0\) if \(i\in\mathcal S_{-1}\) or a point mass at \(b\) if \(i\in\mathcal S_{K+1}\); call this set of measures \(\mathcal M_{0,b}\). The measure \(\overline\mu_i\) cannot have a point mass at 0 if \(i\notin \mathcal S_{-1}\), nor can they have a point mass at \(b\) if \(i\notin\calS_{{K+1}}\). In the sequel we write \(\overline v_i(x)\) as the density of \(\overline\mu_i\), and \(q_{{-1},i}\) and \(q_{{K+1},i}\) as the point masses of \(\overline\mu_i\) at \(0\) and \(b\), respectively (if such point masses exist). 

\subsubsection{A partition with respect to rates of the second fluid}
To use the operators \(\{ \overline{\mathbb  V}(t)\}\) and \( \overline{\mathbb  B}\) to analyse the fluid-fluid queue, \cite{bo2014} explicitly track when \((X(t),\varphi(t))\in(\mathcal F_i^m,i)\) for \(i\in\mathcal S,\, m \in \{+,-,0\}\) by partitioning the operators \( \overline{\mathbb  V}(t)\) and \( \overline{\mathbb  B}\) into \( \overline{\mathbb  V}_{ij}^{m n}\) and \( \overline{\mathbb  B}_{ij}^{m n}\), for \(i,j\in\mathcal S,\, m,n\in \{+,-,0\}\), where
\[\left.\overline\mu_i\right|_{\calF_i^m}  \overline{\mathbb  V}_{ij}^{m n}(t)(\mathcal{E}) := \int_{x\in[0,b]} \left. \wrt \overline\mu_i\right|_{\calF_i^m}(x)\mathbb P (X(t) \in\mathcal{E}\cap \calF_j^n,\varphi(t) = j \mid X(0) = x, \varphi(0) = i),\]
and \(\left.\overline\mu_i\right|_{E}\) is the restriction of \(\overline\mu_i\) to \(E\), \(\left.\overline\mu_i\right|_{E}(\mathcal E)=\overline\mu_i(\mathcal E\cap E)\). Similarly, for \( \overline{\mathbb  B}_{ij}^{m n},\) \(i,j\in\mathcal S,\, m,n\in \{+,-,0\}\).

\subsubsection{A partition as dictated by the approximation schemes}
We claim that numerical schemes are needed to approximate the analytic operator equations introduced in \cite{bo2014}. The schemes we choose to use here work by first partitioning the state space of the fluid level, \(\{X(t)\}\), into a collection of intervals, \({\calD_k,i}\) then constructing an approximation to \(\overline{\mathbb B}\) on each interval. To help elucidate the connection between the operators \(\{ \overline{\mathbb  V}(t)\}\), \( \overline{\mathbb  B}\) and their approximation counterparts we take a slightly different approach to partitioning these operators than that taken in \cite{bo2014}. Rather than partition according to the sets \(\calF_i^m,\,i\in\calS,\,m\in\{+,-,0\}\), we use the same partition as that in the construction of the approximation schemes. By doing so, we can directly correspond elements of the partitioned operators to their approximation counterparts. Since the partition used to construct the approximation schemes is finer, then we can reconstruct the partition in terms of the sets \(\calF_i^m,\,i\in\calS,\,m\in\{+,-,0\}\). 

Let us first partition the space \([0,b]\) into \(\calD_{-1,i}=\calD_{-1} = \{0\}\), \(i\in\calS_{-1}\), \(\calD_{{K+1},i}=\calD_{{K+1}}=\{b\}\), \(i\in\calS_{K+1}\), and non-trivial intervals \(\calD_{k,i}=[y_k,y_{k+1})\setminus \{0\},\) \(i\in\calS_+\cup\calS_0\) and \(\calD_{k,i}=(y_k,y_{k+1}]\setminus \{b\}\), \(i\in\calS_-\), with \(y_0=0,\,y_{K+1}=b,\, y_k<y_{k+1},\,k\in\mathcal K^\circ:=\{0,1,2,...,K\}\) and define \(\mathcal K = \{-1,K+1\}\cup\mathcal K^\circ\). For $\bs{\mu} \in \mathcal{M}_{0,b}(\mathcal{S} \times [0,b])$ we write \(\bs{\mu} = (\mu_{k,i})_{i \in \mathcal{S},k\in\mathcal K},\) where $\mu_{k,i}(\cdot) = \mu_i(\cdot \cap \calD_{k,i}),\,k\in\mathcal K.$ When it exists, we denote by \(v_{k,i}(x),\,x>0\), the densities associated with each measure, \(\mu_{k,i},\,k\in \mathcal K^\circ\). For \( i,j\in\calS,\,k,\ell\in\mathcal K\) define the operators 
\[\mu_{k,i} \mathbb V_{ij}^{k \ell}(t)(\mathcal{E}) := \int_{x\in\calD_{k,i}} \wrt \mu_{k,i}(x)\mathbb P (X(t) \in\mathcal{E}\cap \calD_{\ell,i},\varphi(t) = j \mid X(0) = x, \varphi(0) = i),\]
and the matrices of operators \(\mathbb V^{k\ell}(t) := \vligne{\mathbb V_{ij}^{k\ell}(t)}_{i,j\in\mathcal S},\,k,\ell\in\mathcal K\) and write 
\[\mathbb V(t) = \left[
	\begin{array}{ccccc}
		\mathbb V^{{-1},{-1}}(t)&\mathbb V^{{-1},0}(t)& \mathbb V^{{-1},1}(t) &\hdots & \mathbb V^{{-1},{K+1}}(t)\\
		\mathbb V^{0,{-1}}(t)&\mathbb V^{0,0}(t)&\mathbb V^{0,1}(t)&\hdots&\mathbb V^{0,{K+1}}(t)\\
		\mathbb V^{1,{-1}}(t)&\mathbb V^{1,0}(t)&\mathbb V^{1,1}(t)&\hdots&\mathbb V^{1,{K+1}}(t)\\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		\mathbb V^{{K+1},{-1}}(t) &\mathbb V^{{K+1},0}(t) &\mathbb V^{{K+1},1}(t) & \hdots & \mathbb V^{{K+1},{K+1}}(t) 
%		&\ddots &&&&\reflectbox{\(\ddots\)}\\
%		&&\mathbb V^{k-1,k-1}(t) & \mathbb V^{k-1,k}(t) & \mathbb V^{k-1,k+1}(t)& \\
%		&&\mathbb V^{k,k-1}(t) & \mathbb V^{k,k}(t) & \mathbb V^{k,k+1}(t)& \\
%		&&\mathbb V^{k+1,k-1}(t) & \mathbb V^{k+1,k}(t) & \mathbb V^{k+1,k+1}(t)& \\
%		&\reflectbox{\(\ddots\)}&&&&\ddots
	\end{array}\right].\]
Now define \(\mathbb B=\left.\cfrac{\wrt}{\wrt t}\mathbb V(t)\right|_{t=0}\) as the infinitesimal generator of \(\{\mathbb V(t)\}\), resulting in the tridiagonal matrix of operators 
\[\mathbb B(t) = \left[
	\begin{array}{ccccc}
		\mathbb B^{{-1},{-1}}(t)&\mathbb B^{{-1},0}(t)& & & \\
		\mathbb B^{0,{-1}}(t)&\mathbb B^{0,0}(t)&\mathbb B^{0,1}(t)&& \\
		&\mathbb B^{1,0}(t)&\mathbb B^{1,1}(t)&\ddots& \\
		& & \ddots & \ddots & \mathbb B^{K,{K+1}}\\
		& & & \mathbb B^{{K+1},K} & \mathbb B^{{K+1},{K+1}}
	\end{array}\right],\]
%\begin{align*}
%\mathbb B &= \left[\begin{array}{lllll}
%		\ddots &&&&\\
%		\mathbb B^{k-2,k-1} &\mathbb B^{k-1,k-1} & \mathbb B^{k-1,k} & & \\
%		&\mathbb B^{k,k-1} & \mathbb B^{k,k} & \mathbb B^{k,k+1}& \\
%		& & \mathbb B^{k+1,k} & \mathbb B^{k+1,k+1}&\mathbb B^{k+1,k+2} \\
%		&&&&\ddots
%	\end{array}\right],
%\end{align*}
where the blocks \(\mathbb B^{k\ell}:= \vligne{\mathbb B_{ij}^{k\ell}(t)}_{i,j\in\mathcal S},\,k,\ell\in\mathcal K\).\footnote{We use a blackboard bold font with an overline above the character (e.g.~\(\overline{\mathbb B}\) and \(\overline{\mathbb V}(t)\)) to represent theoretical operators derived in \citep{bo2014} which are constructed using the partition in (\ref{eqn:fil}). The operators denoted with an overline play a minor role in the introductory sections of this thesis, but do not appear again. We use a blackboard font sans overline (e.g.~\(\mathbb V(t)\) and \(\mathbb B\)) to represent the same operators but which are constructed with the finer partition defined by \(\mathcal D_k,\,k\in\mathcal K\). We use the letters \(i,j\in\mathcal S\) to represent states of the phase process, letters \(m,n,\in\mathcal \{+,-,0\}\) to refer to the partition in terms of the sets in Equations (\ref{eqn:fil}), and the letters \(k,\ell\in\mathcal K\) to refer to the finer partition into sets \(\{\calD_k\}_k\). With a slight abuse of notation, whenever we use the dummy variables \(k,\ell\) without qualification we imply \(k,\ell\in\mathcal K\), the dummy variables without qualification \(m,n\) imply \(m,n\in\{+,-,0\}\) and the dummy variables \(i,j\) without qualification imply \(i,j\in\calS\). E.g.~\(\mathbb B_{ij}^{k\ell}\) means \(\mathbb B_{ij}^{k\ell},\, i,j\in\calS, k,\ell\in\mathcal K\) and \(\mathbb B_{ij}^{mn}\) means \(\mathbb B_{ij}^{mn},\, i,j\in\calS, m,n\in\{+,-,0\}\).
} The tridiagonal structure arises since, for \(|k-\ell|\geq2\) it is impossible for \(\{X(t)\}\) to move from \(\mathcal D_{k,i}\) to \(\mathcal D_{\ell,j}\) in an infinitesimal amount of time.

By an appropriate choice of the intervals \(\{\mathcal D_k\},\, k\in\mathcal K\), the partition used in \cite{bo2014} can be recovered. Intuitively, we must ensure that each of the boundaries of \(\calF_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), align with a boundary of a cell \(\mathcal D_{k,i}\). Then, each set \(\calF_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), can be written as a union of cells, \(\mathcal D_k,\,k\in\mathcal K\), sans a collection of points which have measure zero for all measures in \(\mathcal M_{0,b}\), and this collection of points is inconsequential for the purposes of the approximations presented here. 

Formally, to recover the partition used in \citep{bo2014} we choose the intervals \(\calD_{k,i}\) such that \(   l (\calD_{k,i}\cap\calF_i^m) \in \{   l (\calD_{k,i}), 0\}\) for all \(i\in\calS,\,m\in\{+,-,0\},\,k\in\mathcal K\), for all \( l \in\mathcal M_{0,b}\). That is, we choose \(\calD_{k,i}\) such that it is contained, up to sets of measure 0 with respect to measures in \(\mathcal M_{0,b}\), within one of the sets \(\calF_i^m\) for \(m\in\{+,-,0\}\) and \(i\in\calS\). We assume such a partition for the rest of the thesis. For \(i\in\calS,\,m\in\{+,-,0\}\), let \(\mathcal K^m_i = \{k\in\mathcal K\mid  l (\calD_{k,i}\cap\calF_i^m) =   l (\calD_{k,i}),\,l\in\mathcal M_0 \}\), so that \(\bigcup\limits_{k\in\mathcal K_i^m} \calD_{k,i}\) and \(\mathcal F_i^m\) are equal up to a set of \(\mathcal M_{0,b}\)-measure 0. Define \(\mathcal K^m = \bigcup\limits_{i\in\calS}\mathcal K_i^m\), \(m\in\{+,-,0\}\). 
%Then for \(m\in\{+,-,0\}\), \(\mu_i^m(\cdot) = \sum\limits_{k\in\mathcal K_i^m} \mu_i^k(\cdot)\). 

To recover the partition defined by (\ref{eqn:fil}) we bundle together the elements of \(\mathbb V(t)\) which correspond to \(\calF_i^m\) and \(\calF_j^n\). That is, for \(m,n\in\{+,-,0\}\), define \(\mathbb V_{ij}^{m n}(t)\) as the matrix of operators 
\[\mathbb V_{ij}^{m n}(t) = \left[\mathbb V_{ij}^{k \ell}(t)\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n}.\] 
%Then, for \(i,j\in\calS,\,m,n\in\{+,-,0\}\), we can write \(\overline{\mathbb  V}_{ij}^{mn}(t) = \boldsymbol 1_{|\mathcal K_i^m|} \mathbb V_{ij}^{m n}(t) \boldsymbol 1_{|\mathcal K_j^n|}\tr{}\) where \(\boldsymbol 1_{|\mathcal K_i^m|}\) and \(\bs 1_{|\mathcal K_j^n|}\) are row-vectors of 1's of length \(|\mathcal K_i^m|\) and \(|\mathcal K_j^n|\), respectively, and \({}\tr{}\) denotes the transpose. 
The same construction can be achieved with \(\mathbb B\). 

Let \(\calS_k^+=\{i\in\calS\mid r_i(x)>0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^0=\{i\in\calS\mid r_i(x)=0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^-=\{i\in\calS\mid r_i(x)<0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^\bullet=\{i\in\calS\mid r_i(x)\neq0,\,\forall x \in\calD_{k,i}\}\) for \(k\in\mathcal K^\circ\) and \(\calS_{k}^+=\{i\in\calS_{k}\mid r_i(y_{k})>0\}\), \(\calS_{k}^-=\{i\in\calS_{k}\mid r_i(y_{k})<0\}\), \(\calS_{k}^0=\{i\in\calS_{k}\mid r_i(y_{k})=0\}\), \(\calS_{k}^\bullet=\{i\in\calS_{k}\mid r_i(y_{k})\neq 0\}\) for \(k\in\{-1,K+1\}\). For later reference, we need the following constructions. For \(k,\ell\in\mathcal K\) let
\begin{align}
	\mathbb B^{k\ell} & = \left[\mathbb B_{ij}^{k\ell}\right]_{i,j\in\mathcal S},\label{eqn:ref1here}
\end{align}
for \(i,j\in\calS\) let 
\begin{align}
	\mathbb B_{ij} & = \left[\mathbb B_{ij}^{k\ell}\right]_{k,\ell \in \mathcal K},
\end{align}
%for \(i,j\in\calS, \, m,n\in\{+,-,0\}\) 
%\begin{align}
%	\mathbb B_{ij}^{m n} &= \left[\mathbb B_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n},\label{eqn: Bmn etc}
%	\\ \mathbb B_{ij}^{k n} &= \left[\mathbb B_{ij}^{k \ell}\right]_{\ell\in\mathcal K_j^n} \mbox{ for }k\in\{{-1},1,2,...\},
%	\\ \mathbb B_{ij}^{m \ell} &= \left[\mathbb B_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m} \mbox{ for } \ell\in\{{-1},1,2,...\},
%	\label{eqn: Bl0 etc}
%\end{align}
and for \(m,n\in\{+,-,0\}\) let
\begin{align}
	\mathbb B^{m n} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{k\in\mathcal K^m,\ell\in\mathcal K^n},\label{eqn: Bmn2 etc}
	\\ \mathbb B^{k n} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{\ell\in\mathcal K^n} \mbox{ for }k\in\mathcal K,
	\\ \mathbb B^{m \ell} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{k\in\mathcal K^m} \mbox{ for } \ell\in\mathcal K.
	\label{eqn:ref2here}
\end{align}
We persist with the partition \(\calD_{k,i},\,k\in\mathcal K\) throughout this thesis as this is consistent with the partition used in the approximation schemes considered in this thesis. Note that for all the operators defined with this partition, the partitioning used in \citep{bo2014} can always be recovered by the above construction. 

\subsubsection{Definition of \(\mathbb B\)}
When \(v_k\) is differentiable we can write \(\mu_{k,i}\mathbb B_{ij}^{k\ell}(\mathcal{E})\) in kernel form as \[\displaystyle\int_{x\in\calD_{k,i},y\in\mathcal{E}}\wrt \mu_{k,i}(x)\mathbb B_{ij}^{k\ell}(x,dy).\] It is known that
\begin{align}\label{eqn: lfj22}\mu_{k,i}\mathbb B_{ij}^{kk}(\wrt y):=\int_{x\in\calD_{k,i}}\wrt\mu_{k,i}(x)\mathbb B_{ij}^{kk}(x,\wrt y)=\begin{cases}
v_{k,i}(y)T_{ij}\wrt y, & i\neq j,\\
v_{k,i}(y)T_{ii}\wrt y - c_i\cfrac{\wrt}{\wrt y}v_{k,i}(y)\wrt y, & i=j, 
\end{cases}\end{align}
on the interior of \(\calD_{k,i}\), \(k\in\mathcal K^\circ\), \citep{kk1995}. Intuitively, \(v_{k,i}(y)T_{ij}\wrt y\) represents the instantaneous rate of transition from phase \(i\) to \(j\) in the infinitesimal interval \(\wrt y\), \(v_{k,i}(y)T_{ii}\wrt y\) represents no such transition occurring, and \(- c_i\cfrac{\wrt}{\wrt y}v_{k,i}(y)\wrt y\) represents the drift across the interval \(\wrt y\) when the phase is \(i\). 

Translating the results of \cite{bo2014} to use the partition \(\{\calD_{k,i}\}\) we may state that, for all \(i,j\in\calS\), \(i\neq j\), \(k\in\{1,\dots,K-1\}\),
\begin{align*}
	\mu_{k,i}\mathbb B_{ij}^{kk}(\calD_{k,i})&=\int_{x\in\calD_k}v_{k,i}(x)T_{ij}\wrt x,\\
	\mu_{k,i}\mathbb B_{ii}^{kk}(\calD_{k,i})&=\int_{x\in\calD_k}v_{k,i}(x)T_{ii}\wrt x - c_iv_{k,i}(y_{k+1}^-) 1{(c_i>0)} + c_iv_{k,i}(y_{k}^+) 1{(c_i<0)},
\end{align*}
where \( 1(\cdot)\) is the indicator function and \(x^+\) and \(x^-\) denote the right and left limits at \(x\). 
Intuitively, the first expression represents the instantaneous rate of the stochastic transitions of the phase process \(\{\varphi(t)\}\) from \(i\) to \(j\) and \(\{X(t)\}\) remains in \(\calD_{k,i}\). The first term in the second expression represents the net rate of transition out of phase \(i\) while \(\{X(t)\}\) remains in \(\calD_{k,i}\), while the second and third terms in the second expression represent the flux out of the right-hand edge of \(\calD_{k,i}\) when \(c_i>0\) and the flux out of the left-hand edge of \(\calD_{k,i}\) when \(c_i<0\), respectively. Essentially, this is just a rewriting of (\ref{eqn: lfj22}) as an integral equation and including the boundary conditions.

The results of \cite{bo2014} also imply that, 
\begin{align*}
\mu_{k,i}\mathbb B_{ii}^{k,k+1}(\calD_{k+1})&= c_iv_{k,i}(y_{k+1}^-) 1{(c_i>0)},\mbox{ for all \(i\in\calS\), \(k\in\{0,1,2,...,K-1\}\),}
\\\mu_{k,i}\mathbb B_{ii}^{k,k-1}(\calD_{k-1})&= -c_iv_{k,i}(y_{k}^+) 1{(c_i<0)}, \mbox{  for all \(i\in\calS\), \(k\in\{1,2,3,...,K\}\)}.
\end{align*}
Intuitively, the first equation represents the flux from \(\calD_{k,i}\) to \(\calD_{k+1,i}\) across the shared boundary at \(y_{k+1}\) which occurs when \(c_i>0\) only. The second expression represents the flux from \(\calD_{k,i}\) to \(\calD_{k-1,i}\) across the shared boundary at \(y_{k}\) which occurs when \(c_i<0\) only. 

At the boundary \(x=0\), point mass moves between phases according to 
\begin{align*}
	\mu_{-1,i}\mathbb B_{ii}^{{-1},{-1}} &= \mu_{-1,i}(\{0\})T_{ii},\mbox{ if } c_i\leq 0,\,\\
	\mu_{-1,i}\mathbb B_{ij}^{{-1},{-1}} &= \mu_{-1,i}(\{0\})T_{ij},\mbox{ if }c_i\leq 0,\,c_j\leq 0,
	\end{align*}
	point mass becomes density at rate
	\begin{align*}
	\mu_{-1,i}\mathbb B_{ij}^{{-1},0} &= \mu_{-1,i}(\{0\})T_{ij},\mbox{ if }c_i\leq 0,\,c_j> 0,
	\end{align*}
	and density becomes point mass at rate
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,{-1}} &= -c_ip^{{-1}}_{ij}v_{0,i}(0^+),\mbox{ if }c_i< 0,\,c_j\leq 0.
	\end{align*}
	In \(\calD_{0,i}\), if \(c_i<0\) and \(c_j>0\) the phase changes from \(i\) to \(j\) either from a stochastic jump at rate \(T_{ij}\), or from \(\{X(t)\}\) hitting the boundary in phase \(i\) and transitioning with probability \(p_{ij}^{-1}\) to phase \(j\), hence
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ij}\wrt x - c_iv_{0,i}(0^+)p^{-1}_{ij}.
	\end{align*}
	Also in \(\calD_{0,i}\), if \(i\neq j\) and \(c_i\geq 0\) or \(i\neq j\), \(c_i< 0\) and \(c_j\leq 0\), the phase changes from \(i\) to \(j\) while \(\{X(t)\}\) remains in \(\calD_{0,i}\) at rate
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ij}\wrt x,
	\end{align*}
	else, \(\{(X(t),\varphi(t))\}\) leaves \((\calD_{0,i})\) at rate
	\begin{align*}
	\mu_{0,i}\mathbb B_{ii}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ii}\wrt x - c_iv_{0,i}(y_1^-) 1{(c_i> 0)} + c_iv_{0,i}(0^+)1{(c_i< 0)}.
\end{align*}

Similarly, at the boundary \(x=b\), point mass moves between phases according to 
\begin{align*}
	\mu_{K+1,i}\mathbb B_{ii}^{{K+1},{K+1}} &= \mu_{K+1,i}(\{0\})T_{ii}, \mbox{ if }c_i\geq 0,\,\\
	\mu_{K+1,i}\mathbb B_{ij}^{{K+1},{K+1}} &= \mu_{K+1,i}(\{0\})T_{ij},\mbox{ if }c_i\geq 0,\,c_j\geq 0,
	\end{align*}
	point mass becomes density at rate
	\begin{align*}
	\mu_{K+1,i}\mathbb B_{ij}^{{K+1},K} &= \mu_{K+1,i}(\{0\})T_{ij},\mbox{ if }c_i\geq 0,\,c_j<0,
	\end{align*}
	and density becomes point mass at rate
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,{K+1}} &= c_ip^{{K+1}}_{ij}v_{K,i}(0^+),\mbox{ if }c_i> 0,\,c_j\geq 0.
	\end{align*}
	In \(\calD_{K,i}\), if \(c_i>0\) and \(c_j<0\) the phase changes from \(i\) to \(j\) either from a stochastic jump at rate \(T_{ij}\), or from \(\{X(t)\}\) hitting the boundary in phase \(i\) and transitioning with probability \(p_{ij}^{K+1}\) to phase \(j\),
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ij}\wrt x + c_iv_{K,i}(b^-)p^{K+1}_{ij},\,c_i>0,\,c_j<0.
	\end{align*}
	If \(i\neq j\) and \(c_i\leq 0\) or \(i\neq j\), \(c_i>0\) and \(c_j\geq 0\), the phase changes from \(i\) to \(j\) within \(\calD_{K,i}\) at rate
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ij}\wrt x,
	\end{align*}
	else, \(\{(X(t),\varphi(t))\}\) leaves \((\calD_{K,i},i)\) at rate
	\begin{align*}
	\mu_{K,i}\mathbb B_{ii}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ii}\wrt x + c_iv_{K,i}(y_K^+)1{(c_i< 0)} - c_iv_{K,i}(b^-)1{(c_i> 0)}.
\end{align*}
Otherwise, \(\mu_{k,i}\mathbb B_{ij}^{k\ell}=0\).

Note that we have not presented \(\mathbb B\) in its full detail here and refer the reader to \citep{bo2014} for the details. The main goal here is to show how \(\mathbb B\) is used to construct the stationary distribution of the fluid-fluid queue and to illustrate the link between the operator \(\mathbb B\) and the approximations of the same object. As we shall see later, these expressions closely resemble the approximations to the same quantities. 

\subsection{The infinitesimal generator, \(\mathbb D\), of an in-out process}
Let $\beta(t) := \int_0^t \left| r_{\varphi(z)}(X(z)) \right|  \wrt z$ be the total unregulated amount of fluid that has flowed into or out of the second buffer during $[0,t]$, and let $\eta(y) := \inf \{t > 0: \beta(t) = y\}$ be the first time this accumulated in-out amount hits level $y$. Note that at the stopping time \(\eta(y)\) it must be that \((X({\eta(y)}),\varphi({\eta(y)}))\in(\calF_i^m,i)\) for some \(i\in\calS\) and \(m\in\{+,-\}\), i.e.~\(m\neq0\). We define the operators $\mathbb{U}_{ij}^{k\ell}(y,s): \mathcal{M}_{0,b}(\mathcal{D}_{k,i}\cap\calF_i^m) \mapsto \mathcal{M}_{0,b} (\calD_{\ell,j}\cap\mathcal{F}_j^n)$, for $k\in\mathcal K_i^+\cup\mathcal K_i^-$,  $\ell\in\mathcal K_j^+\cup\mathcal K_j^-$, and $i \in \mathcal{S}_k^\bullet,\,j \in \mathcal{S}_k^\bullet$, by 
% 
	\begin{align*} 
		&\mu_{k,i}\mathbb{U}_{ij}^{k\ell} (y,s) (\mathcal{E}) 
		\\&:= \int_{x \in \calD_{k,i}} \wrt  \mu_{k,i}(x) \mathbb{E}\left[e^{-s\eta(y)}{1}\left\{X({\eta(y)}) \in \mathcal{E}\cap\calD_{\ell,j},\;\varphi({\eta(y)}) = j\right\} \mid \varphi(0) = i, X(0) = x\right].
	\end{align*} 
Then, construct the matrix of operators 
\begin{align*}
%\mathbb{U}^{k \ell}(y,s)&:=[\mathbb U_{ij}^{k\ell}(y,s)]_{i\in\calS_k^\bullet,j\in\calS_\ell^\bullet}, \, k,\ell\in \mathcal K^+\cup\mathcal K^-,
\mathbb U(y,s) &:= \left[[\mathbb U_{ij}^{k\ell}(y,s)]_{i\in\calS_k^\bullet,j\in\calS_\ell^\bullet}\right]_{k,\ell\in\mathcal K^+\cup\mathcal K^-}.\end{align*}
%\[\mathbb U(t) = \left[
%	\begin{array}{llll}
%		\mathbb U^{{-1},{-1}}(t)&\mathbb U^{{-1},1}(t)& \mathbb U^{{-1},2}(t) &\hdots \\
%		\mathbb U^{1,{-1}}(t)&\mathbb U^{1,1}(t)&\mathbb U^{1,2}(t)&\hdots\\
%		\mathbb U^{2,{-1}}(t)&\mathbb U^{2,1}(t)&\mathbb U^{2,2}(t)&\hdots\\
%		\vdots & \vdots & \vdots & \ddots 
%	\end{array}\right].\]
%	\begin{align*} 
%		\mathbb{U}(y,s) = \left[\begin{array}{llllll} 
%		\ddots & & & &~\reflectbox{$\ddots$}\\
%            		& \mathbb{U}^{k-1,k-1}(y,s) & \mathbb{U}^{k-1,k}(y,s) & \mathbb{U}^{k-1,k+1}(y,s) \\
%			& \mathbb{U}^{k,k-1}(y,s) & \mathbb{U}^{k,k}(y,s) & \mathbb{U}^{k,k+1}(y,s) \\
%			& \mathbb{U}^{k+1,k-1}(y,s) & \mathbb{U}^{k+1,k}(y,s) & \mathbb{U}^{k+1,k+1}(y,s) \\	
%			\reflectbox{$\ddots$}& & & & \ddots		
%		\end{array}\right], 
%	\end{align*} 
	% 
%	where $\mathbb{U}^{k \ell}:=\mathbb U_{ij}^{k\ell}$ is an $|\mathcal{S}| \times |\mathcal{S}|$ matrix of operators for $y > 0$, $s \in \mathbb{C}$, and Re$(s) \geq 0$. 
The matrix of operators \(\mathbb D(s)\) is the infinitesimal generator of the semigroup \(\{\mathbb U(y,s)\}_{y\geq 0}\) defined by 
\[\mathbb D(s) = \cfrac{\wrt}{\wrt y}\mathbb U(y,s)|_{y=0},\]
whenever this limit exists.
			
	Recalling the constructions in Equations (\ref{eqn:ref1here})-(\ref{eqn:ref2here}) and using Lemma~$4$ of \cite{bo2014} gives the following expression for $\mathbb{D}(s)$. 
\begin{lem}\label{lemma: D(s)}
	For $y \geq 0$, $s \in \mathbb{C}$ with \textit{Re}$(s) \geq 0$, $i,j \in \mathcal{S}$, $k\in \mathcal K_i^+\cup\mathcal K_i^-$, $\ell\in \mathcal K_j^+\cup\mathcal K_j^-$,
	% 
	\begin{align*}
		\mathbb{D}_{ij}^{k\ell}(s) = [\mathbb{R}^{k}(
		\mathbb{B}^{k\ell } - s\mathbb{I} + \mathbb{B}^{k0 }(s \mathbb{I} - \mathbb{B}^{00})^{-1}\mathbb{B}^{0\ell})]_{ij}, 
	\end{align*} 
	% 
	where $\mathbb I$ is the identity operator, and $\mathbb{R}^{k} := diag(\mathbb{R}_i^{k},\,i \in \mathcal{S})$ is a diagonal matrix of operators $\mathbb{R}_i^{k}$ given by 
	\begin{align*} 
		{\mu}_{k,i}\mathbb{R}_{k,i}(\mathcal{E}) := \int_{x \in \mathcal{E} \cap \mathcal{D}_{k,i}} \frac{1}{|r_i(x)|}\wrt  \mu_{k,i}(x),\quad k\in \mathcal K_i^+\cup\mathcal K_i^-.
	\end{align*} 
\end{lem}

	Also, construct the matrices of operators 
	\begin{align*}
            	%\mathbb D^{k \ell} &:= \left[\mathbb D_{ij}^{k \ell}\right]_{i,j\in\mathcal S},\quad k,\ell\in\{{-1},1,2,...\},
		\mathbb D^{m n} &:= \left[\left[\mathbb D_{ij}^{k \ell}\right]_{i\in\calS_k^m,j\in\calS_k^n}\right]_{k\in\mathcal K^m,\ell\in\mathcal K^n}.
		%\\ \mathbb D_{ij}^{m n} &:= \left[\mathbb D_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n},\quad i,j\in\calS,\,m,n\in\{+,-,0\}.
        \end{align*}

\subsection{The first-return operator, $\mathbb\Psi(s)$}\label{sec: intro Psi}
We denote by $\mathbb \Psi(s)$ the matrix of operators with the same dimensions as \(\mathbb D^{+-}\), recording the Laplace-Stieltjes transforms of the time for $\{\dot Y(t)\}$ to return, for the first time, to the initial level of zero as introduced in \cite{bo2014} but constructed with respect to the finer partition \(\{\calD_{k,i}\}\). Define the stopping time $\zeta_Y(E):= \inf \{t > 0: \dot Y(t) \in E\}$ to be the first time $\{\dot Y(t)\}$ hits the set $E$, then each component $\mathbb \Psi_{ij}^{k\ell}(s): \mathcal{M}_{0,b}(\mathcal D_{k,i}) \mapsto \mathcal{M}_{0,b}(\mathcal D_{\ell,j}), i,j \in \mathcal{S},\,k\in\mathcal K_i^+$ and $\ell \in \mathcal K_j^-$, is given by  
% 
\begin{align*} 
    % 
	&\mu_{k,i}\mathbb\Psi_{ij}^{k\ell}(s) (\mathcal{E}) 
	\\& := \int_{x \in \calD_{k,i}} \wrt \mu_{k,i}(x)
	% 
	 \mathbb{E}\big[e^{-s\zeta_Y(\{0\})}1\left(X({\zeta_Y(\{0\})}) \in \mathcal{E}\cap\calD_{\ell,j},\; \varphi({\zeta_Y(\{0\})}) = j \right) \mid X(0) = x, 
	 \\&\qquad{} \dot Y(0) = 0, \varphi(0) = i\big].
	 % 
\end{align*} 
\cite{bo2014}~Theorem~1 provides the following result which characterises \(\mathbb\Psi(s)\).
\begin{theo} 
	\label{theo:Psi} 
	For \textit{Re}$(s) \geq 0$, $\mathbb\Psi(s)$ satisfies the  equation: 
	% 
	\begin{align*} 
		\mathbb{D}^{+-}(s) + \mathbb\Psi(s)\mathbb{D}^{-+}(s)\mathbb\Psi(s) + \mathbb{D}^{++}(s)\mathbb\Psi(s) + \mathbb\Psi(s)\mathbb{D}^{--}(s) = 0. 
	\end{align*} 
	% 
	Furthermore, if $s$ is real then $\mathbb\Psi(s)$ is the minimal non-negative solution. 
\end{theo} 

\subsection{Stationary Distribution} 

Let $\mathbb\Psi := \mathbb\Psi(0)$. We define $\zeta_Y^n(\{0\}) := \inf\{t \geq \zeta_Y^{n - 1}(\{0\}): \dot Y(t) = 0\}$, for $n \geq  2$, to be the sequence of hitting times to level $0$ of $\dot Y(t)$, with $\zeta_Y^1(\{0\}): = \zeta_Y(\{0\})$. Consider the discrete-time Markov process $\{X({\zeta_Y^n(\{0\})}), \varphi(\zeta_Y^n(\{0\}))\}_{n \geq 1}$, and for $i \in \mathcal{S},\,k\in\mathcal K_i^-$ define the measures $\bbxi_{k,i}$ as follows 
	%
	\begin{align*}
		\bbxi_{k,i}(\mathcal{E}) := \lim_{n \rightarrow \infty} \mathbb{P}\left(X(\zeta_Y^n(\{0\})) \in \mathcal{E}\cap\calD_{k,i}, \varphi(\zeta_Y^n(\{0\})) = i\right).
	\end{align*} 
	By \cite{bo2014}, the vector of measures $\boldsymbol{\bbxi} := (\bbxi_{k,i})_{i \in \mathcal{S}_k^-,k\in\mathcal K^-}$ satisfies the following set of equations 
 % 
 	\begin{align}
		\vligne{\boldsymbol{\bbxi}  & \boldsymbol{0}}\left(-\left[\begin{array}{ll} 
			\mathbb{B}^{--} & \mathbb{B}^{-0} \\
			\mathbb{B}^{0-} & \mathbb{B}^{00} 
		\end{array} \right]\right)^{-1}\left[\begin{array}{l} 
			\mathbb{B}^{-+} \\ 
			\mathbb{B}^{0+}
		\end{array} \right]\mathbb\Psi & = \boldsymbol{\bbxi}, \label{eqn:xi1}\\ 
		\sum_{k\in\mathcal K^-}\sum_{i \in \mathcal{S}_k^-}\bbxi_{k,i}(\mathcal{F}^-_i) & = 1. \label{eqn:xi2}
	\end{align} 

We reproduce Theorem 2 of \cite{bo2014} below, which gives the joint stationary distribution of $\{(\dot Y(t),X(t),  \varphi(t))\}$. Recall that the joint stationary density operator ${\bbpi}(y) = (\bbpi_i(y))_{i \in \mathcal{S}}$ for $\{(\dot Y(t), X(t), \varphi(t))\}$ and the joint stationary mass operator ${\mathbb p} = (\mathbb p_i)_{i \in \mathcal{S}}$ are defined by~\eqref{eqn:jointpi} and \eqref{eqn:jointmass}, respectively. %Following out notational convention, define an equivalent joint stationary density operator $\boldsymbol{\bbpi}(y) = (\bbpi_i(y))_{i \in \mathcal{S}}$ for $\{X(t), Y(t), \varphi(t)\}$ and an equivalent joint stationary mass operator $\boldsymbol{\mathbb p} = (\mathbb p_i)_{i \in \mathcal{S}}$, but partitioned accprsing . 
We can partition \(\bbpi\) as follows 
% 
	\begin{align*} 
		\bbpi(y) &= \vligne{\bbpi^{+}(y) & \bbpi^{-}(y) & \bbpi^{0}(y)} 
		\\&= \vligne{\left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^+,k\in\mathcal K^+} & \left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^-,k\in\mathcal K^-} & \left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^0,k\in\mathcal K^0}},
	\end{align*} 
	%
	where  
% 
	\begin{align*} 
		\bbpi_{k,i}(y)(\mathcal{E}) = \bbpi_i(y)(\mathcal{E}\cap \mathcal D_{k,i}).
	\end{align*} 
%
Similarly, we can write 
	\begin{align*} 
		\mathbb{p} &= \vligne{\mathbb{p}^{-} & \mathbb{p}^{0}} 
		= \vligne{\left(\mathbb p_{k,i}\right)_{i \in \mathcal{S}_k^-,k\in\mathcal K^-}  & \left(\mathbb p_{k,i}\right)_{i \in \mathcal{S}_k^0,k\in\mathcal K^0} },
	\end{align*} 
	%
	where  
% 
	\( 
		\mathbb p_{k,i}(\mathcal{E}) = \mathbb p_i(\mathcal{E}\cap \mathcal D_{k,i}).
	\) 
%
\begin{theo} 
	\label{theo:density} 
The operator ${\bbpi}^{m}(y)$, for $m \in \{+,-,0\}$ and $y > 0$, and the probability mass $\mathbb{p}^{m}$, for $m \in \{-,0\}$, satisfy the following set of equations:
% 	 
	\begin{align} 
	&\label{11} \; \bbpi^{0}(y) = \vligne{\bbpi^{+}(y) & \bbpi^{-}(y)}\left[\begin{array}{l} \mathbb{B}^{+0} \\ \mathbb{B}^{-0} \end{array} \right]\left(-\mathbb{B}^{00}\right)^{-1}, \\
	% 
	&  \vligne{\bbpi^{+}(y) & \bbpi^{-}(y)} = \vligne{\mathbb{p}^{-} & \mathbb{p}^{0}}\left[\begin{array}{l} \mathbb{B}^{-+} \\ \mathbb{B}^{0+} \end{array} \right]\vligne{e^{\mathbb{K}y} & e^{\mathbb{K}y}\mathbb\Psi}\left[\begin{array}{cc} \mathbb{R}^{+} & 0 \\ 0 & \mathbb{R}^{-}\end{array}\right], \\
	% 
	&  \vligne{\mathbb{p}^{-}  & \mathbb{p}^{0}} = z \vligne{{\bbxi} & \bs{0}} 
	\left(-\left[\begin{array}{ll} 
		\mathbb{B}^{--} & \mathbb{B}^{-0} \\
		\mathbb{B}^{0-} & \mathbb{B}^{00} 
		\end{array} \right] \right)^{-1},  \label{eqn:mass}\\
	%  
	& \sum_{m \in \{+,-,0\}}\sum_{i \in \mathcal{S}} \int_{y = 0}^{\infty} \bbpi_i^{m}(y)(\mathcal{F}^{m}_i)\wrt y + \sum_{m \in \{-,0\}} \sum_{i \in \mathcal{S}}\mathbb p^{m}_i(\mathcal{F}^{m}_i) = 1, \label{14}
	\end{align}
	% 
	where $\mathbb{K} := \mathbb{D}^{++}(0) + \mathbb\Psi\mathbb{D}^{(-+)}(0)$ and $z$ is a normalising constant. 
\end{theo} 

At this point we reiterate that Equations~(\ref{11})-(\ref{14}) are operator equations and are only amenable to numerical evaluation in the simplest of cases. Sources of this intractability come from, for example, the need to find the inverse operator \(( - \mathbb B^{00})^{-1}\), and the need to find the solution, \(\mathbb \Psi(s)\), of the operator equation in Theorem~\ref{theo:Psi}. There is also the complexity of the partition of the operators defined by the sets \(\mathcal F_{i}^m,\, i\in\calS,\,m\in\{+,-,0\}\). Therefore, there is the need for approximation schemes such as those presented in this thesis. 

\section{Quasi-birth-and-death processes with rational arrival process components}\label{qbd-rap intro}
One of the approximation schemes we develop in this thesis is a quasi-birth-and-death process with rational-arrival-process components (QBD-RAP). QBD-RAP processes are built from matrix-exponentially distributed inter-event times. We now introduce the class of matrix exponential distributions and recount some important properties before introducing the QBD-RAP. 

\subsection{Matrix exponential distributions}
Here we recount some facts about matrix exponential distributions. See \cite{MEinAP} for a more detailed exposition. A random variable, \(Z\), is said to have a matrix exponential distribution if it has a distribution function of the form \(1-\bs \alpha e^{\bs Sx}(-\bs S)^{-1}\bs s\), where \(\bs \alpha\) is a \(1\times p\) \emph{initial vector}, \(\bs S\) a \(p\times p\) matrix, and \(\bs s\) a \(p\times 1\) \emph{closing vector}, and \(\displaystyle e^{\bs S x} := \sum_{n=0}^\infty \cfrac{\left(\bs Sx\right)^n}{n!}\) is the matrix exponential. The density function of \(Z\) is given by \(f_Z(x) = \bs\alpha e^{\bs S x}\bs s\). The only restrictions on the parameters \((\bs \alpha, \bs S, \bs s)\) are that \(\bs \alpha e^{\bs S x} \bs s\) be a valid density function, i.e.~\(\bs \alpha e^{\bs S x} \bs s\geq 0,\) for all \(x\geq 0\) and \(\lim_{x\to\infty} 1-\bs \alpha e^{\bs Sx}(-\bs S)^{-1}\bs s = 1.\) There is the possibility of an \emph{atom} (a point mass) at 0, but here we do not consider this possibility. These condition that \(\bs \alpha e^{\bs Sx}\bs s\) be a valid density imposes some properties on representations \((\bs \alpha, \bs S, \bs s)\). However, in general there is no way to determine whether, a given triplet \((\bs \alpha, \bs S, \bs s)\) is a representation of a matrix exponential distribution, or not. Nonetheless, some properties of a triplet \((\bs \alpha, \bs S, \bs s)\) are known, such as the following, which is used in the characterisation of QBD-RAPs. 
\begin{thm}[Theorem 4.1.3, \cite{MEinAP}]
	The density function of a matrix exponential distribution with representation \((\bs\alpha, \bs S, \bs s)\) can be expressed in terms of real-valued constants as 
	\begin{align}
		\psi(x)&=\sum_{j=1}^{m_1} \sum_{k=1}^{p_j} c_{jk} \cfrac{x^{k-1}}{(k-1)!} e^{\mu_jx} + \sum_{j=1}^{m+2}\sum_{k=1}^{q_j} d_{jk}\cfrac{x^{k-1}}{(k-1)!}e^{\eta_jx}\cos(\sigma_jx) \nonumber 
		\\&\quad{}+ \sum_{j=1}^{m_2}\sum_{k=1}^{q_j}e_{jk}\cfrac{x^{k-1}}{(k-1)!}e^{\eta_jx}\sin(\sigma_jx), \label{eqn: spec}
	\end{align}
	for integers \(m_1,\,m_2,\,p_j,\) and \(q_j\) and some real constants \(c_{jk},\,d_{jk},\,e_{jk},\,\mu_j,\,\eta_j,\) and \(\sigma_j\). Here \(\mu_j,\, j=1,\dots,m_1\) are the real eigenvalues of \(\bs S\), while \(\eta_j+i\sigma_j, \, \eta_j-i\sigma_j,\, j=1,\dots,m_2\) denote its complex eigenvalues, which come in conjugate pairs. Thus, \(m_1+2m_2\) is the total number of eigenvalues, while the dimension of the representation is given by \(\displaystyle p=\sum_{j=1}^{m+1}p_j + 2\sum_{j=1}^{m_2}q_j\). 
\end{thm}
\begin{thm}[Theorem 4.1.4, \cite{MEinAP}]\label{thm: 4.1.4}
	Consider the nonvanishing terms of the matrix exponential density (\ref{eqn: spec}), \emph{i.e.}, the terms for which \(c_{jk}=0,\,d_{jk} =0,\) or \(e_{jk}=0\). Among the corresponding eigenvalues \(\lambda_j\), there is a real dominating eigenvalue \(\kappa\), say. That is, \(\kappa\) is real, \(\kappa \geq Re(\lambda_j)\) for all \(j\), and the multiplicity of \(\kappa\) is at least the multiplicity of every other eigenvalue with real part \(\kappa\).
\end{thm}
\begin{cor}[Corollary 4.1.5, \cite{MEinAP}]
If \((\bs \alpha, \bs S, \bs s)\) is a representation for a matrix exponential distribution, then \(\bs S\) has a real dominating eigenvalues.
\end{cor}
\begin{thm}[Theorem 4.1.6, \cite{MEinAP}]
	Let \(Z\) be a matrix-exponentially distributed random variable with density (\ref{eqn: spec}). Then the dominant real eigenvalue \(\kappa\) of Theorem~\ref{thm: 4.1.4} is strictly negative. 
\end{thm}
We define \(dev(\bs S)\) to be the real dominating eigenvalue of \(\bs S\), that is \(dev(\bs S)=\kappa\) in Theorem~\ref{thm: 4.1.4}.

The class of matrix exponential distribution is characterised as the class of probability distributions which have a rational Laplace transform. That is, \(\displaystyle \int_{x=0}^\infty e^{-\lambda x}\bs \alpha e^{\bs S x}\bs s \wrt x\) is a ratio of two polynomial functions in \(\lambda\). Matrix exponential distributions are an extension of Phase-type distributions, where for the latter, \(\bs S\) must be a sub-generator matrix of a CTMC, \(\bs s = -\bs S\bs e\) where \(\bs e\) is a \(1\times p\) vector of ones, and \(\bs \alpha\) is a discrete probability distribution.  

A \emph{representation} of a matrix exponential distribution is a triplet \((\bs \alpha, \bs S, \bs s)\), and we write \(Z\sim ME(\bs \alpha, \bs S, \bs s)\) to denote that \(Z\) has a matrix exponential distribution with this representation. The order of the representation is the dimension of the square matrix \(\bs S\), i.e.~if \(\bs S\) is \(p\times p\), then the matrix exponential distribution is said to be of order \(p\). Representations of matrix-exponential distributions are not unique \citep{MEinAP}. A representation is called \emph{minimal} when \(\bs S\) has the smallest possible dimension. Throughout this work, we assume that the representation of any matrix exponential distribution is minimal. Let \(\bs e_i\) be a vector with a 1 in the \(i\)th position and zeros elsewhere. We assume that \(\bs s = -\bs S\bs e\), and that \((\bs e_i,\bs S,\bs s)\) for \(i=1,\dots,p\) are representations of matrix exponential distributions. It is always possible to find such a representation \cite[Theorem 4.5.17, Corollary 4.5.18]{MEinAP}. As such, we abbreviate our notation \(Z\sim ME(\bs \alpha, \bs S, \bs s)\) to \(Z\sim ME(\bs \alpha, \bs S)\). Further, given \(\bs s=-\bs S\bs e\) then observe that  \(\displaystyle \int_{x=0}^\infty e^{\bs S x} \bs s\wrt x = (-\bs S)^{-1}\bs s= \bs e\). 

For a given \(p\times p\) matrix \(\bs S\), denote by \(\mathcal{A}\subset \mathbb R^p\) the space of all possible vectors \(\bs a\) such that \((\bs a, \bs S)\) is a valid representation of a (possibility defective) matrix exponential distribution. %By CITE it is known that \(\mathcal{E}\) is a closed, bounded, convex, affine subspace of \(\mathbb R^p\). 



%\subsection{Rational arrival processes}
%Let \(N\) be a simple point process, with event time \(Y_0=0<Y_1<Y_2<\cdot\). Let \(\{N(t)\}_{t\geq0}\) be the right-continuous counting process associated with \(N\); \(N(t)\) returns the number of events by time \(t\). Denote by \(f_{N,n}(y_1,\dots,y_n)\) the joint density of \(Y_1, Y_2-Y_1, \dots,Y_n-Y_{n-1}\), the first \(n\) inter-arrival times. For a matrix \(\bs B\) let \(dev(\bs B)\) denote the dominant eigenvalue of \(\bs B\) (the one with maximal real part). Theorem 1.1 of Asmussen and Bladt CITE states that the point process \(N\) is a RAP if there exists matrices \(\bs S\), \(\bs D\), a row vector \(\bs\alpha\), and a column vector \(\bs s\) such that \(dev(\bs S)<1\), \(dev(\bs S+\bs D)=0\), \((\bs S+\bs D)\bs e=0,\) and 
%\[f_{N,n}(y_1,\dots,y_n) = \bs \alpha e^{\bs Sy_1} \bs D e^{\bs Sy_1} \bs D \dots e^{\bs Sy_n} \bs s.\]
%Here \(\bs s\) can be taken to be \(\bs D e\). The \(n\)th inter-arrival, \(T_n-T_{n-1}\), has a matrix exponential distribution with density \(\bs \alpha(-\bs S\bs D )^{n-1}e^{\bs S y_n}\bs s\). Denote such a process \(N \sim RAP(\bs \alpha, \bs S, \bs D)\). 
%
%Asmussen and Bladt CITE, Corollary 2.2, show that associated with the RAP is a row-vector-valued \emph{orbit} process, \(\{\bs A(t)\}_{t\geq0},\), 
%\[\bs A(t) = \cfrac{\bs \alpha\left( \prod_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs S\right)e^{\bs S t-Y_{N(t)}}}{\bs \alpha\left( \prod_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs S\right)e^{\bs S( t-Y_{N(t)})}\bs e}.\]
%Thus, \(\{\bs A(t)\}\) is a piecewise-deterministic Markov process where, in between jumps \(\{\bs A(t)\}\) according to 
%\[\bs A(t) = \cfrac{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}}{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}\bs e},\]
%where \(\bs A(Y_{N(t)}^-) = \lim_{u\to 0^+}\bs A(Y_{N(t)}-u)\). The process \(\{\bs A(t)\}\) jumps at event times of \(N\). At time \(t\) the intensity with which \(\{\bs A(t)\}\) has a jump is \(\bs A(t) \bs D\bs e\), and upon a jump at time \(t\), the new position of the orbit is \(\bs A(t) = \bs A(t^-) \bs D/\bs A(t^-) \bs D\bs e\). 
%
%RAPs are an extension of Markovian arrival processes (MAPs) to include matrix-exponential inter-arrival times. For MAPs, the vector \(\bs A(t)\) is a vector of posterior probabilities of a continuous-time Markov chain. 
%
%Intuitively, \(\bs A(t)\) encodes all of the information about the jump times of the RAP up to time \(t\). Let \(\mathcal F_{t}\) be the \(\sigma\)-algebra generated by \(N(u), u\in[0,t]\). Then \(N\mid \mathcal F_t \equiv N\mid \bs A(t) \sim ME(\bs A(t), \bs S, \bs S)\). In words, the future of the point process after time \(t\) given all of the information about the process up to and including time \(t\), is distributed as a RAP with initial vector \(\bs A(t)\). 

\subsection{QBD-RAPs}\label{sec: qbd-rap}
% As RAPs are an extension of Markovian arrival processes to include matrix-exponential inter-arrival times, QBD-RAPs are extensions of QBDs to include matrix-exponential times between level changes. 

To define a QBD-RAP we first need a Batch (Marked) rational-arrival-process (RAP). Let \(\mathscr K\subset \mathbb Z\) be a set of \emph{marks}. Let \(N\) be a point process, and \(Y_0=0<Y_1<Y_2\cdots\) be event times of \(N\). Let \(\{N(t)\}\) be the counting process associated with \(N\) such that \(N(t)\) returns the number of events by time \(t\). Associated with the \(n\)th event is a mark \(M_n\). For \(i\in\mathscr K\), let \(N_i\) be simple point processes associated with events with marks of type \(i\) only, and let \(\{N_i(t)\}_{t\geq 0}\) be the associated counting processes of events of mark \(i\). Denote by \(f_{N,n}(y_1,m_1,y_2,m_2,\dots,y_n,m_n)\) the joint density, probability mass function of the first \(n\) inter-arrival times, \(Y_1,Y_2-Y_1,\dots,Y_n-Y_{n-1}\), and the associated marks \(M_n\). From Bean and Nielsen \cite[Theorem 1]{bn2010} we have the following. 
\begin{thm}
A process \(N\) is a Marked RAP if there exist matrices \(\bs S\), \(\bs D_i,\,i\in\mathscr K\), and a row vector \(\bs \alpha\) such that \(dev(\bs S)<0\), \(dev(\bs S+\bs D)=0\), \((\bs S+\bs D)\bs e = 0\), \(\bs D = \sum_{i\in\mathscr K} \bs D_i\), and 
\begin{align}f_{N,n}(y_1,m_1,y_2,m_2,\dots,y_n,m_n) = \bs \alpha e^{\bs S y_1}\bs D_{m_1} e^{\bs S y_2} \bs D_{m_2}\dots e^{\bs S y_n} \bs D_{m_n}\bs e.\label{eqn: brap}\end{align}
Conversely, if a point process has the property (\ref{eqn: brap}) then it is a Marked RAP.
\end{thm}

Denote such a process \(N\sim BRAP(\bs \alpha, \bs S, \bs D_i,\,i\in\mathscr K).\)

Also, from \cite{bn2010}, associated with a Marked RAP is a row-vector-valued \emph{orbit} process, \(\{\bs A(t)\}_{t\geq0},\)
\[\bs A(t) = \cfrac{\bs \alpha\left( \prod\limits_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs D_{M_i}\right)e^{\bs S (t-Y_{N(t)})}}{\bs \alpha\left( \prod\limits_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs D_{M_i}\right)e^{\bs S( t-Y_{N(t)})}\bs e}.\]
Thus, \(\{\bs A(t)\}\) is a piecewise-deterministic Markov process where, in between events \(\{\bs A(t)\}\) evolves deterministically according to 
\[\bs A(t) = \cfrac{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}}{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}\bs e},\]
where \(\bs A(Y_{N(t)}^-) = \lim_{u\to 0^+}\bs A(Y_{N(t)}-u)\). The process \(\{\bs A(t)\}\) `jumps' at event times of \(N\) (the process may not actually jump at these times, but we still call it a `jump' and typically the dynamics change at this point). At time \(t\) the intensity with which \(\{\bs A(t)\}\) has a jump is \(\bs A(t) \bs D\bs e\), i.e.~\(\mathbb P(N(t)=n,N(t+dt)=n+1)=\bs A(t) \bs D\bs e\wrt t\). Upon an event the event is associated with mark \(i\) with probability \(\bs A(t) \bs D_i\bs e/\bs A(t) \bs D\bs e\). Upon on an event at time \(t\) with mark \(i\), the new position of the orbit is \(\bs A(t) = \bs A(t^-) \bs D_i/\bs A(t^-) \bs D_i\bs e\). It is important to note that the jumps of the orbit process are \emph{linear} transformations of the orbit process immediately before the time of the jump. 

Marked RAPs are an extension of Marked Markovian arrival processes to include matrix-exponential inter-arrival times. For Marked MAPs, the vector \(\bs A(t)\) is a vector of posterior probabilities of a continuous-time Markov chain. 

Intuitively, \(\bs A(t)\) encodes all the information about the event times of the Marked RAP and associated marks up to time \(t\) that is needed to determine the future behaviour of the point process. Let \(\mathcal F_{t}\) be the \(\sigma\)-algebra generated by \(N(u), u\in[0,t]\). Then \(N\mid \mathcal F_t \equiv N\mid \bs A(t) \sim BRAP(\bs A(t), \bs S, \bs D_i,i\in\mathscr K)\). In words, the future of the point process after time \(t\) given all the information about the process up to and including time \(t\), is distributed as a Marked RAP with initial vector \(\bs A(t)\). 

Now consider a Marked RAP, \(N\sim BRAP(\bs \alpha, \bs S, \bs D_i,\,i\in\{-1,0,+1\}).\) The process \(\{(L(t),\bs A(t))\}_{t\geq0}\) formed by letting \(L(t) = N_{+1}(t) - N_{-1}(t)\) is a QBD-RAP. QBDs are QBD-RAPs where the inter-event times are of Phase-type.


% vector space? Polynomial basis?
% me basis? semigroup? generator? 
\section{Laplace Transforms}
In Chapters~\ref{sec: conv} and~\ref{ch: global results} we work with an object called the \emph{Laplace transform}. For a measure \( \mu\), defined on the Borel sets of \([0,\infty)\), we define the Laplace transform of \(\mu\) to be
\[\widehat \mu(\lambda) = \mathcal L(\mu)(\lambda) = \int_{t=0}^\infty e^{-\lambda t}\wrt \mu,\]
where the \emph{region on convergence} is the set of values of \(\lambda\in\mathbb R\) such that the integral is finite. When \(\mu\) has a density, \(v\), then the Laplace transform is 
\[\widehat \mu(\lambda) = \widehat v(\lambda) = \mathcal L(v)(\lambda) = \int_{t=0}^\infty e^{-\lambda t}v(x)\wrt x.\]
When \(\mu\) is the probability measure associated with a random variable, \(Z\), say, then we may write 
\[\widehat \mu(\lambda)=\mathbb E[e^{-\lambda Z}],\]
and the region of convergence is at least \([0,\infty)\). 
Further, letting \(E^\lambda\) be an exponentially distributed random variable with rate \(\lambda\) and noting that \(\mathbb P(E^\lambda >t ) = e^{-\lambda t}\), then 
\[\widehat \mu(\lambda)=\mathbb P(Z<E^\lambda),\]
which gives a probabilistic interpretation of the Laplace transform. 

A convenient property of the Laplace transform which we utilise is the Convolution Theorem. 
\begin{thm}[Convolution Theorem]
	Let \(f,g: [0,\infty) \to \mathbb R\) be integrable functions, then 
	\[\mathcal L\left(\int_{u=0}^tf(u)g(t-u)\wrt u\right)(\lambda) = \mathcal L\left(f\right)\cdot \mathcal L\left(g\right).\]
\end{thm}
The Convolution Theorem states that the Laplace transform of the convolution, given by \(\displaystyle \int_{u=0}^tf(u)g(t-u)\wrt u\), is equal to the product of the Laplace transform of \(f\) and \(g\). 

The Laplace transform is unique in the sense that, if \(\mu\) and \(\nu\) are two measures on the Borel sets of \([0,\infty)\) and 
\[\widehat \mu(\lambda) = \widehat \nu(\lambda)\] 
for all \(\lambda > a\) with \(a<\infty\), then \(\mu\) and \(\nu\) are the same. In terms of functions, \(f,g: [0,\infty) \to \mathbb R\), if 
\[\mathcal L(f)(\lambda) = \mathcal L(g)(\lambda),\]
for all \(\lambda > a\) with \(a<\infty\), and \(f\) and \(g\) are continuous, then \(f(t)=g(t)\) for all \(t\geq 0\). Without knowing \(f\) and \(g\) are continuous, then we can only claim that 
\(f(t)=g(t)\) for all \(t\geq 0, t\notin \mathcal N,\) where \(\mathcal N\) is a \emph{null set} with respect to Lebesgue measure. 

\section{Convergence theorems}
We use the following convergence theorems to help us prove that the QBD-RAP scheme converges weakly to the distribution of the fluid queue. The first result we state, the Portmanteau Theorem, is a sweeping statement about convergence of measures. First, let's define the notion of weak convergence. We follow \cite{portmanteaubook}. 

Denote by \(\mathcal M_f(E)\) the set of all finite measures on \((E,\mathcal E)\), where \(E\) is a non-empty set and \(\mathcal E\) is a \(\sigma\)-algebra. Further, denote by \(C_b(E)\) the set of continuous bounded functions on \(E\). Let \(\mu,\,\mu_1,\,\mu_2,...\in \mathcal M_f(E)\). We say that \(\{\mu_n\}_{n\in\mathbb N}\) converges weakly to \(\mu\), formally \(\mu_n\to \mu\) weakly as \(n\to\infty\), if 
\[\int f \wrt \mu_n \to \int f\wrt \mu,\mbox{ for all }f \in C_b(E).\]
We use part of The Portmanteau Theorem in Chapter~\ref{ch: global results}. Let \(\mathcal M_{\leq 1}(E):=\{\mu \in \mathcal M_f(E)\mid \mu(E)\leq 1\},\) the set of all sub-probability measures on \((E,\mathcal E)\). 
\begin{thm}[[Part of] The Portmanteau Theorem, Theorem~13.16 of \cite{portmanteaubook}]
	Let \(E\) be a metric space and let \(\mu,\, \mu_1,\,\mu_2,... \in \mathcal M_{\leq 1}(E)\). The following are equivalent.
	\begin{enumerate}
		\item[(i)] \(\mu_n\to \mu\) weakly as \(n\to \infty\). 
		\item[(ii)] \(\int f\wrt \mu_n \to \int f\wrt \mu\) for all bounded, Lipschitz continuous \(f\).
		\\ \(\vdots\) 
	\end{enumerate}
\end{thm}
There are 8 parts to The Portmanteau Theorem in \cite{portmanteaubook}, Theorem~13.16. Here we only quote to relevant parts; the other 6 parts require us to define other concepts which are not relevant to this thesis, so they are omitted. See \cite{portmanteaubook}, Theorem~13.16 for details. 

Another tool we can use to show convergence of measures is to show that the Laplace transforms converge, as stated in the following theorem.
\begin{thm}[Extended Continuity Theorem, \cite{feller1957}, Theorem 2a]\label{thm: ext cont thm}
	For \(p=1,2,...,\) let \(U_p\) be a measure with Laplace transform \(\zeta_p\). If \(\zeta_p(\lambda)\to\zeta(\lambda)\) for \(\lambda > a\geq 0\), then \(\zeta\) is the Laplace transform of a measure \(U\) and \(U_p\to U\) [weakly].
	
	Conversely, if \(U_p\to U\)[weakly] and the sequence \(\{\zeta_p(a)\}\) is bounded, then \(\zeta_p(\lambda)\to\zeta(\lambda)\) for \(\lambda >a\). 
\end{thm}

In Chapters~\ref{sec: conv} and~\ref{ch: global results} we use the Dominated Convergence Theorem to aid our convergence arguments. In applied probability we often want to prove convergence of certain probabilistic expressions but the expressions themselves are too difficult to characterise as one. An approach which can simplify matters is to partition the expression on certain events where we have a simpler characterisation of the expression on each event, thereby enabling us to prove convergence on each element in the partition. The original expression can be written as a sum over the partition. To establish the convergence result we initially desired, we can use the convergence of each element of the partition and the Dominated Convergence Theorem. I have taken the following from Theorem~1.13~\cite{steinreal}. \cite{steinreal} use the notation 
\[\int f = \int f \wrt x = \int f \wrt m(x),\]
where \(m\) denotes Lebesgue measure, to denote the Lebesgue integral. With their notation clarified we now quote their results.
\begin{thm}[Dominated Convergence Theorem]
	Suppose \(\{f_n\}\) is a sequence of measurable functions such that \(f_n(x)\to f(x)\) almost everywhere with respect to \(x\), as \(n\) tends to infinity. If \(|f_n(x)|\leq g(x)\), where \(g\) is integrable, then 
	\[\int|f_n-f|\to 0 \mbox{ as } n \to \infty,\]
	and consequently 
	\[\int f_n\to\int f \mbox{ as } n\to \infty.\]
\end{thm}

Also in Chapters~\ref{sec: conv} and~\ref{ch: global results} we want to manipulate infinite sums or integrals and rearrange the order of summation or integration. However, things can go awry when we swap the order of integration/summation if we are not careful. The next few results give some conditions under which we have equality under before and after swapping the order of summation/integration. Once again, we follow \cite{steinreal} quoting their Theorem~2.13. If \(f\) is a function in \(\mathbb R^{d_1}\times \mathbb R^{d_2}\), the \emph{slice} of \(f\) corresponding to \(y\in\mathbb R^{d_2}\) is the function \(f^y\) of the \(x\in\mathbb R^{d_1}\) variable, given by 
\[f^y(x)=f(x,y).\]
Similarly, the slice of \(f\) for a fixed \(x\in\mathbb R^{d_1}\) is \(f_x(y)=f(x,y)\). 
\begin{thm}[Fubini's Theorem]\label{Fubini}
	Suppose \(f(x,y)\) is integrable on \(\mathbb R^{d_1}\times \mathbb R^{d_2}\). Then for almost every \(y\in\mathbb R^{d_2}\):
	\begin{itemize}
		\item The slice \(f^y\) is integrable on \(\mathbb R^{d_1}\).
		\item The function defined by \(\int_{R^{d_1}} f^y(x)\wrt x\) is integrable on \(\mathbb R^{d_2}\). 
	\end{itemize}
	Moreover:
	\begin{itemize}
		\item[(iii)] \(\displaystyle\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y = \int_{\mathbb R^d}f.\)
	\end{itemize}
\end{thm}
\cite{steinreal} then follow ``Clearly, the [Fubini] theorem is symmetric in \(x\) and \(y\) so that we also may conclude that the slice \(f_x\) is integrable on \(\mathbb R^{d_2}\) for almost every \(x\). Moreover, \(\int_{\mathbb R^{d_2}}f_x(y)\wrt y\) is integrable and 
\[\displaystyle\int_{R^{d_1}}\left(\int_{R^{d_2}}f(x,y)\wrt y\right)\wrt x = \int_{\mathbb R^d}f.\]
In particular, Fubini's theorem states that the integral of \(f\) on \(\mathbb R^d\) can be computed by iterating lower-dimensional integrals, and that the iterations can be taken in any order
\[\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y=\displaystyle\int_{R^{d_1}}\left(\int_{R^{d_2}}f(x,y)\wrt y\right)\wrt x = \int_{\mathbb R^d}f.\mbox{''}\]
It is this last statement which is most powerful. Effectively, if either
\[\int_{R^{d_2}}\left(\int_{R^{d_1}}|f(x,y)|\wrt x\right)\wrt y<\infty,\]
or
\[\int_{R^{d_1}}\left(\int_{R^{d_2}}|f(x,y)|\wrt y\right)\wrt x<\infty,\]
then we can swap the order of integration. 

A closely related theorem which is often used alongside Fubini's Theorem is the following 
\begin{thm}[Tonelli's Theorem]\label{Tonelli}
	Suppose \(f(x,y)\) is a non-negative measurable function on \(\mathbb R^{d_1}\times \mathbb R^{d_2}\). Then for almost every \(y\in\mathbb R^{d_2}\):
	\begin{itemize}
		\item The slice \(f^y\) is integrable on \(\mathbb R^{d_1}\).
		\item The function defined by \(\int_{R^{d_1}} f^y(x)\wrt x\) is integrable on \(\mathbb R^{d_2}\). 
	\end{itemize}
	Moreover:
	\begin{itemize}
		\item[(iii)] \(\displaystyle\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y = \int_{\mathbb R^d}f(x,y)\wrt x\wrt y \mbox{ in the extended sense}.\)
	\end{itemize}
\end{thm}

Where the \emph{extended Lebesgue integral} of an extended valued (it can take the value \(+\infty\)) non-negative function \(f\) is defined by 
\[\int f(x)\wrt x = \sup_{g}\int g(x)\wrt x.\]
Once again, we note that the theorem is symmetric in \(x\) and \(y\), so we can establish that we may swap the order of integration provided that \(f\) is non-negative. 

Collectively, we refer to Theorems~\ref{Fubini} and~\ref{Tonelli} together as the Fubini-Tonelli Theorem, but they are otherwise known collectively as just Fubini's Theorem. Often, they are used in conjunction. Since \(|f|\) is a non-negative function, then we may use Tonelli's Theorem and compute (or bound) the integral \(\int |f|\) via computing an iterated integral. If this is found to be finite, then Fubini's Theorem applies so \(f\) is integrable, and we may evaluate \(\int f\) via an iterated integral. 

In the context of probability, the function \(f\) which we are integrating is often positive, so Tonelli's Theorem is all that is required to justify a swap of iterated integrals. 

\section{More on relevant literature \& the context of the thesis} 
In the process of introducing the technical concepts we have already covered some the relevant literature already. Here, we provide some more context for, and elucidate exactly what, the contributions of this thesis. The focus of this thesis is on developing numerical approximations of fluid queues such that we may approximate the operator-analytic analysis of fluid-fluid queues form \cite{bo2014}. %The analysis of fluid-fluid queues in \cite{bo2014} relies on a certain transient analysis of the driving process, which is a fluid queue.

\paragraph{Fluid-fluid queues and related models} Related to the analysis of fluid-fluid queues are the works of \cite{mz2012,lnp13,bo2013b} and \cite{bop2020}. \cite{mz2012} analyse related discrete-time multidimensional Markov additive processes and derive operator-analytic expressions for the stationary distribution. Although markedly different in their approach \cite{mz2012}, like  \cite{bo2014}, is inspired by a matrix-analytic approach. The work of \cite{lnp13} considers a fluid-fluid queue where the driving process \(\{(X(t),\varphi(t))\}\) is level-dependent, and derives computable expressions for the marginal probability distribution of the first level, and bounds for that of the second level. Their analysis is somewhat specific to the data handling model considered and only arrives at bounds for the marginal limiting distribution of the second fluid level. In contrast, the work of \cite{bo2014} treats more general models and derives operator-analytic expressions for the joint stationary distribution. Therefore, compared to \cite{lnp13}, the methods of this thesis therefore apply to a general class of models and enable to approximation of the joint stationary distribution. However, unlike \cite{lnp13}, the analysis of \cite{bo2014} does not consider a level-dependent driving process such as this, but I believe that it would be relatively straight forward to extend their results (and the results of this thesis) to the level-dependent case where the rates \(c_i\) depend on \(X(t)\) in a piecewise-constant way. \cite{bo2013b} analyse a model which is simple special case of the fluid-fluid queue. They consider a model where the second level process \(\{\dot Y(t)\}\) has constant rates which do not depend on \(\ddot X(t)\) (but do depend on \(\varphi(t)\)), while \(\{(\ddot X(t),\varphi(t))\}\) is an \emph{unbounded} fluid queue. They derive matrix-analytic expressions for certain Laplace transforms, with respect to the position of the first fluid \(\{X(t)\}\) relative to its starting position \(X(0)\), of the probability of certain transient events (such as first return and draining/filling times) of \(\{Y(t)\}\). In contrast, \cite{bo2013b} together with this thesis, treat a more general model and together, give computational methods for various performance measures of fluid-fluid queues. The yet-unpublished work of \cite{bop2020} treats a similar model to that of \cite{bo2013b}, except that the driving process has a regulated lower boundary at \(0\); this significantly complicates the analysis. They derive matrix-analytic expressions for the first return operator of the second fluid given a specific exponential form of the initial distribution and a certain boundary condition is met. Once again, \cite{bo2013b} together with this thesis, treat a more general model and together, give computational methods for various performance measures of fluid-fluid queues.

Returning now to \cite{bo2014} which introduces the fluid-fluid queue in its generality and derives operator-analytic expressions for the first-return operator of the second fluid and the joint stationary distribution. Their analysis is inspired by the analysis of classical fluid queues in \cite{bean2005} whereby matrix-analytic expressions for the fluid level are derived in terms of the generator of the driving process. The driving process of a fluid-fluid queue is a fluid queue. The analysis of \cite{bo2014} requires specific expressions about the transient distribution of the fluid queue which cannot, in general, be obtained from the existing literature.

\paragraph{Transient analysis of fluid queues} The analysis of fluid queues can be classified broadly into two approaches, matrix-analytic methods (for example, \cite{ajr2005,ar2003,ar2004,bean2005b,bean2005,bot08,bean2009,dasilva2005,latouche2018}) and differential equations based approaches (such as \cite{anick1982,kk1995,blnos2022}). Often, it is the stationary distribution of the fluid queue which is of interest. In the context of the analysis of fluid-fluid queue as in \cite{bo2014}, however, we require information about the transient distribution of the fluid queue. Specifically, on the event that the second fluid is non-decreasing (non-increasing), we need to know the amount of fluid to have flowed into or out of the second level process, the time (or Laplace transform of time) taken to do so, and the position of the first fluid and phase at the time when the in-out process of the second fluid reaches a given height. Moreover, we want the resulting expressions to be readily computable. None of the existing literature gives exact expressions which have all the aforementioned properties. Even if we consider a simpler model where the rates \(r_i(x)\) are constant on intervals \(\calD_{k,i}\), then the existing results are not satisfactory. Perhaps the closest is the work of \cite{bean2009} who compute expressions for the Laplace transform with respect to the time take for the fluid to exit an interval, \(\calD_{k,i}\), say, on the even that fluid exits in some phase \(j\), given it started within the interval at some point \(x_0\) and in some phase \(i\). These expressions could perhaps be used to analyse a fluid-fluid queue where the rates \(r_i(x)=r_k\) for all \(i\in\calS\), and \(x\in\calD_{k,i}\). Even with this simplified fluid-fluid queue, it is unclear how we could use these expressions to compute the position of the second fluid. 

An approximate method which is appropriate for the analysis of fluid-fluid queues has been proposed. The method of \cite{bo2013}, which we refer to as the uniformisation method as it is derived via uniformisation arguments, approximates a fluid queue by a discrete-time Markov chain. \cite{bo2013} approximate the fluid queue \(\{(\dot X(t),\varphi(t))\}\) by the quasi-birth-and-death-process \(\{(L(t),\varphi(t))\}\). In both processes, \(\{\varphi(t)\}\) is the same so there is no approximation in phase process. The level process \(\{L(t)\}\) approximates the level process \(\{\dot X(t)\}\). Specifically, \cite{bo2013} derive the level process such that the event that \(L(t)=k\) and \(\varphi(t)\) approximates the event \(\dot X(t)\in \calD_{k,i}\) and \(\varphi(t)=i\), where all the levels \(\{\calD_{k,i}\}_{k}\) have the same width, \(\Delta=y_{k+1}-y_k\). By considering arbitrarily small \(\Delta\), they prove that the distribution of the quasi-birth-and-death-process \(\{(L(t),\varphi(t))\}\) can be made arbitrarily close to the distribution of the fluid queue \(\{(\dot X(t),\varphi(t))\}\). Replacing the driving process of the fluid-fluid queue, \(\{(\dot X(t),\varphi(t))\}\), by the approximation \(\{(L(t),\varphi(t))\}\) and approximating the rate functions \(r_i(x)\) by constants on each interval \(\calD_k\) yields a classical fluid queue which, intuitively, approximates the fluid-fluid queue. \cite{bo2013} numerically investigate the ability of their approximation to approximate \(\bs \Psi_X\). To date, the uniformisation approximation has not been applied in the context of fluid-fluid queues, and its ability to approximate transient quantities of fluid queues (other than the aforementioned investigation of \(\bs\Psi_X\)) has not been well studied. In Chapter~\ref{sec: numerics} we investigate the numerical performance of the uniformisation approximation and show that it is effective. However, of the methods considered in Chapter~\ref{sec: numerics}, its performance is the poorest and its rate of convergence is the slowest for the numerical experiments we conduct. Though we should not rush to discount the uniformisation approximation. Due to the stochastic interpretation of the uniformisation approximation as a QBD, approximations to probabilities are guaranteed to be non-negative, and we can employ probabilistic techniques to analyse the approximation (as \cite{bo2013} did). Moreover, its simplicity lends itself more easily to mathematical analysis. Further, when the uniformisation scheme is used in the context of approximating a fluid-fluid queue the resultant approximation is itself a fluid queue, and this stochastic interpretation could aid in the analysis of this approximation. 

It turns out that the uniformisation approximation of \cite{bo2013} is equivalent to a certain finite-element approximation which is a specific application of the \emph{discontinuous Galerkin (DG) method}. We introduce the discontinuous Galerkin method and its application to fluid queues, and fluid-fluid queues, in Chapter~\ref{ch:galerkin}. DG approximation schemes have found great success for approximating solutions of partial differential equations, however, to my knowledge, they have not been applied to fluid queues, or fluid-fluid queue to date (except for our paper \cite{blnos2022}, of which I am a co-author and the relevant parts of that work are included in this thesis). The DG scheme has the advantage that its rate of convergence is rapid for smooth problems, however, it can produce oscillatory approximations and result in negative probabilities.

In Chapter~\ref{sec: construction and modelling} we attempt to derive a new approximation scheme which converges faster than the uniformisation scheme and still has a stochastic interpretation so that estimates of probability are guaranteed to be positive.


% Even if we assume \(r_i(x)\) is piecewise constant on intervals \(\calD_k\), say, then we still need expressions for the amount of time spent in each phase while the fluid level remains in \(\calD_k\). Analyses, such as \cite{bean2009}, derive expressions for the time spent in the intervals \(\calD_k\), but do not track how much time is spent in each phase. Moreover, 



% For example, Ramaswami CITE, analysed fluid queues by mapping them to a quasi-birth-and-death process (QBD), after which they applied known matrix-analytic methods for QBDs to compute quantities of interest. Anick Mitra Sondhi CITE, analysed fluid queues using a more direct differential equation-based method. Since Ramaswami's CITE initial work, there has been significant developments in the analysis of fluid queues CITE and related algorithms CITE. 

% The analysis of fluid-fluid queues of \cite{bo2014} is inspired by the matrix-analytic approach to fluid queues. In particular, the approach taken by Bean CITE THEIR FIRST HITTING TIME PAPER AND LATOUCHE AND DA SILVA SOARES. The approach requires 

% ...