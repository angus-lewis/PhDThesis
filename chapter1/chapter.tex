%!TEX root = ../thesis.tex
\chapter{The existing literature \& mathematical preliminaries\label{ch:math}} 
% intro to intro
% contextual background
	% Fluid queues 
		% definition
		% applications 
		% existing results 
	% Fluid-fluid queues
		% definition 
		% basic analysis in terms of generators
		% the need for approximation and how the old work doesn't fit here
	% The DG method
		% pertition into cells
		% project on to a basis
		% problems: oscillations
		% soutions and why they dont fit
		% observation about constant-basis methods
		% this thesis asks the question: motivated by the constant basis being a probability model, can we dream up a more accurate probability model to approximate a fluid queue?
	% On the structure of the constant basis/uniformisation method (QBD)
	% least variable is PH, so what about MEs?
% structure of the thesis
	% rest of chapter 1 gives mathematical preliminaries
	% chapter 2 explains the DG method, problems/oscillations, slope limiting, and appllication to SFFMs
	% inspired by the structure of the order-1 scheme, and to solve the negativity problem, chapter 3 introduces a new approximation method; The QBD-RAP. In this chapter we derive the intuitively of the model and explain it's dynamics.
	% we then move to convergence, with chapter 4 proving a convergence of the QBD-RAP up to the first hitting time of the fluid queue on the edges of an interval. technical results and extensions are left to the appendix: certain properties of closing vectors, convergence without ephemeral phases, and certain matrix algebraic manipulations. 
	% chapter 5 stitches together the results of chapter 4 to prove a global result about convergence
	% chapter 6 investigate, numerically, the performance of the DG, DG with limiter, uniformisation and QBD-RAP schemes. 
	% chapter 7  makes concluding remarks.
% mathematical preliminaries 
	% CTMCs
	% fluid queues
	% fluid-fluid queues and operator-analytic expressions (all of them)
		% the equations of bo2014
		% meets the partition of the DG/QBD-RAP, and reconstructing the Fil sets 
    % why we cant solve the equations, what we need to approximate to solve it, i.e. B, R, then D, Psi
	% projections
	% phase-type distributions 
		% definition
		% least variable property
	% matrix exponentials
		% definition 
		% properties 
		% verification that parameters give an ME
		% CMEs
	% QBD-RAPs, then QBDs as a special case
		% orbit processes, their interpretation for PH and how they differ for MEs
	% convergence theorems 
		% DCT 
		% Continuity of Laplace transforms

		
% THE ROUGH IDEA & CONTEXT
% 	THE CONTEXT: FLUID-FLUID QUEUES
%	PDEs 
%	NON-NEGATIVITY & CONSERVATION 
% MATHEMATICAL PRELIMINARIES
\section{Stochastic processes}
% \subsubsection{Random variables}
% Let \(\mathcal S\) be a set. 

% Roughly following \cite{billingsleyconvergence}, a random variable \(Z\), is a mapping between a \emph{sample space}, \(\mathcal S\), say, to the real numbers. 


% An \emph{event} is a set \(\{\omega \in \mathcal S \mid Z(\omega)\in E\}\equiv \{\omega\in Z^{-1}(E)\}\) where \(E\subseteq \mathcal S\) and \(Z^{-1}\) is the inverse image of the mapping \(Z\). Associated with a \(Z\) is a \emph{probability measure}, \(P\), which maps events to \([0,1]\) and has the properties \(P(\emptyset)=0\), \(P(S)=1\) and if . The notation \(P(\{\omega\in Z^{-1}(E)\})\) is cumbersome, so we write \(\mathbb P(Z\in E)\) in its place. 

% The sample space, \(\mathcal S\), is the set of all possible outcomes. Associated with the random variable \(Z\) is a \emph{probability measure} which assigns 


Following \cite{rossBook}, a stochastic process is a sequence, \(\{X(t)\}_{t\in\mathcal T}\), of random variables (or random vectors) indexed by some index set \(\mathcal T\). In this thesis, in the case that the index set is \(\mathcal T=\{0,1,2,...\}\) we say that the process \(\{X(n)\}_{n\in \mathcal T}\) is a discrete-time process, and we will typically use the dummy variable \(n\) for the `time' index. When the index set is \(\mathcal T=[0,\infty)\) we say \(\{X(t)\}_{t\in\mathcal T}\) is a continuous-time process, and we will typically use the dummy variable \(t\) for the `time' index. We may omit the index set and write \(\{X(t)\}\) in place of \(\{X(t)\}_{t\in\mathcal T}\) when it is not explicitly needed, or we may write \(\{X(t)\}_{t\geq t_0}\) to mean \(\{X(t)\}_{t\in[t_0,\infty)}\). The \emph{state space}, \(\mathcal S\), of \(X(t)\) is the set of possible values that \(X(t)\) can take at any time \(t\in\mathcal T\).

The \emph{initial distribution}, \(\mu\), of a stochastic process is the distribution of \(X(0)\). More generally, for a random variable, \(Z\), we write \(Z\sim \nu\) when \(Z\) has the distribution given by the probability measure \(\nu\). For the probability that \(X(t)\) lies in some measurable set \(E\subset \mathcal S\) given \(X(0)\sim \mu\), we write \(\mathbb P(X(t)\in E \mid X(0)\sim \mu)\). When \(\mu\) assigns probability 1 to a single point, \(x\in\calS\), say, we write \(\mathbb P(X(t)\in E \mid X(0)=x).\) 

A stochastic process is \emph{stationary} if, for any \(n\), \(t_0<t_1<...<t_n\) and any \(t\), then \(X(t_n),X(t_{n-1}),...,X(t_0)\) and \(X(t_n+t),X(t_{n-1}+t),...,X(t_0+t)\) have the same distribution. That is, 
\begin{align}\label{eqn: stationary}
	&\nonumber\mathbb P(X(t_n)\in E_n,X(t_{n-1})\in E_{n-1},...,X(t_0)\in E_0) 
	\\&= \mathbb P(X(t_n+t)\in E_n,X(t_{n-1}+t)\in E_{n-1},...,X(t_0+t)\in E_0),
\end{align}
for any \(t,t_0,...,t_n\in\mathcal T\) with \(t_0+t,...,t_n+t\in\mathcal T\) and any \(E_0,...,E_n\subseteq \mathcal S\). A random variable, \(\tau\), is a \emph{stopping time} for the stochastic process \(\{X(t)\}_{t\in\mathcal T}\) if \(\tau\in\sigma(X(s), s\leq t),\) where \(\sigma(X(s), s\leq t)\) denotes the \(\sigma\)-algebra generated by \(\{X(s), s\leq t\}\).

As in \cite[Chapter~1.2]{MEinAP}, we call \(\{X(n)\}_{n\in\{0,1,2,\dots\}}\) a \emph{discrete-time Markov chain} if \(\calS\) is countable and
\begin{align}\label{eqn: Markov property}
	\nonumber &\mathbb P(X(n+1)=j \mid X(n)=i,X(n-1)=i_{n-1},...,X(0)=i_0) 
	\\&= \mathbb P(X(n+1)=j \mid X(n)=i) 
\end{align}
for all \(n\in\{0,1,2,...\}\) and \(i,i_0,...,i_{n-1},j\in\calS,\) and referred to (\ref{eqn: Markov property}) as the \emph{Markov property}. The process \(\{X(n)\}_{n\in\{0,1,2,...\}}\) is said to be time-homogeneous if 
\(\mathbb P(X(n+1)=j \mid X(n)=i)\) does not depend on \(n\). The probabilities \(\mathbb P(X(n+1)=j \mid X(n)=i)=:p_{ij}\) are the \emph{transition probabilities}, and \(\bs P=[p_{ij}]_{i,j\in\calS}\) is the \emph{transition matrix}, of the Markov chain. The \emph{strong Markov property} states that for each stopping time \(\tau\) of the Markov chain \(\{X(n)\}_{n\in\{0,1,2,...\}}\), on the event that \(\{\tau<\infty\}\), then 
\begin{align}\label{eqn: strong Markov property}
	\mathbb P(X(\tau+1)=j \mid \sigma(X(0),X(1),...,X(\tau))) = \mathbb P(X(\tau+1)=j \mid X(\tau)).
\end{align}

Also, as in \cite[Section~1.3]{MEinAP}, we call \(\{X(t)\}_{t\geq 0}\) a \emph{continuous-time Markov chain} (CTMC) if \(\calS\) is countable and for all \(t_{n+1} > t_n > t_{n-1} > ... > t_0 \geq 0\) and \(i,i_0,...,i_{n-1},j\in\calS,\)
\begin{align}\label{eqn: Markov property cmtc}
	\mathbb P(X(t_{n+1})=j \mid X(t_n)=i, X(t_{n-1})=i_{n-1},...,X(t_0)=i_0) = \mathbb P(X(t_{n+1})=j \mid X(t_n)=i),
\end{align}
and refer to (\ref{eqn: Markov property cmtc}) as the \emph{Markov property}. The process \(\{X(t)\}_{t\geq 0}\) is said to be time-homogeneous if 
\(\mathbb P(X(t+s)=j \mid X(s)=i)\) does not depend on \(s\). For a time-homogeneous CTMC, the \emph{transition function} is the matrix function 
\[\bs P(t) = [\mathbb P(X(t)=j \mid X(t)=i)]_{i,j\in\calS}.\]
The Chapman-Kolmogorov equations state that 
\begin{align} 
	&\nonumber \bs P(t+s)=\bs P(t)\bs P(s).
\end{align}
The \emph{infinitesimal generator} matrix of a CTMC is 
\begin{align} 
	&\nonumber \bs T = [T_{ij}]_{i,j\in\calS} = \cfrac{\wrt}{\wrt t}P(t),
\end{align}
and the elements \(T_{ij}\) are the \emph{transition rates}. A CTMC is said to be \emph{honest} if \(\bs P(t)\bs 1=\bs 1\) for all \(t\geq 0\) where \(\bs 1\) is a column vector of ones. 

The \emph{strong Markov property} states that for each stopping time \(\tau\) of the Markov chain \(\{X(t)\}_{t\geq 0}\), on the event that \(\{\tau<\infty\}\), then 
\begin{align}\label{eqn: strong Markov property ctmc}
	\mathbb P(X(\tau + t)=j \mid \sigma(X(s),s\leq \tau)) = \mathbb P(X(\tau+t)=j \mid X(\tau)).
\end{align}

Most generally, we call \(\{X(t)\}_{t\geq 0}\) a \emph{Markov process} if for all \(t_n > t_{n-1} > ... > t_0 \geq 0,\, t_k\in\mathcal T,\, k=0,...,n,\) and \(E_0,...,E_n\subseteq \mathcal S\)
\begin{align}\label{eqn: Markov process}
	\mathbb P(X(t_n)\in E_n,\mid X(t_{n-1})\in E_{n-1},...,X(t_0)\in E_0) 
	= \mathbb P(X(t_n)\in E_n\mid X(t_{n-1})),
\end{align}
and we say that it is time homogeneous if \(\mathbb P(X(t+s)\in E_n\mid X(s))\) does not depend on \(s\). 

The Chapman-Kolmogorov equations state that 
\begin{align}\label{eqn: CK}
	&\nonumber \mathbb P(X(t+s)\in E\mid X(0)\sim \mu) 
	\\&= \int_{x\in\calS}\mathbb P(X(t+s)\in E\mid X(t)=x)\mathbb P(X(t)\in \wrt x\mid X(0)\sim \mu).
\end{align}
% Markov processes, time-homogeneity, strong Markov prperty, partitioning and LOTP

\section{Some semigroup theory}
The evolution of Markov processes can be described by an \emph{operator semigroup}. Semigroups also arise in other contexts related to the analysis of fluid queues and fluid-fluid queues (see, for example, Sections~\ref{sec: ffq intro} and~\ref{sec: transient ffq intro}). Semigroups (and therefore Markov processes) can be characterised by their \emph{infinitesimal generator}, or generator for short. One of the aims of this thesis is to approximate the generator of a fluid queue. Here we very briefly introduce semigroups and their \emph{infinitesimal generators}. We refer to the reader to \cite{ethierkurtz} for more details (see also \cite{kallenberg}, or the more approachable course notes of \cite{shalizi}). 

\subsubsection{Operators}
Let \(L\) be a \emph{Banach space}. A Banach space is a \emph{complete, normed} vector space. Complete means that every Cauchy sequence of points in \(L\) has a limit which also lies in \(L\). Normed means that a norm is defined on \(L\). Intuitively, a norm is a function which tells us the size of a vector in \(L\), hence we can use a norm to compute a distance between two points \(f,g\in L\) by computing the norm of \(f-g\). 

An \emph{operator} is a mapping which takes elements of a vector space and sends them to elements of a vector space (not necessarily the same space). That is, let \(V\) and \(W\) be normed vector spaces, then \(\mathbb B:\mathcal D(\mathbb B)\to W\) is an operator. The set \(\mathcal D(\mathbb B)\subseteq V\) is the domain of the operator, and is not necessarily all of \(V\). When specification of the domain is unnecessary we sometimes write \(\mathbb B:V\to W\). 

Let \(c\in \mathbb R\) (\(\mathbb R\) is the reals), \(v\in V\) and \(w\in W\). Then \(\mathbb B\) is a \emph{linear} operator if \(\mathbb B(cv) = c(\mathbb Bv)\) and \(\mathbb B(v+w)=\mathbb Bv+\mathbb Bw\). We define the norm of an operator as \(||\mathbb B||=\sup_{v\in V}\cfrac{||\mathbb Bv||_W}{||v||_V}\), where \(||\cdot||_V\) and \(||\cdot||_W\) are the norms defined on \(V\) and \(W\), respectively.  An operator is said to be bounded if \(||\mathbb B||<M\) for some \(M<\infty\). 

% For example, let \(L\) be the space of real, smooth functions for which \(f(x)\to 0\) as \(x\to \infty\), and consider the sup norm, \(||f||=\sup\limits_{x\in\mathbb R}f(x)\). Then differentiation is an operator which maps \(L\) to it self. That is \(B=\cfrac{\wrt}{\wrt x}:L\to L\). In addition \(B\) is linear since for any \(f,g\in L\), \(B(f+g) = \cfrac{\wrt}{\wrt x}(f(x)+g(x)) =  \cfrac{\wrt}{\wrt x}f(x) +  \cfrac{\wrt}{\wrt x}g(x)\) and \(B(cf)= \cfrac{\wrt}{\wrt x}cf(x)=c \cfrac{\wrt}{\wrt x}f(x)\). The operator \(B\) is, however, not bounded.

% The \emph{graph} of \(\mathbb B\) is defined as the linear subspace of \(L\times L\), \(\mathcal G(B)=\{(f,Bf):f\in \mathcal D(B)\}\subset V\times W\}\). The operator \(B\) is said to be closed if \(\mathcal G(B)\) is a closed subset of \(V\times W\). That is, for every sequence \(\{f_n\}\) in \(\mathcal D(B)\) such that \(f_n\to f\) and \(Bf_n=g_n\to g \), then \(f\in\mathcal D(B)\) and \(Bf = g\). Note that we must have both \(\{f_n\}\) and \(\{g_n\}\) converge. 

% For example, let \(B=\cfrac{\wrt}{\wrt x}:C^1[0,1]\subset C^0[0,1]\to C^0[0,1]\), where \(C^n[0,1]\) is the set of all \(n\)-times continuously differentiable functions on the interval \([0,1]\). The graph of \(B\) is the set \(G(B)=\{(f,Bf); f\in C^1[0,1]\}\). The operator \(B\) is closed since for any \(f_n\in C^1[0,1]\) with \(f_n\to f\) and \(Bf_n = \cfrac{\wrt f_n}{\wrt x}=g_n\to g\), then \(f\in C^1[0,1]=\mathcal D(B)\) and \(Bf=g\). Notice that we can construct \(\{f_n\}\) such that \(f_n\to f\notin \mathcal D(B)\) (e.g.~from the Weierstrass Approximation Theorem there is a sequence of polynomials in \(C^1[0,1]\) converging to \(f(x)=|x-1/2|\notin C^1[0,1]\)) but in these cases \(Bf_n = \cfrac{\wrt f_n}{\wrt x}=g_n\) must not converge. 

\subsubsection{Semigroups}  A family of linear operators is a collection of indexed linear operators \(\{\mathbb B(i)\}_{i\in I}\) where \(I\) is some index set. For each \(i\in I\), \(\mathbb B(i)\) is a linear operator. A family of operators \(\{\mathbb T(t)\}_{t\geq 0}\), \(\mathbb T(t):L\to L\), is said to have the \emph{semigroup} property if \(\mathbb T(t+s)=\mathbb T(t)\mathbb T(s)\). If \(\mathbb T(0)=I\), the identity operator, then the family \(\{\mathbb T(t)_{t\geq 0}\}\) is known as an operator semigroup. If, for all \(f\in L,\, \mathbb T(t)f\to f\) as \(t\to 0^+\), then \(\{\mathbb T(t)\}_{t\geq 0}\) is said to be strongly continuous. If \(||\mathbb T(t)||\leq 1\) for all \(t\geq 0\) then \(\{\mathbb T(t)\}_{t\geq 0}\) is known as a contraction semigroup.

For a bounded linear operator \(\mathbb B:\mathcal D(\mathbb B)\to \mathcal D(\mathbb B)\), define the operator exponential as \(e^{\mathbb Bt}:=\sum_{k=0}^\infty\cfrac{1}{k!}t^k\mathbb B^k\) where, for \(f\in \mathcal D(\mathbb B)\), \(\mathbb B^kf\) is defined recursively by \(\mathbb B^k f = \mathbb B^{k-1} (\mathbb B f)\). It can be shown that \(e^{\mathbb B(t+s)}=e^{\mathbb Bt}e^{\mathbb Bs}\), and from the definition \(e^{\mathbb B0}=I\), which implies that \(\{e^{\mathbb Bt}:t\geq 0\}\) is a semigroup. As another example, let \(\mathbb B=\cfrac{d}{dx}:C^\omega(\mathbb R) \to C^\omega(\mathbb R)\), where \(C^\omega(\mathbb R)\) is the class of analytic functions on the whole of \(\mathbb R\). Then \(\mathbb T(t)f(x)=\sum_{k=0}^\infty \cfrac{1}{k!}t^k\cfrac{\wrt^k}{\wrt x^k}f(x)=f(x-t)\), and the sum converges since \(f\in C^\omega(\mathbb R)\); the series representation is the Taylor series of \(f\) about the point \(x\), and we see that the operator \(\mathbb T(t)f(x)=f(x-t)\) is the shift operator. The semigroup property follows since 
\[\mathbb T(t)\mathbb T(s)f(x)=\mathbb T(t)f(x-s)=f(x-s-t)=\mathbb T(t+s)f(x).\]

\subsubsection{Infinitesimal generators} The \emph{infinitesimal generator} (or just \emph{generator} for short) of a semigroup \(\{\mathbb T(t)\}\) is the linear operator defined by 
\[\mathbb Bf = \lim\limits_{t\to 0^+}\frac{1}{t}(\mathbb T(t)f-f),\]
and the domain, \(\mathcal D(\mathbb B)\), is the subspace of \(L\) for which this limit exists. Note that an essential part of the definition of the generator is its domain. For this reason it is common to denote the generator as the pair \((\mathbb B,\mathcal D(\mathbb B))\). 

To determine the generator \(\mathbb B\) we can proceed by either differentiation of the semigroup, 
\[\mathbb Bf = \lim\limits_{t\to 0^+}\frac{1}{t}(\mathbb T(t)f-f),\]
or integration;
\[\int_{t=0}^\infty e^{-st}\mathbb T(t)\wrt t = (sI-\mathbb B)^{-1}=:R_s.\]
The operator \(R_s\) is known as the resolvent. 

One of the fundamental results of semigroup theory is the Hille-Yosida Theorem which essentially states that a semigroup is entirely characterised by its generator. This is not immediately obvious as the generator is defined on a subset of \(L\) only, whereas \(\{\mathbb T(t)\}\) is defined on all of \(L\).

\subsubsection{Markov processes and semigroups}
Semigroups arise naturally in Markov processes. Let \(\{X(t):t\geq 0\}\) be a Markov process with state space \(\mathcal S\). The Markov property states that, given \(X(t)\), then for \(s,t\geq 0\), \(X(t+s)\) is independent of \(X(u),\, 0\leq u<t\). That is, \[P(X(t+s)\in\mathcal{E} \mid X(t), X(u), 0\leq u<t)=P(X(t+s)\in\mathcal{E} \mid X(t)).\] 
If \(P(X(t+s)\in\mathcal{E} \mid X(t))=P(X(s)\in\mathcal{E} \mid X(0))\), are invariant under translation by \(t\), then \(X(t)\) is said to be time-homogeneous.

% Markov processes can be characterised by \emph{probability kernels}. Let \((U,\mathcal U)\) and \((V,\mathcal V)\) be two measure spaces, then a probability kernel \(\nu\), is a mapping \(\nu:U\times \mathcal V\to \mathbb R_+\) such that \(\nu(u,\mathcal{E})\) is \(\mathcal U\)-measurable in \(u\in U\) for fixed \(\mathcal{E}\in \mathcal V\), and a probability measure in \(\mathcal{E}\in \mathcal V\) for fixed \(u\in U\). We say that \(\nu\) is a probability kernel from \(U\) to \(V\). 

% With a probability kernel comes associated integral operators. For a measure \(\mu\) on \((U,\mathcal U)\), then 
% \[\mu \nu (\mathcal{E})=\int_{U} \mu(\wrt x) \nu(x,\mathcal{E})\]
% is also a measure on \((U,\mathcal U)\). For a function \(f:V\to \mathbb R\) then 
% \[\nu f (x)=\int_{V}  \nu(x,\wrt y)f(y).\] 
% We can also compose two transition kernels. Let \(\nu\) be a transition kernel from \(U\) to \(V\) and \(\eta\) be a transition kernel from \(V\) to \(W\). Then \(\nu\eta\) is also a probability kernel from \(U\) to \(W\);
% \[\nu\eta(x,\mathcal{E}) = \int_{V}\nu(x,\wrt y)\eta(y,\mathcal{E}).\]
 
Consider the expectation
\[\mathbb E[f(X(t))\mid X(0)=x]=\int_{y\in\calS} f(y) P(X(t)\in\wrt y\mid X(0)=x),\]
where \(f\) is some bounded function, \(|f|<F\). We can think of expectation as an operator which is acting on the function \(f\). The expectation depends on \(t\) and \(x\) as these define the distribution with which we are taking the expectation, hence, let us write 
\[\mathbb T(t)f(x) = \mathbb E[f(X(t))\mid X(0)=x].\]
The result of the operator acting on \(f\), \(\mathbb T(t)f\), is another function (a function of the initial point \(x\)).

By the Markov property, for any \(s\in[0,t]\), 
\begin{align*}
	\mathbb T(t)f(x)&=\int_{y\in\calS} f(y) P(X(t)\in\wrt y \mid X(0)=x)
	\\&=\int_{y\in\calS}\int_{z\in\mathcal S}f(y) P(X(t)\in\wrt y\mid X(s)=z) P(X(s)\in\wrt z\mid X(0)=x)
	\\&=\int_{z\in\mathcal S}\mathbb E[f(X(t))\mid X(s)=z] P(X(s)\in\wrt z\mid X(0)=x)
	\\&=\int_{z\in\mathcal S} \mathbb E[f(X(t-s))\mid X(0)=z]P(X(s)\in\wrt z\mid X(0)=x)
	\\&=\int_{z\in\mathcal S} (\mathbb T(t-s)f(z))P(X(s)\in\wrt z\mid X(0)=x)
	\\&=\mathbb T(s) (\mathbb T(t-s)f)(x),
\end{align*}
where the third equality holds by time-homogeneity. Hence, \(\{\mathbb T(t)(\cdot)\}_{t\geq 0}\) is an operator semigroup. Moreover, since \(f\) is bounded, then \(\mathbb T(t)f(x) = \mathbb E[f(X(t))\mid X(0)=x] \leq F\), which means that \(\{\mathbb T(t)\}\) is a contraction semigroup.

% Let \(\nu_{t}(x,\cdot)=P(X(t)\in\cdot \mid X(0)=x)\) for \(t\geq 0\) be a one-parameter family of probability kernels from \(\mathcal S\) to \(\mathcal S\). Then \(\mu_t\) and \(\nu_{t}\) characterise the time-homogeneous Markov process \(X(t)\). The probability kernel \(\nu_t\) is referred to as a transition kernel. The kernels \(\nu_t\) form a semigroup since, using the law of total probability and time-homogeneity, 
% \begin{align*}
% \nu_{t+s}(x,\mathcal{E})&=P(X(t+s)\in\mathcal{E}\mid X(0)=x)
% \\&=\int_{y\in\mathcal S}P(X(t)\in\wrt y\mid X(0)=x)P(X(t+s)\in\mathcal{E}\mid X(t)=y)
% \\&=\int_{y\in\mathcal S}P(X(t)\in\wrt y\mid X(0)=x)P(X(s)\in\mathcal{E}\mid X(0)=y)
% \\&=\int_{y\in\mathcal S}\nu_t(x,\wrt y)\nu_s(y,\mathcal{E})=\nu_t\nu_s(x,\mathcal{E}).
% \end{align*}

% We can also construct other semigroups from \(\nu_t\). First we need dome more definitions. Let \((\mathcal S,\mathcal F)\) be a measure space. A \emph{Markov operator on measures} is an operator \(M\) which takes finite measures on \((\mathcal S,\mathcal F)\) to finite measures on \((\mathcal S,\mathcal F)\) such that \(M\) is linear and for a finite measure \(\mu\), \(\mu M(\mathcal S)=\mu (\mathcal S)\). We also have the following. Let \((\mathcal S,\mathcal F, \mu)\) be a measure space and let \(L_1\) be the class of all \(\mu\)-integrable generalised functions on \(\mathcal S\). A linear operator \(P:L_1\to L_1\) is a \emph{Markov operator on densities} when; if \(f\geq 0\) almost everywhere with respect to \(\mu\), then \(fP \geq 0\) almost everywhere with respect to \(\mu\); and if \(f\geq 0\) almost everywhere with respect to \(\mu\), then \(||fP|| = ||f||\). 
% \begin{lem}
% For a Markov operator \(M\) and measure \(\mu <<\lambda\) (absolutely continuous with respect to Lebesgue measure), if \(\mu M <<\lambda\), then \(M\) induces an almost-unique Markov operator on densities \(P\) with respect to \(\mu\). 
% \end{lem}
% Let \((\mathcal S,\mathcal F)\) be a measure space and let \(B(\mathcal S)\) be the class of bounded measurable functions. An operator \(K:B(\mathcal S)\to B(\mathcal S)\) is a \emph{transition operator} when \(K\) is linear and for \(f\), \(\{f_n\}\in B(\mathcal S)\), if \(f\geq 0\) almost everywhere with respect to a measure \(\mu\), \(Kf\geq 0\) almost everywhere with respect to \(\mu\), \(K1(\mathcal S)=1(\mathcal S)\) and if \(f_n\to 0^+\), then \(Kf_n\to 0^+\). 
% \begin{lem}
% Every transition probability kernel \(\nu\) induces a Markov operator on measures, \(\mu M(\mathcal{E}) = \mu \nu(\mathcal{E})\), and a transition operator on functions, \(Kf(x)=\nu f(x)\). In the other direction, every Markov operator induces a probability kernel \(\delta(x)M(\mathcal{E})=\nu(x,\mathcal{E})\), and every transition operator induces a transition kernel \(\nu(x,\mathcal{E})=K1(\mathcal{E})(x)\).
% \end{lem}

% We can now use Markov operators and transition operators to construct semigroups from the transition kernel semigroup. A transition semigroup is the semigroup \(K(t)f(x) = \nu_tf(x)\). Similarly we can construct the Markov semigroup, \(\mu M(t)(\mathcal{E}) = \mu \nu_t (\mathcal{E})\) for measures and \(fM(t)(\wrt y) = f\nu_t (\wrt y)\) for densities. 

% \subsubsection{Feller processes}
% Feller processes are Markov processes with particularly nice transition semigroups. A continuous-time homogeneous Markov process \(\{X(t):t\geq 0\}\) is a Feller process when, for all \(x\in\mathcal S\) 
% \begin{align}
% y\to x&\implies X_y(t)\xrightarrow[]{d} X_x(t),\, \forall t\geq 0,
% \\t \to 0 &\implies X_x(t)\xrightarrow[]{P}x, 
% \end{align}
% where \(\xrightarrow[]{d}\) denotes convergence in distribution and \(\xrightarrow[]{P}\) denotes convergence in probability. It can be shown that these probabilistic properties are equivalent to conditions on the transition semigroup. A semigroup of linear, positive (maps positive functions to positive functions), conservative (\(K(t)1(\mathcal S) = 1(\mathcal S)\)), contractions, is a Feller semigroup if for every \(f\in C_0\) (continuous function for which \(|x|\to \infty  \implies f(x)\to 0\)), and every \(x\in \mathcal S\), 
% \begin{align*}
% K(t)f&\in C_0,\\
% \lim_{t\to 0^+}K(t)f(x) &= f(x).
% \end{align*}
% \begin{thm}
% A Markov process is a Feller process if and only if its evolution operators form a Feller semigroup. 
% \end{thm}

% It is a fact that transition operators and Markov operators on densities are \emph{adjoint} operators. Therefore in many cases we can work with transition operators and carry the results over to Markov operators on densities, or \emph{vice-versa}.

% \subsubsection{A long story short}
% A time homogeneous Markov processes can be defined by a transition kernel, \(\nu_t(x,\mathcal{E})=P(X(t)\in\mathcal{E}\mid X(0)=x)\) and an initial distribution \(\mu(\mathcal{E})=P(X(0)\in\mathcal{E})\). The operators \(\nu_t\) form a semigroup due to the Markov property and time homogeneity. We can construct other semigroups from \(\nu_t\) by either integrating measures on the left, \(\mu\nu_t(\mathcal{E})=:\mu M(t)(\mathcal{E})\) (which we will call Markov semigroups), or by integrating functions on the right, \(\nu_tf(x) =: K(t)f(x)\). The latter is sometimes known as the transition semigroup, \(K(t):L\to L\) where \(L\) is a Banach space. These semigroup operators are adjoint, so results from transition semigroups may be able to be translated to results for Markov semigroups. 

% Feller processes are Markov processes with nice properties. It can be shown that a process is a Feller process if and only if its transition semigroup is a Feller semigroup. A Feller semigroup is a semigroup which maps continuous functions to continuous functions and is strongly continuous, \(\lim\limits_{t\to 0^+}K(t)f(x)=f(x)\).

% Under certain conditions, a semigroup, (and hence also a Feller process), can be characterised by its generator, \(B\) defined by;
% \[Bf:=\lim_{t\to 0^+} \frac{1}{t}(K(t)-I)f,\]
% on the set of functions \(f\) for which this limit exists. This set is known as the domain of the generator \(\mathcal D(B)\), which is not necessarily all of \(L\). The domain of \(B\) is a very important part of its definition, and some authors highlight this by writing the generator as the pair \((B,\mathcal D(B))\). 

% On \(\mathcal D\left(\bigcup\limits_{n=0}^\infty B^n\right)\) the semigroup with generator \(B\) is given by the operator exponential 
% \[e^{Bt}=\sum_{k=0}^\infty \frac{t^kB^k}{k!} \mbox{ for }t\geq 0.\]

% It may be surprising that the semigroup \(K(t)\), which is defined on all of \(L\), can be characterised entirely in term of its generator \(B\), which is only defined on \(\mathcal D(B)\) and furthermore, only defines \(K(t)\) in terms of an operator exponential on \(\mathcal D\left(\bigcup\limits_{n=0}^\infty B^n\right)\). A step in the direction of characterisation of \(K(t)\) in terms of its generator is the fact that, when \(\{K(t)\}\) is strongly continuous, \(\mathcal D(B)\) is dense in \(L\).

% The Hille-Yosida Theorem gives necessary and sufficient conditions for \((B,\mathcal D(B))\) to be the generator of a strongly continuous semigroup. A key element in the proof of the Hille-Yosida Theorem is the Yosida approximation given by 
% \[B_\lambda = \lambda B(\lambda - B)^{-1},\]
% where \((\lambda - B)^{-1}\) is the resolvent of \(B\). It can be shown that the domain of \((\lambda - B)^{-1}=L\) for every \(\lambda >0\), that \(\{e^{tB_\lambda}\}\) is a strongly continuous semigroup of \(L\), and that \(B_\lambda \to B\) as \(\lambda \to \infty\). Hille and Yosida were able to use these facts to determine necessary and sufficient conditions in term of \(B\), for \((B,\mathcal D(B))\) to generate a strongly continuous semigroup. Furthermore, since \(R_\lambda\) is one-to-one with domain \(L\) and range \(\mathcal D(B)\), then there is a correspondence between elements in \(\mathcal D(B)\) and \(L\). That is, we can represent any element of \(\mathcal D(B)\) as \(R_\lambda f\) for some \(f\in L\). This can be useful to characterise the domain, \(\mathcal D(B)\).

\section{Fluid queues}\label{sec: fqs}
An unbounded fluid queue is a two-dimensional stochastic process which we denote by \(\{\ddot{\bs X}(t)\} = \{(\ddot X(t),\varphi(t))\}_{t\geq0}\) where \(\{\varphi(t)\}_{t\geq0}\) is known as the phase or driving process, and \(\{\ddot X(t)\}_{t\geq0}\) is known as the level process or buffer. The phase process \(\{\varphi(t)\}_{t\geq0}\), is an irreducible continuous-time Markov chain (CTMC) with finite state space, which we assume to be \(\mathcal S=\{1,2,\dots,N\}\), and infinitesimal generator \(\bs T= [T_{ij}]_{i,j\in\mathcal S}\). We assume that \(\bs T\) is {conservative} and time-homogeneous. Associated with states \(i\in\mathcal S\) are real-valued \emph{rates} \(c_i\in\mathbb R\) which determine the rate at which \(\{X(t)\}\) moves. 

Partition the state space \(\calS\) into \(\calS_+ = \{i\in\calS\mid c_i>0\}\), \(\calS_- = \{i\in\calS\mid c_i<0\}\), \(\calS_0 = \{i\in\calS\mid c_i=0\}\), \(\mathcal S_{-1}=\{i\in\calS\mid c_i\leq0\},\, \mathcal S_{K+1}=\{i\in\calS\mid c_i\geq0\}\)\footnote{The notation \(\mathcal S_{-1}\) and \(\mathcal S_{K+1}\) will make sense later when we introduce the partition into cells for the approximation schemes into \(K\) cells, plus a lower and upper boundary which we represent with indices \(-1\) and \(K+1\) respectively.}. We assume, without loss of generality, that the generator \(\bs T\) is partitioned into sub-matrices
\[\bs T = \left[\begin{array}{ccc}\bs T_{++} & \bs T_{+-} & \bs T_{+0} \\ \bs T_{-+} & \bs T_{--} & \bs T_{-0} \\ \bs T_{0+} & \bs T_{0-} & \bs T_{00}  \end{array}\right],\]
where \(\bs T_{mn} = [T_{ij}]_{i\in\mathcal S_m, j\in\mathcal S_n}\), \(m,n\in\{+,-,0\}\).

{Let's clarify some notation. We use the notation \(\bs u = (u_h)_{h\in \mathcal H}\) to denote a row-vector, \(\bs u\), defined by its elements, \(u_h\), indexed by \(h\in\mathcal H\), where \(\mathcal H\) is some index set. Similarly, \(\bs u = (\bs u_h)_{h\in\mathcal H}\), is a row-vector defined by a collection of row-vectors \(\bs u_h\). The notation \(\bs u_m=(u_h)_{h\in\mathcal H_m}\) refers to the vector containing the subset of elements corresponding to \(\mathcal H_m\subseteq \mathcal H\). When the index set is empty, the resulting vector \(\bs u_m\) is a vector of dimension 0. In cases when there are two indices, we order the elements of the vector according to the first index, then the second; i.e.~\(\bs u = (u_{g}^h)_{g\in\mathcal G,h\in\mathcal H} = ((u_g^h)_{g\in\mathcal G})_{h\in\mathcal H}\). Here we use the convention that for a vector \(\bs u=(u)_{h\in \mathcal H}\) where the elements \(u\) do not depend on the index \(h\) and \(H\) is some index set, then we repeat \(u\) \(h\)-times; i.e.~\(\bs u = (u)_{h\in \mathcal H} =\underbrace{(u,\dots,u)}_{h-\mbox{times}}\). The notation \(\bs U = [u_{gh}]_{g\in\mathcal G, h\in\mathcal H}\) (square brackets) is used to denote a matrix defined by its elements, or sub-blocks, \(u_{gh}\).} 

Also define the diagonal matrices 
\begin{align*}
	\bs C &= \left[\begin{array}{ccc} \bs C_+ && \\ &\bs C_-& \\ && \bs 0\end{array}\right], && \bs C_+ = diag(c_i,i\in\calS_+), && \bs C_- = diag(|c_i|,i\in\calS_-),
\end{align*}
and 
\(
	\widehat{\bs C} = diag(c_i,i\in\calS),
\)
where \(diag(a_i,i\in\mathcal I)\) denotes a diagonal matrix with entries \(a_i\) down the diagonal. 

Now, back to fluid queues. The level process is given by 
\[\ddot X(t) = \ddot X(0) + \int_{s=0}^t c_{\varphi(s)}\wrt s.\]
Sample paths of \(\{\ddot X (t)\}\) are continuous and piecewise linear, with \(\cfrac{\wrt }{\wrt t} \ddot X(t) = c_\varphi(t)\), when \(\ddot X(t)\) is differentiable. Given sample paths of \(\{\varphi(t)\}\), then \(\{\ddot X(t)\}\) is deterministic, and in this sense, \(\{\varphi(t)\}\) is the only stochastic element of the fluid queue. 

Often, boundary conditions are imposed. We denote a fluid queue bounded below at \(0\) and unbounded above by \(\{\dot{\bs X}(t)\} = \{(\dot X(t),\varphi(t))\}_{t\geq0}\), and a fluid queue bounded below at \(0\) and above at \(b<\infty\) by \(\{{\bs X}(t)\} = \{( X(t),\varphi(t))\}_{t\geq0}\). Here, we consider a mixture of \emph{regulated} and \emph{reflecting} boundary conditions. Upon hitting a boundary we suppose that, with probability \(p_{ij},\,i,j\in\mathcal S\), the phase process instantaneously transitions from phase \(i\) to phase \(j\) (note that we might have \(i=j\) i.e.~no transition) and if \(sign(c_i)=sign(c_j)\) or \(sign(c_j)=0\) then the process is absorbed in the boundary, otherwise it is reflected. At a lower boundary, if \(j\in\calS_0\cup\calS_-\), then \(\cfrac{\wrt}{\wrt t} X(t) = 0\), and the phase process continues to evolve according to the sub-generator 
\[\left[\begin{array}{cc} \bs T_{--} & \bs T_{-0} \\ \bs T_{0-} & \bs T_{00}  \end{array}\right],\]
until such a time that \(\{\varphi(t)\}\) transitions to a phase \(k\in\calS_+\), at which time \(\{X(t)\}\) leaves the boundary. Similarly, at an upper boundary if \(j\in\calS_0\cup\calS_+\), then \(\cfrac{\wrt}{\wrt t} X(t) = 0\) and the phase process continues to evolve according to the sub-generator 
\[\left[\begin{array}{cc} \bs T_{++} & \bs T_{+0} \\ \bs T_{0+} & \bs T_{00}  \end{array}\right],\]
until such a time that \(\{\varphi(t)\}\) transitions to a phase \(k\in\calS_-\) at which time \(\{X(t)\}\) leaves the boundary. It is without loss of generality that we assume the lower and upper boundaries (when present) are at \(x=0\) and \(x=b>0\), respectively.

In summary, the evolution of the level can be expressed as 
\[\cfrac{\wrt}{\wrt t} X(t) = \begin{cases} c_{\varphi(t)}, & \mbox{ if } X(t)>0, \\ \max\{0,c_{\varphi(t)}\}, & \mbox{ if } X(t)=0, \\ \min\{0,c_{\varphi(t)}\}, & \mbox{ if } X(t)=b.  \end{cases}\]

Let \(\bs f(x,t) = (f_i(x,t))_{i\in\calS}\) be a row-vector function where \(f_i(x,t)\) is the density of \(\mathbb P(X(t)\leq x, \varphi(t) = i\mid \bs X(0)\sim \bs \mu )\), assuming it exists. When a differentiable density exists, the system of partial differential equations which describes the evolution of the densities \(\bs f(x,t)\) is 
\begin{align}
	\cfrac{\partial}{\partial t} \bs f(x,t) = \bs f(x,t)\bs T - \cfrac{\partial}{\partial x}\bs f(x,t)\widehat{\bs C},\label{eqn: pde}
\end{align}
on the interior \(x\in(0,b)\), with appropriate boundary conditions (see Section~\ref{subsec: boundary DG}). The initial condition is the initial distribution of the fluid queue which, when it has a density, we write as \(f_i(x,0)\). Often a differentiable density function does not exist and therefore the partial differential equation~(\ref{eqn: pde}) is not well-defined. For example, for a fluid queue with no upper boundary, if the initial distribution of the fluid queue is a point mass at any point \(x_0\geq 0\) and in phase \(i\in\calS_+\cup\calS_0\), then a density function \(f_i(x,t)\) will not exist for any finite \(t\). Specifically, a point mass will persist along the ray \(x_0+c_it\), \(t\geq 0\). In such situations, it is the \emph{weak solution} to (\ref{eqn: pde}) that we seek. A weak solution (ignoring boundary conditions) satisfies
\begin{align}
	-\int_{x=0}^b\int_{t=0}^\infty \bs f(x,t)\cfrac{\partial}{\partial t} \bs \psi(x,t)\wrt t \wrt x= &\int_{x=0}^b\int_{t=0}^\infty\bs f(x,t)\bs T\bs \psi(x,t)\wrt t \wrt x \nonumber 
	\\&{} + \int_{x=0}^b \int_{t=0}^\infty\bs f(x,t)\widehat{\bs C} \cfrac{\partial}{\partial x} \bs \psi(x,t)\wrt t \wrt x,\label{eqn: weak pde}
\end{align}
for every row-vector of test functions, \(\bs \psi(x,t) = ( \psi_i(x,t))_{i\in\calS}\), which are smooth, have compact support and \(\bs \psi(x,0)=\bs \psi(0,t) = \bs \psi(b,t)=\bs 0\) \cite[Chapter~10]{borthwickBook}. 

\subsection{Transient analysis of fluid queues}\label{sec: transient ffq intro}
The transient analysis of fluid queues has been relatively well studied (see, for example, \cite{ar2004,bean2005,dasilva2005,bean2009} which are matrix-analytics-methods-based, and also \cite{rs2003} which is differential-equations-based and treats a slightly more complex model than the ones considered here, but is none-the-less relevant). Given the interest in discontinuous and point-mass initial conditions, transient analysis of fluid queues rarely relies on solving governing differential equations numerically, although it is quite possible to do so\footnote{We have already alluded to some difficulties with differential-equation-based approaches to problems with discontinuous solutions such as ill-defined PDEs, the need for weak solutions, and oscillatory approximations with possibly infeasible approximate solutions, for example negative probability approximations.}. Instead, expressions for transient distributions of fluid queues are derived in terms of Laplace transforms with respect to time, and/or moments of the transient distributions are derived \citep{ar2004,bean2005,bean2009}. Moreover, these techniques often lead to quantities which have direct probabilistic interpretations which further aids in their analysis and also applications to other problems \citep{ar2003,dasilva2005,bnp2018} (we also leverage these stochastic interpretations in Chapter~\ref{sec: conv} to derive expressions for certain the Laplace transforms of the fluid queue). In some contexts, the actual transient distributions are not required, and Laplace transforms or moments, which can be obtained relatively straightforwardly from the Laplace transforms, are all that are required. In other cases, the Laplace transforms may be inverted using known methods (\cite{aw2006}, or in the case of the discontinuities we might prefer \cite{hhat2020}, which uses the same concentrated matrix exponential distribution that we do).

One such analysis of fluid queues derives the Laplace transform of the time taken for the level of an unbounded fluid queue to return to its initial level in a certain phase, given it started in a phase with positive rate \citep{bean2005}. The principles underlying the derivation of this first return operator are the same as those applied in \cite{bo2014} to derive the first return operator for fluid-fluid queues. Moreover, certain matrices appearing in the analysis have a stochastic interpretation which we leverage to write down certain Laplace transforms in Chapter~\ref{sec: conv} in terms of these matrices. Given its relevance we briefly recount the analysis of \cite{bean2005} here. 

Consider an unbounded fluid queue \(\{(\ddot X(t), \varphi(t))\}\). Let \(\zeta_X(E)\) be the random variable which is the first hitting time of \(\{\ddot X(t)\}\) on the set \(E\) and define the matrix \(\bs \Psi_X(s)\) with elements \(\left[\bs \Psi_X(s)\right]_{ij}\), \(i\in\calS_+\), \(j\in\calS_-\), given by
\begin{align}
	\left[\bs \Psi_X(\lambda)\right]_{ij} = \mathbb E\left[ e^{-\zeta_X(z)\lambda} 1(\zeta_X(z)<\infty, \varphi(\zeta_X(z)=j)\mid \ddot X(0)=z, \varphi(0)=i\right].
\end{align}
\(\left[\bs \Psi_X(\lambda)\right]_{ij}\) is the Laplace-Stieltjes transform of the time taken for the fluid queue to first return to level \(z\) and do so in phase \(j\in\calS_-\), given it started at level \(z\) in phase \(i\in\calS_+\). Define the in-out fluid level by $\beta_X(t) := \int_0^t \left| c_{\varphi(z)} \right|  \wrt z$, which is the total amount of fluid to flow in to or out of the buffer \(\{\ddot X(t)\}\) by time \(t\), and also define $\eta_X(y) := \inf \{t > 0: \beta_X(t) = y\}$ as the first hitting time of the in-out process \(\{\beta_X(t)\}\) on level \(y\geq 0\). Further, let \(\bs H(\lambda,y)\) be the matrix with elements \(h_{ij}(\lambda,y)\), \(i,j\in\calS_+\cup\calS_-\), given by
\begin{align}
	\mathbb E\left[e^{-\lambda \eta_X(y)}1(\eta_X(y)<\infty,\varphi(\eta_X(y))=j)\mid \ddot X(0)=0, \varphi(0)=i\right],
\end{align}
which is the Laplace-Stieltjes transform of the time taken for \(y\) amount of fluid to flow in or out of the fluid queue and to be in phase \(j\) at this time, given the initial level of the fluid queue was \(0\) and the initial phase was \(i\). 

It turns out that, for fixed \(\lambda \geq 0\), \(\bs H (\lambda,y)\) is a semigroup (with variable \(y\)) \citep{bean2005}. \cite{bean2005} find the infinitesimal generator, \(\bs Q(\lambda)\) of \(\bs H(\lambda,y)\) and, since the generator is bounded, we can write \(\bs H(\lambda,y)=e^{\bs Q(\lambda)y}\). The derivation of the generator \(\bs Q(\lambda)=[Q_{ij}(\lambda)]_{ij\in\calS_+\cup\calS_-}\) is based on a direct analysis of sample paths of the fluid queue over an infinitesimal time, \(u\) say, and by leveraging the fact that complex sample paths occur with probability \(\mathcal O(u^2)\). As a result, there are only three types of sample paths to consider. Further, \cite{bean2005} argue that the generator \(\bs Q(\lambda)\) can be partitioned into blocks \(\bs Q_{mn}(\lambda) = \left[Q_{ij}(\lambda)\right]_{i\in\calS_m,j\in\calS_n}\), \(m\in\{+,-\},\,n\in\{+,-,0\}\) where 
\begin{align*}
	\bs Q_{+0}(\lambda) &= \bs C_+^{-1}\bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1},
	%
	\\\bs Q_{-0}(\lambda) &= \bs C_-^{-1}\bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1},
	%
	\\\bs Q_{++}(\lambda) &= \bs C_+^{-1} \left(\bs T_{++} - \lambda \bs I + \bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0+}\right),
	%
	\\\bs Q_{+-}(\lambda) &= \bs C_+^{-1} \left(\bs T_{+-} + \bs T_{+0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0-} \right) ,
	%
	\\\bs Q_{--}(\lambda) &= \bs C_-^{-1} \left(\bs T_{--}  - \lambda \bs I + \bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0-}\right),
	%
	\\\bs Q_{-+}(\lambda) &=\bs C_-^{-1} \left(\bs T_{-+}+ \bs T_{-0}\left[\lambda \bs I - \bs T_{00}\right]^{-1}\bs T_{0+}\right).
\end{align*}
\cite{bean2005} also define the functions 
\begin{align}
	\bs H^{++}(\lambda,y)&= \left[h_{ij}^{++}(\lambda,y)\right]_{i\in \mathcal S_+,j\in\mathcal S_{+}\cup\calS_{+0}} := e^{\bs{Q}_{++}(\lambda)y}\vligne{\bs C_+^{-1} & \bs Q_{+0}(\lambda)},  \label{eqn: lst 1a}
	\\\bs H^{--}(\lambda,y) &= \left[h_{ij}^{--}(\lambda,y)\right]_{i\in\mathcal S_-,j\in\mathcal S_{-}\cup\calS_{-0}}:= e^{\bs{Q}_{--}(\lambda)y}\vligne{\bs C_-^{-1} & \bs Q_{-0}(\lambda)},
	\\\bs H^{+-}(\lambda,y)  &= \left[h_{ij}^{+-}(\lambda,y)\right]_{i\in\mathcal S_+,\,j\in\mathcal S_-}:= e^{\bs{Q}_{++}(\lambda)y}\bs{Q}_{+-}(\lambda), 
	\\\bs H^{-+}(\lambda,y)&= \left[h_{ij}^{-+}(\lambda,y)\right]_{i\in\mathcal S_-,\, j\in\mathcal S_+} := e^{\bs{Q}_{--}(\lambda)y}\bs{Q}_{-+}(\lambda), \label{eqn: lst 4a}
\end{align}
for \(y,\lambda\geq 0,\) which have the following stochastic interpretations. The function \(h_{ij}^{++}(\lambda,y)\) (\(h_{ij}^{--}(\lambda,y)\)) is the Laplace transform (with respect to time) of the time taken for the fluid level to shift by an amount \(y\) whilst remaining in phases in \(\mathcal S_+\cup\calS_{+0}\) (\(\mathcal S_-\cup\calS_{-0}\)), given the phase was initially \(i\in\mathcal S_+\) (\(i\in\mathcal S_-\)). The function \(h_{ij}^{+-}(\lambda,y)\) (\(h_{ij}^{-+}(\lambda,y)\)) is the Laplace transform (with respect to time) of the time taken for the fluid level, \(\{Y(t)\}\) to shift by an amount \(y\) whilst remaining in phases in \(\mathcal S_+\cup\calS_{+0}\) (\(\mathcal S_-\cup\calS_{-0}\)), after which time the phase instantaneously changes to \(j\in\mathcal S_-\) (\(\mathcal S_+\)), given the phase was initially \(i\in\mathcal S_+\) (\(\mathcal S_-\)) \citep{bean2005}.

Thus, \cite{bean2005} are able to characterise, by the above matrix expressions, segments of sample paths of the fluid queue where the fluid level is non-decreasing and non-increasing. \cite{bean2005} then partition the sample paths which contribute to \(\bs \Psi_X\) as those which either, (a) have a single transition from \(\calS_+\) to \(\calS_-\) (perhaps via \(\calS_0\)), and, (b) those which have more than one transition from \(\calS_+\) to \(\calS_-\) (perhaps via \(\calS_0\)). An expression for Laplace-Stieltjes transform of (a) is 
\[\int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y}\bs Q_{+-}(\lambda)e^{\bs Q_{--}y}\wrt y.\]
For the sample paths (b), there must be at least one point at which the phase process transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) before the first return time. Further, one of the transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) must occur lower than all others. By considering the lowest level, \(y\), at which a phase transitions from \(\calS_-\) to \(\calS_+\) (perhaps via \(\calS_0\)) occurs, \cite{bean2005} characterise the Laplace transform of the paths (b) by the expression 
\begin{align}
	\int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y} \bs \Psi_X(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) e^{\bs Q_{--}(\lambda)y}\wrt y.
\end{align}

Adding the expressions for (a) and (b) together, \cite{bean2005} state 
\begin{align}
	\bs \Psi_X(\lambda) = \int_{y=0}^\infty e^{\bs Q_{++}(\lambda)y}\bs Q_{+-}(\lambda)e^{\bs Q_{--}y} + e^{\bs Q_{++}(\lambda)y} \bs \Psi_x(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) e^{\bs Q_{--}(\lambda)y}\wrt y,
\end{align}
which can be shown to be equivalent to \citep[Lemma~3 and Theorem~9.2]{br1997}
\begin{align}
	\bs Q_{++}(\lambda)\bs \Psi_X(\lambda) + \bs \Psi_X(\lambda)\bs Q_{--}(\lambda)\bs \Psi_X(\lambda) + \bs Q_{+-}(\lambda) + \bs \Psi_X(\lambda)\bs Q_{-+}(\lambda)\bs \Psi_X(\lambda) = \bs 0.
\end{align}

The key concepts of this argument are; (1) to partition the state space of the driving process into sets on which the fluid level is increasing, decreasing, or constant; (2) to characterise the sections of sample paths of the fluid level which are non-decreasing and non-increasing as semigroups and derive their generators; (3) to partition the sample paths which comprise \(\bs \Psi_X(\lambda)\) such that we can write down an expression for \(\bs \Psi_X(\lambda)\) in terms of the expressions derived in (2) and \(\bs \Psi_X(\lambda)\) itself and then to solve the resulting expression. 


% DO THE \(\bs Q(s)\) equation for \(\Psi(s)\); this is all possible because we have \(\bs T\) and the process can be partitioned in to up down sections. For FFQs the same can be done. The first step towards this is finding the generator of the fluid queue and partitioning it appropriately, which is what we do next. 


% fluid queues (with or without boundaries) are Feller processes, hence their associated semigroups are Feller semigroups. Let \(\boldsymbol f(x,t)\) be the density of a fluid queue at time \(t\). Denote the Markov semigroup associated with the fluid queue as \(V(t)\). Therefore, \(\boldsymbol f(x,t)\wrt x=\boldsymbol \mu V(t)(\wrt x)\), where \(\boldsymbol \mu(\wrt x)=\boldsymbol f(x,0)\wrt x\) is the initial probability density.

% Let \(L=C(\{1,2,...,N\}\times[-\infty,\infty])\) be the space of bounded continuous functions on \(\{1,2,...,N\}\times[-\infty,\infty]\) equipped with the sup norm, \(||\boldsymbol f||_\infty = \sup\limits_{x\in[-\infty,\infty],i\in\mathcal S}|f_i(x)|\). Continuity at \(\infty\) means that \(\lim\limits_{x\to \infty} f(x)=\lim\limits_{x\to -\infty} f(x)\) and the limit exists and is finite. That is, for \(\boldsymbol f\in L\), \(\boldsymbol f(x)=(f_1(x),\dots,f_N(x))\), where each \(f_i(x)\) is a bounded continuous function on the extended reals. Denote by \(V^*(t):L\to L\) the transition semigroup of a SFM: \(V^*(t)\boldsymbol f(x) = \int_{y}P(X(t)\in\wrt y, \varphi(t)=k\mid X(0)=x,\varphi(0)=i)f_j(y)\). The superscript \(^*\) alludes to the fact that \(V^*(t)\) is the adjoint of \(V(t)\). 
% \begin{lem}\label{lem: adjoint generator}
% For an unbounded SFM the generator of the semigroup \(V^*(t)\) is 
% \[A = T+\cfrac{\wrt}{\wrt x}C.\]
% \end{lem}
%The weak form, (\ref{eqn: weak pde}) is derived by multiplying (\ref{eqn: pde}) by \(\bs \psi(x,t)\), then integrating the last term by parts and assuming \( \psi(x,t)

%%
%%
% On the analysis fluid queues, MAM and why the methods dont apply here. 

\section{Fluid-fluid Queues}\label{sec: ffq intro}
	\label{sec:prelim}
	\begin{center}
		\begin{minipage}{0.8\textwidth}
			\textit{This subsection was largely taken from Sections~2 and 3 of \cite{blnos2022} with changes, such as notation, so that this chapter is consistent with the rest of the thesis. I am a co-author of the paper \cite{blnos2022}. %The conceptualisation of \cite{blnos2022} was originally by Vikram Sunkara, Nigel Bean and Giang Nguyen, and the original coding was done by Vikram Sunkara. I made significant contributions to Section~3 of the paper, expressing the operator-theoretic expressions to use the same partition as the approximation scheme. I contributed Sections~4.4 and 5.1. I extended the numerical experiments in Section~6 to higher orders and made all the plots in Section~6. Appendix~A is also my original work. I did a significant proportion of the writing of the manuscript and addressed the reviewers comments and also developed code for the numerical experiments.
			}
		\end{minipage}
		\end{center}

A stochastic fluid-fluid queue \citep{bo2013} is a Markov process with three elements, $\{( \ddot{Y}(t),{X}(t), \varphi(t))\}_{t \geq 0},$ where $\{(X(t),\varphi(t))\}_{t\geq 0}$ is a classical fluid queue\footnote{We assume \(X(t)\) is doubly-bounded as this results in a finite-dimensional approximation, but this is not necessary} and $\ddot{Y}(t)$ is the second fluid, which varies at rate $r_{\varphi(t)}(\dot{X}(t))$:
% 
\begin{align*} 
	\ddot{Y}(t) := \ddot Y(0)) + \int_0^t r_{\varphi(s)}({X}(s)) \wrt s.
\end{align*} 
% 
Regulated boundaries may also be included for the second fluid level. In this thesis we consider the second fluid to have a regulated boundary at \(y=0\) and unbounded above. To distinguish between unbounded and bounded processes, we use the notation \(\ddot Y(t)\) to denote the unbounded process and \(\dot Y(t)\) to denote the second fluid level process with a regulated lower boundary at 0.

% As classic fluid processes, $\{(\widehat X(t), \varphi(t)),t \geq 0\}$, or unbounded analogues, are used extensively in many areas, such as insurance and environmental modelling, it is clear that stochastic fluid-fluid queues have an even wider range of applicability. 

In the following, we assume that $\dot Y(t) \in [0,\infty)$ and that there is a boundary at level $0$ for both the first and second fluid levels and the first fluid level is also bounded above at \(b\). Thus, the rates of change of the fluid levels can be summarised as:
% 
	\begin{align*} 
		\frac{\wrt}{\wrt t}  X(t) & := \max\{0, c_i\} \quad \mbox{if }  X(t) = 0 \mbox{ and } \varphi(t) = i, \\
		\frac{\wrt}{\wrt t}  X(t) & := \min\{0, c_i\} \quad \mbox{if }  X(t) = b \mbox{ and } \varphi(t) = i, \\
          	\frac{\wrt}{\wrt t} \dot Y(t) & := \max\{0, r_i(x)\} \quad \mbox{if } \dot Y(t) = 0,\, X(t) = x \mbox{ and } \varphi(t) = i, 	
	\end{align*} 
	% 
for $i \in \mathcal{S} = \{1,...,N\}$. Let $\bs R(x) := \diag(r_i(x))_{i \in \mathcal{S}}$ be the diagonal fluid-rate matrix of functions for $\{\dot Y(t)\}$. 

For the remainder of this section, we summarise the findings of~\cite{bo2014} on the joint limiting distribution of $\{( \dot Y(t), X(t), \varphi(t))\}_{t \geq 0}$. The derivation of the limiting distribution relies on obtaining the operator \(\mathbb \Psi\) which gives the distribution of the process \(\{(X(t),\varphi(t))\}\) at the time when \(\{\dot Y(t)\}\) first returns to the level 0, given \(\dot Y(0)=0\).

The concepts leading to the derivation of \( {\mathbb \Psi}\) for the fluid-fluid queue are much the same as the derivation of the analogous quantity, \(\bs\Psi_X\), for a classical fluid queue. First, we need the infinitesimal generator of the driving process, \(\{(X(t),\varphi(t))\}\), which we denote by \(\mathbb B\). Then we need to partition \(\mathbb B\) on sets for which \(\{\dot Y(t)\}\) is increasing, decreasing or constant -- these are the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), below. We can then derive an operator-Riccati equation for the first return operator for the fluid-fluid queue, \(\mathbb \Psi\), in terms of the partitioned generator and the rates \(r_i(x)\). The solution to the operator-Riccati equation is an operator which can be used to obtain the distribution of the driving process \(\{(X(t),\varphi(t))\}\) at the time when \(\{\dot Y(t)\}\) first returns to \(0\). 

The problem we have is to solve the operator-Riccati equation -- something which is only possibly in the very simplest of cases. This is where the methods in this thesis come in. Here, we approximate \(\mathbb B\) by a finite-dimensional matrix which we then partition according to the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\). We then substitute the resulting matrices into the operator-Riccati equation and this gives us a matrix-Riccati equation which we can then solve using known methods \citep{bot08}. 

A key element of the cell-based approximation schemes is the partition of the state space of \(\{(X(t),\varphi(t))\}\) into sets of the form \((\calD_{k,i},i)\), where \(\calD_{k,i}\) are intervals are known as cells. We must choose the cells wisely so that the partition into the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\) can be recovered from the partition into cells. This is not terribly difficult to do, but does require some notation and explanation. We now proceed to introduce the work of \cite{bo2014} and explain how we can recover the partition into the sets \(\mathcal F_i^m,\, i\in\calS,\,m\in\{+,-,0\}\) from the cells as determined by the cell-based approximations.

For each Markovian state $i \in \mathcal{S}$, we partition the state space of \( X(t)\), \([0,b]\), according to the rates of change $r_i(\cdot)$ for the second fluid $\{\dot Y(t)\}$: $[0,b] := \mathcal{F}^{+}_i \cup \mathcal{F}^{-}_i \cup \mathcal{F}^{0}_i,$  
where 
% 
		\begin{align} 
%				\label{eqn:Fi1}
			\mathcal{F}^{+}_i & := \{u \in [0,b] : r_i(u) > 0\},  \; \nonumber
%				\label{eqn:Fi2}
			\\\mathcal{F}^{-}_i &:=  \{u \in [0,b]:  r_i(u) < 0\}, \; \nonumber
%				\label{eqn:Fi3}
			\\\mathcal{F}^{0}_i &:= \{u \in [0,b]: r_i(u) = 0\}.\label{eqn:fil}
		\end{align} 
% 
For all $i \in \mathcal{S}$, the functions $r_i(\cdot)$ are assumed to be sufficiently well-behaved so that $\mathcal{F}^{m}_i$, $m \in \{+, -, 0\}$, is a finite union of intervals and isolated points. 

We assume that the process $\{(\dot Y(t), X(t), \varphi(t))\}_{t\geq 0}$ is positive recurrent, in order to guarantee the existence of the joint limiting distribution. Define limiting operators 
\begin{align} 
		\label{eqn:jointpi} 
		\bbpi_i(y)(\mathcal{E}) & := \lim_{t \rightarrow \infty} \frac{\partial}{\partial y} \mathbb{P}\left( \dot Y(t) \leq y,  X(t) \in \mathcal{E}, \varphi(t) = i\right), \quad y>0,\\
% 
		\label{eqn:jointmass}
		{\mathbb p}_i(\mathcal{E}) & := \lim_{t \rightarrow \infty}  \mathbb{P}( \dot Y(t) = 0,  X(t) \in \mathcal{E}, \varphi(t) = i]),
\end{align} 
where $\mathcal{E} \subset [0,b]$. The keen observer will notice that (\ref{eqn:jointpi}) assumes a certain differentiability condition on the limiting distribution. Indeed, it is not always the case that (\ref{eqn:jointpi}) will be differentiable with respect to \(y\) (trivially, consider \(\dot Y(t)=y_0\) for all \(t\)). Conditions on fluid-fluid queues to ensure that (\ref{eqn:jointpi}) will be differentiable with respect to \(y\) are not known, and here we just assume that it is. 

Now, let ${{\bbpi}}(y) = ( \bbpi_i(y))_{i \in \mathcal{S}}$ be a vector containing the joint limiting density operators and ${\mathbb{p}} = ( {\mathbb p}_i)_{i \in \mathcal{S}}$ be a vector containing the joint limiting mass operators. The determination of ${{\bbpi}}(y)$ involves two important matrices of operators, $ {\mathbb D}$ and $ {\mathbb \Psi}$ which we now introduce. %The operator \( {\mathbb B}\) is the \textit{infinitesimal generator} of the process \(\{(X(t),\varphi(t))\}\). %Intuitively, for a set $\mathcal{E} \subset \mathcal{F}$ and a measure vector $\boldsymbol{\mu} = (\mu_i)_{i \in \mathcal{S}}$, $\bs{\mu}V(t)(\mathcal{E})$, where \(V(t)\) is a semigroup with infinitesimal generator \(B\), gives the conditional probability of $X(t) \in \mathcal{E}$, 
% The operator \( {\mathbb \Psi}\) is such that ${\boldsymbol{\mu}}^{+} {\mathbb \Psi} (\mathcal{E})$ gives the conditional probability of $\{\dot Y(t)\}$ returning to level zero and doing so when $X(t) \in \mathcal{E}$, given that the initial distribution of \((X(0),\varphi(0))\) in the set \(\bigcup_{i\in\calS}(\mathcal F_i^+,i)\), is ${\boldsymbol{\mu}}^{+}$. 

\subsection{The infinitesimal generator, \(\mathbb B\), of the driving process}
Given the discussion above, if we are to replicate the arguments of \cite{bean2005} to derive the Laplace-Stieltjes transform of the first return operator of a fluid-fluid queue, we first need an expression for the generator of the driving process. We also need to partition it according to whether the second fluid is increasing, decreasing or constant. 

The generator of a fluid queue is a differential operator and to enable computation of the first-return operator, approximation methods are needed. The approximation schemes which we discuss in this thesis are all cell-based methods which discretise the level of the fluid queue into intervals called cells. Thus, one complexity in approximation is to reconcile the partition according to whether the second fluid is increasing, decreasing or constant, and the partition which the approximation method uses. %As long as the rates of the fluid-fluid queue are not `too-badly' behaved, this is not terribly difficult to do, but it does require some discussion and notation. 

\label{subsec:B_operators}
\subsubsection{The transition semigroup of a fluid queue}
Since \(\{( X(t),\varphi(t))\}_{t\geq 0}\) is a Markov process, the evolution of probability can be described by a semigroup. Let $\mathcal{M}(\mathcal{S} \times [0,b])$ be the set of integrable complex-valued Borel measures on the Borel $\sigma$-algebra $\mathcal{B}_{\mathcal{S} \times [0,b]}$. For $\overline{\bs{\mu}} \in \mathcal{M}(\mathcal{S} \times [0,b])$, we can write \(\overline{\bs{\mu}} = (\overline{\mu_i})_{i \in \mathcal{S}}\). The measures \(\overline{\mu_i}(\cdot)\) represent an initial distribution, \(\overline{\mu_i}(\cdot) = \mathbb P(X(0)\in\cdot, \varphi(0) = i)\). 
Let \(\{\overline{\mathbb V}(t)\}_{t\geq 0},\, \overline{\mathbb V}(t):\mathcal{M}(\mathcal{S} \times [0,b]\mapsto \mathcal{M}(\mathcal{S} \times [0,b]\) be the semigroup describing the evolution of probability for \(\{(X(t),\varphi(t))\}_{t\geq 0}\) structured as a matrix of operators, \(\left[ \overline{\mathbb V}(t)\right]_{ij}= \overline{\mathbb V}_{ij}(t)\) where, 
\[\overline\mu_i \overline{ \mathbb V}_{ij}(t)(\mathcal{E}) = \int_{x\in[0,b]} \wrt \overline\mu_i(x)\mathbb P (X(t) \in\mathcal{E},\varphi(t) = j \mid X(0) = x, \varphi(0) = i).\]
Intuitively, the operator \( \overline{\mathbb V}(t)\) maps an initial measure \(\overline{\boldsymbol \mu}\) on \((X(0),\varphi(0))\) to the measure \(\mathbb P(X(t)\in \mathcal{E}, \varphi(t)=j)=:\overline\mu_j(t)(\mathcal{E})\). 
The matrix of operators \( \overline{\mathbb B}:=[\overline{\mathbb B}_{ij}]_{i,j\in\mathcal S}\) is the \textit{infinitesimal generator} of the semigroup \(\{ \overline{\mathbb V}(t)\}\) defined by 
\[ \overline{\mathbb B} =\left.\cfrac{\wrt}{\wrt t} \overline{\mathbb V}(t)\right|_{t=0},\]
with domain the set of measures for which this limit exists. Specifically, the domain of \(\overline{\mathbb B}\) is the set of measures, \(\overline{\bs \mu} = (\overline\mu_i)_{i\in\calS}\), for which each \(\overline\mu_i\) admits an absolutely continuous density on \((0,b)\), and can have a point mass at \(0\) if \(i\in\mathcal S_{-1}\) or a point mass at \(b\) if \(i\in\mathcal S_{K+1}\); call this set of measures \(\mathcal M_{0,b}\). The measures \(\overline\mu_i\) cannot have a point masses at 0 if \(i\notin \mathcal S_{-1}\), nor can they have a point masses at \(b\) if \(i\notin\calS_{{K+1}}\), nor can they have point masses in \((0,b)\) for any \(i\in\calS\). In the sequel we write \(\overline v_i(x)\) as the density of \(\overline\mu_i\), and \(q_{{-1},i}\) and \(q_{{K+1},i}\) as the point masses of \(\overline\mu_i\) at \(0\) and \(b\), respectively (if such point masses exist). 

\subsubsection{A partition with respect to rates of the second fluid}
To use the operators \(\{ \overline{\mathbb  V}(t)\}\) and \( \overline{\mathbb  B}\) to analyse the fluid-fluid queue, \cite{bo2014} explicitly track when \((X(t),\varphi(t))\in(\mathcal F_i^m,i)\) for \(i\in\mathcal S,\, m \in \{+,-,0\}\) by partitioning the operators \( \overline{\mathbb  V}(t)\) and \( \overline{\mathbb  B}\) into \( \overline{\mathbb  V}_{ij}^{m n}\) and \( \overline{\mathbb  B}_{ij}^{m n}\), for \(i,j\in\mathcal S,\, m,n\in \{+,-,0\}\), where
\[\left.\overline\mu_i\right|_{\calF_i^m}  \overline{\mathbb  V}_{ij}^{m n}(t)(\mathcal{E}) := \int_{x\in[0,b]} \left. \wrt \overline\mu_i\right|_{\calF_i^m}(x)\mathbb P (X(t) \in\mathcal{E}\cap \calF_j^n,\varphi(t) = j \mid X(0) = x, \varphi(0) = i),\]
and \(\left.\overline\mu_i\right|_{E}\) is the restriction of \(\overline\mu_i\) to \(E\), \(\left.\overline\mu_i\right|_{E}(\mathcal E)=\overline\mu_i(\mathcal E\cap E)\). Similarly, for \( \overline{\mathbb  B}_{ij}^{m n},\) \(i,j\in\mathcal S,\, m,n\in \{+,-,0\}\).

\subsubsection{A partition as dictated by the approximation schemes}
We claim that numerical schemes are needed to approximate the analytic operator equations introduced in \cite{bo2014}. The schemes we choose to use here work by first partitioning the state space of the fluid level, \(\{X(t)\}\), into a collection of intervals, \({\calD_{k,i}}\) then constructing an approximation to \(\overline{\mathbb B}\) on each interval. To help elucidate the connection between the operators \(\{ \overline{\mathbb  V}(t)\}\), \( \overline{\mathbb  B}\) and their approximate counterparts we take a slightly different approach to partitioning these operators than that taken in \cite{bo2014}. Rather than partition according to the sets \(\calF_i^m,\,i\in\calS,\,m\in\{+,-,0\}\), we use the same partition as that in the construction of the approximation schemes. By doing so, we can directly correspond elements of the partitioned operators to their approximation counterparts. Since the partition used to construct the approximation schemes is finer, then we can reconstruct the partition in terms of the sets \(\calF_i^m,\,i\in\calS,\,m\in\{+,-,0\}\). 

Let us first partition the space \([0,b]\) into \(\calD_{-1,i}=\calD_{-1} = \{0\}\), \(i\in\calS_{-1}\), \(\calD_{{K+1},i}=\calD_{{K+1}}=\{b\}\), \(i\in\calS_{K+1}\), and non-trivial intervals \(\calD_{k,i}=[y_k,y_{k+1})\setminus \{0\},\) \(i\in\calS_+\cup\calS_0\) and \(\calD_{k,i}=(y_k,y_{k+1}]\setminus \{b\}\), \(i\in\calS_-\), with \(y_0=0,\,y_{K+1}=b,\, y_k<y_{k+1},\,k\in\mathcal K^\circ:=\{0,1,2,...,K\}\) and define \(\mathcal K = \{-1,K+1\}\cup\mathcal K^\circ\). For $\bs{\mu} \in \mathcal{M}_{0,b}(\mathcal{S} \times [0,b])$ we write \(\bs{\mu} = (\mu_{k,i})_{i \in \mathcal{S},k\in\mathcal K},\) where $\mu_{k,i}(\cdot) = \mu_i(\cdot \cap \calD_{k,i}),\,k\in\mathcal K.$ We denote by \(v_{k,i}(x),\,x>0\), the density associated with the measure, \(\mu_{k,i},\,k\in \mathcal K^\circ\). For \( i,j\in\calS,\,k,\ell\in\mathcal K\) define the operators 
\[\mu_{k,i} \mathbb V_{ij}^{k \ell}(t)(\mathcal{E}) := \int_{x\in\calD_{k,i}} \wrt \mu_{k,i}(x)\mathbb P (X(t) \in\mathcal{E}\cap \calD_{\ell,i},\varphi(t) = j \mid X(0) = x, \varphi(0) = i),\]
and the matrices of operators \(\mathbb V^{k\ell}(t) := \vligne{\mathbb V_{ij}^{k\ell}(t)}_{i,j\in\mathcal S},\,k,\ell\in\mathcal K\) and write 
\[\mathbb V(t) = \left[
	\begin{array}{ccccc}
		\mathbb V^{{-1},{-1}}(t)&\mathbb V^{{-1},0}(t)& \mathbb V^{{-1},1}(t) &\hdots & \mathbb V^{{-1},{K+1}}(t)\\
		\mathbb V^{0,{-1}}(t)&\mathbb V^{0,0}(t)&\mathbb V^{0,1}(t)&\hdots&\mathbb V^{0,{K+1}}(t)\\
		\mathbb V^{1,{-1}}(t)&\mathbb V^{1,0}(t)&\mathbb V^{1,1}(t)&\hdots&\mathbb V^{1,{K+1}}(t)\\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		\mathbb V^{{K+1},{-1}}(t) &\mathbb V^{{K+1},0}(t) &\mathbb V^{{K+1},1}(t) & \hdots & \mathbb V^{{K+1},{K+1}}(t) 
%		&\ddots &&&&\reflectbox{\(\ddots\)}\\
%		&&\mathbb V^{k-1,k-1}(t) & \mathbb V^{k-1,k}(t) & \mathbb V^{k-1,k+1}(t)& \\
%		&&\mathbb V^{k,k-1}(t) & \mathbb V^{k,k}(t) & \mathbb V^{k,k+1}(t)& \\
%		&&\mathbb V^{k+1,k-1}(t) & \mathbb V^{k+1,k}(t) & \mathbb V^{k+1,k+1}(t)& \\
%		&\reflectbox{\(\ddots\)}&&&&\ddots
	\end{array}\right].\]
Now define \(\mathbb B=\left.\cfrac{\wrt}{\wrt t}\mathbb V(t)\right|_{t=0}\) as the infinitesimal generator of \(\{\mathbb V(t)\}\), resulting in the tridiagonal matrix of operators 
\[\mathbb B(t) = \left[
	\begin{array}{ccccc}
		\mathbb B^{{-1},{-1}}(t)&\mathbb B^{{-1},0}(t)& & & \\
		\mathbb B^{0,{-1}}(t)&\mathbb B^{0,0}(t)&\mathbb B^{0,1}(t)&& \\
		&\mathbb B^{1,0}(t)&\mathbb B^{1,1}(t)&\ddots& \\
		& & \ddots & \ddots & \mathbb B^{K,{K+1}}\\
		& & & \mathbb B^{{K+1},K} & \mathbb B^{{K+1},{K+1}}
	\end{array}\right],\]
%\begin{align*}
%\mathbb B &= \left[\begin{array}{lllll}
%		\ddots &&&&\\
%		\mathbb B^{k-2,k-1} &\mathbb B^{k-1,k-1} & \mathbb B^{k-1,k} & & \\
%		&\mathbb B^{k,k-1} & \mathbb B^{k,k} & \mathbb B^{k,k+1}& \\
%		& & \mathbb B^{k+1,k} & \mathbb B^{k+1,k+1}&\mathbb B^{k+1,k+2} \\
%		&&&&\ddots
%	\end{array}\right],
%\end{align*}
where the blocks \(\mathbb B^{k\ell}:= \vligne{\mathbb B_{ij}^{k\ell}(t)}_{i,j\in\mathcal S},\,k,\ell\in\mathcal K\).\footnote{We use a blackboard bold font with an overline above the character (e.g.~\(\overline{\mathbb B}\) and \(\overline{\mathbb V}(t)\)) to represent theoretical operators derived in \citep{bo2014} which are constructed using the partition in (\ref{eqn:fil}). The operators denoted with an overline play a minor role in the introductory sections of this thesis, but do not appear again. We use a blackboard font sans overline (e.g.~\(\mathbb V(t)\) and \(\mathbb B\)) to represent the same operators but which are constructed with the finer partition defined by \(\mathcal D_k,\,k\in\mathcal K\). We use the letters \(i,j\in\mathcal S\) to represent states of the phase process, letters \(m,n,\in\mathcal \{+,-,0\}\) to refer to the partition in terms of the sets in Equations (\ref{eqn:fil}), and the letters \(k,\ell\in\mathcal K\) to refer to the finer partition into sets \(\{\calD_k\}_k\). With a slight abuse of notation, whenever we use the dummy variables \(k,\ell\) without qualification we imply \(k,\ell\in\mathcal K\), the dummy variables without qualification \(m,n\) imply \(m,n\in\{+,-,0\}\) and the dummy variables \(i,j\) without qualification imply \(i,j\in\calS\). E.g.~\(\mathbb B_{ij}^{k\ell}\) means \(\mathbb B_{ij}^{k\ell},\, i,j\in\calS, k,\ell\in\mathcal K\) and \(\mathbb B_{ij}^{mn}\) means \(\mathbb B_{ij}^{mn},\, i,j\in\calS, m,n\in\{+,-,0\}\).
} The tridiagonal structure arises since, for \(|k-\ell|\geq2\) it is impossible for \(\{X(t)\}\) to move from \(\mathcal D_{k,i}\) to \(\mathcal D_{\ell,j}\) in an infinitesimal amount of time.

By an appropriate choice of the intervals \(\{\mathcal D_{k,i}\}\), the partition used in \cite{bo2014} can be recovered. Intuitively, we must ensure that each of the boundaries of \(\calF_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), align with a boundary of a cell \(\mathcal D_{k,i}\). Then, each set \(\calF_i^m,\, i\in\calS,\,m\in\{+,-,0\}\), can be written as a union of cells, \(\mathcal D_{k,i},\,k\in\mathcal K\), sans a collection of points which have measure zero for all measures in \(\mathcal M_{0,b}\), and this collection of points is inconsequential for the purposes of the approximations presented here. 

Formally, to recover the partition used in \citep{bo2014} we choose the intervals \(\calD_{k,i}\) such that \(   l (\calD_{k,i}\cap\calF_i^m) \in \{   l (\calD_{k,i}), 0\}\) for all \(i\in\calS,\,m\in\{+,-,0\},\,k\in\mathcal K\), for all \( l \in\mathcal M_{0,b}\). That is, we choose \(\calD_{k,i}\) such that it is contained, up to sets of measure 0 with respect to measures in \(\mathcal M_{0,b}\), within one of the sets \(\calF_i^m\) for \(m\in\{+,-,0\}\) and \(i\in\calS\). We assume such a partition for the rest of the thesis. For \(i\in\calS,\,m\in\{+,-,0\}\), let \(\mathcal K^m_i = \{k\in\mathcal K\mid  l (\calD_{k,i}\cap\calF_i^m) =   l (\calD_{k,i}),\,l\in\mathcal M_0 \}\), so that \(\bigcup\limits_{k\in\mathcal K_i^m} \calD_{k,i}\) and \(\mathcal F_i^m\) are equal up to a set of \(\mathcal M_{0,b}\)-measure 0. Define \(\mathcal K^m = \bigcup\limits_{i\in\calS}\mathcal K_i^m\), \(m\in\{+,-,0\}\). 
%Then for \(m\in\{+,-,0\}\), \(\mu_i^m(\cdot) = \sum\limits_{k\in\mathcal K_i^m} \mu_i^k(\cdot)\). 

To recover the partition defined by (\ref{eqn:fil}) we bundle together the elements of \(\mathbb V(t)\) which correspond to \(\calF_i^m\) and \(\calF_j^n\). That is, for \(m,n\in\{+,-,0\}\), define \(\mathbb V_{ij}^{m n}(t)\) as the matrix of operators 
\[\mathbb V_{ij}^{m n}(t) = \left[\mathbb V_{ij}^{k \ell}(t)\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n}.\] 
%Then, for \(i,j\in\calS,\,m,n\in\{+,-,0\}\), we can write \(\overline{\mathbb  V}_{ij}^{mn}(t) = \boldsymbol 1_{|\mathcal K_i^m|} \mathbb V_{ij}^{m n}(t) \boldsymbol 1_{|\mathcal K_j^n|}\tr{}\) where \(\boldsymbol 1_{|\mathcal K_i^m|}\) and \(\bs 1_{|\mathcal K_j^n|}\) are row-vectors of 1's of length \(|\mathcal K_i^m|\) and \(|\mathcal K_j^n|\), respectively, and \({}\tr{}\) denotes the transpose. 
The same construction can be achieved with \(\mathbb B\). 

Let \(\calS_k^+=\{i\in\calS\mid r_i(x)>0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^0=\{i\in\calS\mid r_i(x)=0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^-=\{i\in\calS\mid r_i(x)<0,\,\forall x \in\calD_{k,i}\}\), \(\calS_k^\bullet=\{i\in\calS\mid r_i(x)\neq0,\,\forall x \in\calD_{k,i}\}\) for \(k\in\mathcal K^\circ\) and \(\calS_{k}^+=\{i\in\calS_{k}\mid r_i(y_{k})>0\}\), \(\calS_{k}^-=\{i\in\calS_{k}\mid r_i(y_{k})<0\}\), \(\calS_{k}^0=\{i\in\calS_{k}\mid r_i(y_{k})=0\}\), \(\calS_{k}^\bullet=\{i\in\calS_{k}\mid r_i(y_{k})\neq 0\}\) for \(k\in\{-1,K+1\}\). For later reference, we need the following constructions. For \(k,\ell\in\mathcal K\) let
\begin{align}
	\mathbb B^{k\ell} & = \left[\mathbb B_{ij}^{k\ell}\right]_{i,j\in\mathcal S},\label{eqn:ref1here}
\end{align}
for \(i,j\in\calS\) let 
\begin{align}
	\mathbb B_{ij} & = \left[\mathbb B_{ij}^{k\ell}\right]_{k,\ell \in \mathcal K},
\end{align}
%for \(i,j\in\calS, \, m,n\in\{+,-,0\}\) 
%\begin{align}
%	\mathbb B_{ij}^{m n} &= \left[\mathbb B_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n},\label{eqn: Bmn etc}
%	\\ \mathbb B_{ij}^{k n} &= \left[\mathbb B_{ij}^{k \ell}\right]_{\ell\in\mathcal K_j^n} \mbox{ for }k\in\{{-1},1,2,...\},
%	\\ \mathbb B_{ij}^{m \ell} &= \left[\mathbb B_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m} \mbox{ for } \ell\in\{{-1},1,2,...\},
%	\label{eqn: Bl0 etc}
%\end{align}
and for \(m,n\in\{+,-,0\}\) let
\begin{align}
	\mathbb B^{m n} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{k\in\mathcal K^m,\ell\in\mathcal K^n},\label{eqn: Bmn2 etc}
	\\ \mathbb B^{k n} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{\ell\in\mathcal K^n} \mbox{ for }k\in\mathcal K,
	\\ \mathbb B^{m \ell} &= \left[\left[\mathbb B_{ij}^{k\ell}\right]_{i\in\calS_k^m,j\in\calS_\ell^n}\right]_{k\in\mathcal K^m} \mbox{ for } \ell\in\mathcal K.
	\label{eqn:ref2here}
\end{align}
We persist with the partition \(\calD_{k,i},\,k\in\mathcal K\) throughout as this is consistent with the partition used in the approximation schemes considered in this thesis. Note that for all the operators defined with this partition, the partitioning used in \citep{bo2014} can always be recovered by the above construction. 

\subsubsection{Definition of \(\mathbb B\)}
When \(v_k\) is differentiable we can write \(\mu_{k,i}\mathbb B_{ij}^{k\ell}(\mathcal{E})\) in kernel form as \[\displaystyle\int_{x\in\calD_{k,i},y\in\mathcal{E}}\wrt \mu_{k,i}(x)\mathbb B_{ij}^{k\ell}(x,dy).\] It is known that
\begin{align}\label{eqn: lfj22}\mu_{k,i}\mathbb B_{ij}^{kk}(\wrt y):=\int_{x\in\calD_{k,i}}\wrt\mu_{k,i}(x)\mathbb B_{ij}^{kk}(x,\wrt y)=\begin{cases}
v_{k,i}(y)T_{ij}\wrt y, & i\neq j,\\
v_{k,i}(y)T_{ii}\wrt y - c_i\cfrac{\wrt}{\wrt y}v_{k,i}(y)\wrt y, & i=j, 
\end{cases}\end{align}
on the interior of \(\calD_{k,i}\), \(k\in\mathcal K^\circ\), \citep{kk1995}. Intuitively, \(v_{k,i}(y)T_{ij}\wrt y\) represents the instantaneous rate of transition from phase \(i\) to \(j\) in the infinitesimal interval \(\wrt y\), \(v_{k,i}(y)T_{ii}\wrt y\) represents no such transition occurring, and \(- c_i\cfrac{\wrt}{\wrt y}v_{k,i}(y)\wrt y\) represents the drift across the interval \(\wrt y\) when the phase is \(i\). 

Translating the results of \cite{bo2014} to use the partition \(\{\calD_{k,i}\}\) we may state that, for all \(i,j\in\calS\), \(i\neq j\), \(k\in\{1,\dots,K-1\}\),
\begin{align*}
	\mu_{k,i}\mathbb B_{ij}^{kk}(\calD_{k,i})&=\int_{x\in\calD_{k,i}}v_{k,i}(x)T_{ij}\wrt x,\\
	\mu_{k,i}\mathbb B_{ii}^{kk}(\calD_{k,i})&=\int_{x\in\calD_{k,i}}v_{k,i}(x)T_{ii}\wrt x - c_iv_{k,i}(y_{k+1}^-) 1{(c_i>0)} + c_iv_{k,i}(y_{k}^+) 1{(c_i<0)},
\end{align*}
where \( 1(\cdot)\) is the indicator function and \(x^+\) and \(x^-\) denote the right and left limits at \(x\). 
Intuitively, the first expression represents the instantaneous rate of the stochastic transitions of the phase process \(\{\varphi(t)\}\) from \(i\) to \(j\) while \(\{X(t)\}\) remains in \(\calD_{k,i}\). The first term in the second expression represents the net rate of transition out of phase \(i\) while \(\{X(t)\}\) remains in \(\calD_{k,i}\), while the second and third terms in the second expression represent the flux out of the right-hand edge of \(\calD_{k,i}\) when \(c_i>0\) and the flux out of the left-hand edge of \(\calD_{k,i}\) when \(c_i<0\), respectively. Essentially, this is just a rewriting of (\ref{eqn: lfj22}) as an integral equation and including the boundary conditions.

The results of \cite{bo2014} also imply that, 
\begin{align*}
\mu_{k,i}\mathbb B_{ii}^{k,k+1}(\calD_{k+1,i})&= c_iv_{k,i}(y_{k+1}^-) 1{(c_i>0)},\mbox{ for all \(i\in\calS\), \(k\in\{0,1,2,...,K-1\}\),}
\\\mu_{k,i}\mathbb B_{ii}^{k,k-1}(\calD_{k-1,i})&= -c_iv_{k,i}(y_{k}^+) 1{(c_i<0)}, \mbox{  for all \(i\in\calS\), \(k\in\{1,2,3,...,K\}\)}.
\end{align*}
Intuitively, the first equation represents the flux from \(\calD_{k,i}\) to \(\calD_{k+1,i}\) across the shared boundary at \(y_{k+1}\) which occurs when \(c_i>0\) only. The second expression represents the flux from \(\calD_{k,i}\) to \(\calD_{k-1,i}\) across the shared boundary at \(y_{k}\) which occurs when \(c_i<0\) only. 

At the boundary \(x=0\), the rate at which point masses move between phases is 
\begin{align*}
	\mu_{-1,i}\mathbb B_{ii}^{{-1},{-1}} &= \mu_{-1,i}(\{0\})T_{ii},\mbox{ if } c_i\leq 0,\,\\
	\mu_{-1,i}\mathbb B_{ij}^{{-1},{-1}} &= \mu_{-1,i}(\{0\})T_{ij},\mbox{ if }c_i\leq 0,\,c_j\leq 0. 
	\end{align*}
	The rate at which point mass leaves the boundary is
	\begin{align*}
	\mu_{-1,i}\mathbb B_{ij}^{{-1},0} &= \mu_{-1,i}(\{0\})T_{ij},\mbox{ if }c_i\leq 0,\,c_j> 0. 
	\end{align*}
	The rate at which point masses accumulate at the boundary is 
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,{-1}} &= -c_ip^{{-1}}_{ij}v_{0,i}(0^+),\mbox{ if }c_i< 0,\,c_j\leq 0.
	\end{align*}
	In \(\calD_{0,i}\), if \(c_i<0\) and \(c_j>0\) the phase changes from \(i\) to \(j\) either from a stochastic jump at rate \(T_{ij}\), or from \(\{X(t)\}\) hitting the boundary in phase \(i\) and transitioning with probability \(p_{ij}^{-1}\) to phase \(j\), hence
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ij}\wrt x - c_iv_{0,i}(0^+)p^{-1}_{ij}.
	\end{align*}
	Also in \(\calD_{0,i}\), if \(i\neq j\) and \(c_i\geq 0\) or \(i\neq j\), \(c_i< 0\) and \(c_j\leq 0\), the phase changes from \(i\) to \(j\) while \(\{X(t)\}\) remains in \(\calD_{0,i}\) at rate
	\begin{align*}
	\mu_{0,i}\mathbb B_{ij}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ij}\wrt x,
	\end{align*}
	else, \(\{(X(t),\varphi(t))\}\) leaves \((\calD_{0,i})\) at rate
	\begin{align*}
	\mu_{0,i}\mathbb B_{ii}^{0,0}(\calD_{0,i})&=\int_{x\in\calD_{0,i}}v_{0,i}(x)T_{ii}\wrt x - c_iv_{0,i}(y_1^-) 1{(c_i> 0)} + c_iv_{0,i}(0^+)1{(c_i< 0)}.
\end{align*}

Similarly, at the boundary \(x=b\), the rate at which point masses move between phases is 
\begin{align*}
	\mu_{K+1,i}\mathbb B_{ii}^{{K+1},{K+1}} &= \mu_{K+1,i}(\{0\})T_{ii}, \mbox{ if }c_i\geq 0,\,\\
	\mu_{K+1,i}\mathbb B_{ij}^{{K+1},{K+1}} &= \mu_{K+1,i}(\{0\})T_{ij},\mbox{ if }c_i\geq 0,\,c_j\geq 0.
	\end{align*}
	The rate at which point mass leaves the boundary is
	\begin{align*}
	\mu_{K+1,i}\mathbb B_{ij}^{{K+1},K} &= \mu_{K+1,i}(\{0\})T_{ij},\mbox{ if }c_i\geq 0,\,c_j<0.
	\end{align*}
	The rate at which point masses accumulate at the boundary is 
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,{K+1}} &= c_ip^{{K+1}}_{ij}v_{K,i}(0^+),\mbox{ if }c_i> 0,\,c_j\geq 0.
	\end{align*}
	In \(\calD_{K,i}\), if \(c_i>0\) and \(c_j<0\) the phase changes from \(i\) to \(j\) either from a stochastic jump at rate \(T_{ij}\), or from \(\{X(t)\}\) hitting the boundary in phase \(i\) and transitioning with probability \(p_{ij}^{K+1}\) to phase \(j\),
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ij}\wrt x + c_iv_{K,i}(b^-)p^{K+1}_{ij},\,c_i>0,\,c_j<0.
	\end{align*}
	If \(i\neq j\) and \(c_i\leq 0\) or \(i\neq j\), \(c_i>0\) and \(c_j\geq 0\), the phase changes from \(i\) to \(j\) within \(\calD_{K,i}\) at rate
	\begin{align*}
	\mu_{K,i}\mathbb B_{ij}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ij}\wrt x,
	\end{align*}
	else, \(\{(X(t),\varphi(t))\}\) leaves \((\calD_{K,i},i)\) at rate
	\begin{align*}
	\mu_{K,i}\mathbb B_{ii}^{K,K}(\calD_{K,i})&=\int_{x\in\calD_{K,i}}v_{K,i}(x)T_{ii}\wrt x + c_iv_{K,i}(y_K^+)1{(c_i< 0)} - c_iv_{K,i}(b^-)1{(c_i> 0)}.
\end{align*}
Otherwise, \(\mu_{k,i}\mathbb B_{ij}^{k\ell}=0\).

Note that we have not presented \(\mathbb B\) in its full detail here and refer the reader to \citep{bo2014} for the details. The main goal here is to show how \(\mathbb B\) is used to construct the limiting distribution of the fluid-fluid queue and to illustrate the link between the operator \(\mathbb B\) and the approximations of the same object. As we shall see later, these expressions closely resemble the approximations to the same quantities. 

\subsection{The infinitesimal generator, \(\mathbb D\), of an in-out process}
Let $\beta(t) := \int_0^t \left| r_{\varphi(z)}(X(z)) \right|  \wrt z$ be the total unregulated amount of fluid that has flowed into or out of the second buffer during $[0,t]$, and let $\eta(y) := \inf \{t > 0: \beta(t) = y\}$ be the first time this accumulated in-out amount hits level $y$. Note that at the stopping time \(\eta(y)\) it must be that \((X({\eta(y)}),\varphi({\eta(y)}))\in(\calF_i^m,i)\) for some \(i\in\calS\) and \(m\in\{+,-\}\), i.e.~\(m\neq0\). We define the operators $\mathbb{U}_{ij}^{k\ell}(y,s): \mathcal{M}_{0,b}(\mathcal{D}_{k,i}\cap\calF_i^m) \mapsto \mathcal{M}_{0,b} (\calD_{\ell,j}\cap\mathcal{F}_j^n)$, for $k\in\mathcal K_i^+\cup\mathcal K_i^-$,  $\ell\in\mathcal K_j^+\cup\mathcal K_j^-$, and $i \in \mathcal{S}_k^\bullet,\,j \in \mathcal{S}_k^\bullet$, by 
% 
	\begin{align*} 
		&\mu_{k,i}\mathbb{U}_{ij}^{k\ell} (y,s) (\mathcal{E}) 
		\\&:= \int_{x \in \calD_{k,i}} \wrt  \mu_{k,i}(x) \mathbb{E}\left[e^{-s\eta(y)}{1}\left\{X({\eta(y)}) \in \mathcal{E}\cap\calD_{\ell,j},\;\varphi({\eta(y)}) = j\right\} \mid \varphi(0) = i, X(0) = x\right].
	\end{align*} 
Then, construct the matrix of operators 
\begin{align*}
%\mathbb{U}^{k \ell}(y,s)&:=[\mathbb U_{ij}^{k\ell}(y,s)]_{i\in\calS_k^\bullet,j\in\calS_\ell^\bullet}, \, k,\ell\in \mathcal K^+\cup\mathcal K^-,
\mathbb U(y,s) &:= \left[[\mathbb U_{ij}^{k\ell}(y,s)]_{i\in\calS_k^\bullet,j\in\calS_\ell^\bullet}\right]_{k,\ell\in\mathcal K^+\cup\mathcal K^-}.\end{align*}
%\[\mathbb U(t) = \left[
%	\begin{array}{llll}
%		\mathbb U^{{-1},{-1}}(t)&\mathbb U^{{-1},1}(t)& \mathbb U^{{-1},2}(t) &\hdots \\
%		\mathbb U^{1,{-1}}(t)&\mathbb U^{1,1}(t)&\mathbb U^{1,2}(t)&\hdots\\
%		\mathbb U^{2,{-1}}(t)&\mathbb U^{2,1}(t)&\mathbb U^{2,2}(t)&\hdots\\
%		\vdots & \vdots & \vdots & \ddots 
%	\end{array}\right].\]
%	\begin{align*} 
%		\mathbb{U}(y,s) = \left[\begin{array}{llllll} 
%		\ddots & & & &~\reflectbox{$\ddots$}\\
%            		& \mathbb{U}^{k-1,k-1}(y,s) & \mathbb{U}^{k-1,k}(y,s) & \mathbb{U}^{k-1,k+1}(y,s) \\
%			& \mathbb{U}^{k,k-1}(y,s) & \mathbb{U}^{k,k}(y,s) & \mathbb{U}^{k,k+1}(y,s) \\
%			& \mathbb{U}^{k+1,k-1}(y,s) & \mathbb{U}^{k+1,k}(y,s) & \mathbb{U}^{k+1,k+1}(y,s) \\	
%			\reflectbox{$\ddots$}& & & & \ddots		
%		\end{array}\right], 
%	\end{align*} 
	% 
%	where $\mathbb{U}^{k \ell}:=\mathbb U_{ij}^{k\ell}$ is an $|\mathcal{S}| \times |\mathcal{S}|$ matrix of operators for $y > 0$, $s \in \mathbb{C}$, and Re$(s) \geq 0$. 
The matrix of operators \(\mathbb D(s)\) is the infinitesimal generator of the semigroup \(\{\mathbb U(y,s)\}_{y\geq 0}\) defined by 
\[\mathbb D(s) = \cfrac{\wrt}{\wrt y}\mathbb U(y,s)|_{y=0},\]
whenever this limit exists.
			
	Recalling the constructions in Equations (\ref{eqn:ref1here})-(\ref{eqn:ref2here}) and using Lemma~$4$ of \cite{bo2014} gives the following expression for $\mathbb{D}(s)$. 
\begin{lem}\label{lemma: D(s)}
	For $y \geq 0$, $s \in \mathbb{C}$ with \textit{Re}$(s) \geq 0$, $i,j \in \mathcal{S}$, $k\in \mathcal K_i^+\cup\mathcal K_i^-$, $\ell\in \mathcal K_j^+\cup\mathcal K_j^-$,
	% 
	\begin{align*}
		\mathbb{D}_{ij}^{k\ell}(s) = [\mathbb{R}^{k}(
		\mathbb{B}^{k\ell } - s\mathbb{I} + \mathbb{B}^{k0 }(s \mathbb{I} - \mathbb{B}^{00})^{-1}\mathbb{B}^{0\ell})]_{ij}, 
	\end{align*} 
	% 
	where $\mathbb I$ is the identity operator, and $\mathbb{R}^{k} := diag(\mathbb{R}_i^{k},\,i \in \mathcal{S})$ is a diagonal matrix of operators $\mathbb{R}_i^{k}$ given by 
	\begin{align*} 
		{\mu}_{k,i}\mathbb{R}_{k,i}(\mathcal{E}) := \int_{x \in \mathcal{E} \cap \mathcal{D}_{k,i}} \frac{1}{|r_i(x)|}\wrt  \mu_{k,i}(x),\quad k\in \mathcal K_i^+\cup\mathcal K_i^-.
	\end{align*} 
\end{lem}

	Also, construct the matrices of operators 
	\begin{align*}
            	%\mathbb D^{k \ell} &:= \left[\mathbb D_{ij}^{k \ell}\right]_{i,j\in\mathcal S},\quad k,\ell\in\{{-1},1,2,...\},
		\mathbb D^{m n} &:= \left[\left[\mathbb D_{ij}^{k \ell}\right]_{i\in\calS_k^m,j\in\calS_k^n}\right]_{k\in\mathcal K^m,\ell\in\mathcal K^n}.
		%\\ \mathbb D_{ij}^{m n} &:= \left[\mathbb D_{ij}^{k \ell}\right]_{k\in\mathcal K_i^m,\ell\in\mathcal K_j^n},\quad i,j\in\calS,\,m,n\in\{+,-,0\}.
        \end{align*}

\subsection{The first-return operator, $\mathbb\Psi(s)$}\label{sec: intro Psi}
We denote by $\mathbb \Psi(s)$ the matrix of operators with the same dimensions as \(\mathbb D^{+-}\), recording the Laplace-Stieltjes transforms of the time for $\{\dot Y(t)\}$ to return, for the first time, to the initial level of zero as introduced in \cite{bo2014} but constructed with respect to the finer partition \(\{\calD_{k,i}\}\). Define the stopping time $\zeta_Y(E):= \inf \{t > 0: \dot Y(t) \in E\}$ to be the first time $\{\dot Y(t)\}$ hits the set $E$, then each component $\mathbb \Psi_{ij}^{k\ell}(s): \mathcal{M}_{0,b}(\mathcal D_{k,i}) \mapsto \mathcal{M}_{0,b}(\mathcal D_{\ell,j}), i,j \in \mathcal{S},\,k\in\mathcal K_i^+$ and $\ell \in \mathcal K_j^-$, is given by  
% 
\begin{align*} 
    % 
	&\mu_{k,i}\mathbb\Psi_{ij}^{k\ell}(s) (\mathcal{E}) 
	\\& := \int_{x \in \calD_{k,i}} \wrt \mu_{k,i}(x)
	% 
	 \mathbb{E}\big[e^{-s\zeta_Y(\{0\})}1\left(X({\zeta_Y(\{0\})}) \in \mathcal{E}\cap\calD_{\ell,j},\; \varphi({\zeta_Y(\{0\})}) = j \right) \mid X(0) = x, 
	 \\&\qquad{} \dot Y(0) = 0, \varphi(0) = i\big].
	 % 
\end{align*} 
\cite{bo2014}~Theorem~1 provides the following result which characterises \(\mathbb\Psi(s)\).
\begin{theo} 
	\label{theo:Psi} 
	For \textit{Re}$(s) \geq 0$, $\mathbb\Psi(s)$ satisfies the  equation: 
	% 
	\begin{align*} 
		\mathbb{D}^{+-}(s) + \mathbb\Psi(s)\mathbb{D}^{-+}(s)\mathbb\Psi(s) + \mathbb{D}^{++}(s)\mathbb\Psi(s) + \mathbb\Psi(s)\mathbb{D}^{--}(s) = 0. 
	\end{align*} 
	% 
	Furthermore, if $s$ is real then $\mathbb\Psi(s)$ is the minimal non-negative solution. 
\end{theo} 

\subsection{Limiting Distribution} 

Let $\mathbb\Psi := \mathbb\Psi(0)$. We define $\zeta_Y^n(\{0\}) := \inf\{t \geq \zeta_Y^{n - 1}(\{0\}): \dot Y(t) = 0\}$, for $n \geq  2$, to be the sequence of hitting times to level $0$ of $\dot Y(t)$, with $\zeta_Y^1(\{0\}): = \zeta_Y(\{0\})$. Consider the discrete-time Markov process $\{X({\zeta_Y^n(\{0\})}), \varphi(\zeta_Y^n(\{0\}))\}_{n \geq 1}$, and for $i \in \mathcal{S},\,k\in\mathcal K_i^-$ define the measures $\bbxi_{k,i}$ given by 
	%
	\begin{align*}
		\bbxi_{k,i}(\mathcal{E}) := \lim_{n \rightarrow \infty} \mathbb{P}\left(X(\zeta_Y^n(\{0\})) \in \mathcal{E}\cap\calD_{k,i}, \varphi(\zeta_Y^n(\{0\})) = i\right).
	\end{align*} 
	By \cite{bo2014}, the vector of measures $\boldsymbol{\bbxi} := (\bbxi_{k,i})_{i \in \mathcal{S}_k^-,k\in\mathcal K^-}$ satisfies the set of equations 
 % 
 	\begin{align}
		\vligne{\boldsymbol{\bbxi}  & \boldsymbol{0}}\left(-\left[\begin{array}{ll} 
			\mathbb{B}^{--} & \mathbb{B}^{-0} \\
			\mathbb{B}^{0-} & \mathbb{B}^{00} 
		\end{array} \right]\right)^{-1}\left[\begin{array}{l} 
			\mathbb{B}^{-+} \\ 
			\mathbb{B}^{0+}
		\end{array} \right]\mathbb\Psi & = \boldsymbol{\bbxi}, \label{eqn:xi1}\\ 
		\sum_{k\in\mathcal K^-}\sum_{i \in \mathcal{S}_k^-}\bbxi_{k,i}(\mathcal{F}^-_i) & = 1. \label{eqn:xi2}
	\end{align} 

We reproduce Theorem 2 of \cite{bo2014} below, which gives the joint limiting distribution of $\{(\dot Y(t),X(t),  \varphi(t))\}$. Recall that the joint limiting density operator ${\bbpi}(y) = (\bbpi_i(y))_{i \in \mathcal{S}}$ for $\{(\dot Y(t), X(t), \varphi(t))\}$ and the joint limiting mass operator ${\mathbb p} = (\mathbb p_i)_{i \in \mathcal{S}}$ are defined by~\eqref{eqn:jointpi} and \eqref{eqn:jointmass}, respectively. %Following out notational convention, define an equivalent joint limiting density operator $\boldsymbol{\bbpi}(y) = (\bbpi_i(y))_{i \in \mathcal{S}}$ for $\{X(t), Y(t), \varphi(t)\}$ and an equivalent joint limiting mass operator $\boldsymbol{\mathbb p} = (\mathbb p_i)_{i \in \mathcal{S}}$, but partitioned accprsing . 
We can partition \(\bbpi\) so that 
% 
	\begin{align*} 
		\bbpi(y) &= \vligne{\bbpi^{+}(y) & \bbpi^{-}(y) & \bbpi^{0}(y)} 
		\\&= \vligne{\left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^+,k\in\mathcal K^+} & \left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^-,k\in\mathcal K^-} & \left(\bbpi_{k,i}(y)\right)_{i \in \mathcal{S}_k^0,k\in\mathcal K^0}},
	\end{align*} 
	%
	where  
% 
	\begin{align*} 
		\bbpi_{k,i}(y)(\mathcal{E}) = \bbpi_i(y)(\mathcal{E}\cap \mathcal D_{k,i}).
	\end{align*} 
%
Similarly, we can write 
	\begin{align*} 
		\mathbb{p} &= \vligne{\mathbb{p}^{-} & \mathbb{p}^{0}} 
		= \vligne{\left(\mathbb p_{k,i}\right)_{i \in \mathcal{S}_k^-,k\in\mathcal K^-}  & \left(\mathbb p_{k,i}\right)_{i \in \mathcal{S}_k^0,k\in\mathcal K^0} },
	\end{align*} 
	%
	where  
% 
	\( 
		\mathbb p_{k,i}(\mathcal{E}) = \mathbb p_i(\mathcal{E}\cap \mathcal D_{k,i}).
	\) 
%
\begin{theo} 
	\label{theo:density} 
The operator ${\bbpi}^{m}(y)$, for $m \in \{+,-,0\}$ and $y > 0$, and the probability mass $\mathbb{p}^{m}$, for $m \in \{-,0\}$, satisfy the set of equations:
% 	 
	\begin{align} 
	&\label{11} \; \bbpi^{0}(y) = \vligne{\bbpi^{+}(y) & \bbpi^{-}(y)}\left[\begin{array}{l} \mathbb{B}^{+0} \\ \mathbb{B}^{-0} \end{array} \right]\left(-\mathbb{B}^{00}\right)^{-1}, \\
	% 
	&  \vligne{\bbpi^{+}(y) & \bbpi^{-}(y)} = \vligne{\mathbb{p}^{-} & \mathbb{p}^{0}}\left[\begin{array}{l} \mathbb{B}^{-+} \\ \mathbb{B}^{0+} \end{array} \right]\vligne{e^{\mathbb{K}y} & e^{\mathbb{K}y}\mathbb\Psi}\left[\begin{array}{cc} \mathbb{R}^{+} & 0 \\ 0 & \mathbb{R}^{-}\end{array}\right], \\
	% 
	&  \vligne{\mathbb{p}^{-}  & \mathbb{p}^{0}} = z \vligne{{\bbxi} & \bs{0}} 
	\left(-\left[\begin{array}{ll} 
		\mathbb{B}^{--} & \mathbb{B}^{-0} \\
		\mathbb{B}^{0-} & \mathbb{B}^{00} 
		\end{array} \right] \right)^{-1},  \label{eqn:mass}\\
	%  
	& \sum_{m \in \{+,-,0\}}\sum_{i \in \mathcal{S}} \int_{y = 0}^{\infty} \bbpi_i^{m}(y)(\mathcal{F}^{m}_i)\wrt y + \sum_{m \in \{-,0\}} \sum_{i \in \mathcal{S}}\mathbb p^{m}_i(\mathcal{F}^{m}_i) = 1, \label{14}
	\end{align}
	% 
	where $\mathbb{K} := \mathbb{D}^{++}(0) + \mathbb\Psi\mathbb{D}^{(-+)}(0)$ and $z$ is a normalising constant. 
\end{theo} 

At this point we reiterate that Equations~(\ref{11})-(\ref{14}) are operator equations and are only amenable to numerical evaluation in the simplest of cases. Sources of this intractability come from, for example, the need to find the inverse operator \(( - \mathbb B^{00})^{-1}\), and the need to find the solution, \(\mathbb \Psi(s)\), of the operator equation in Theorem~\ref{theo:Psi}. There is also the complexity of the partition of the operators defined by the sets \(\mathcal F_{i}^m,\, i\in\calS,\,m\in\{+,-,0\}\). Therefore, there is the need for approximation schemes such as those presented in this thesis. 


\section{Discontinuous Galerkin}
The discontinuous Galerkin (DG) method is a general methodology for numerically approximating solutions to differential equations. The discontinuous Galerkin method is a finite-element method which approximates differential operators by finite dimensional matrices. In this thesis we use the discontinuous Galerkin method to approximate the infinitesimal generator of a fluid queue. Here, we give a very brief introduction to discontinuous Galerkin methods in general and refer the reader to Chapter~\ref{ch:galerkin} for details on the application to fluid queues, and to \cite{nodalDGBook} for more details on the method in general. 

Intuitively, the DG method works by first partitioning the spatial domain into cells, on each cell a basis of (typically polynomial) functions is defined, then the differential equation is projected onto the basis of polynomials. Thus, the differential equation is discretised it two ways, by the partition of the state space into cells, then by the projection onto a finite dimensional basis. Linking the cells together essentially amounts to defining boundary conditions for each cell. To form the boundary condition for cells adjacent to the boundary we take the boundary condition defined by the problem we are solving and discretise it by projecting it onto a set of basis functions. For cells in the interior the boundary condition is defined by the solutions on adjacent cells. In the context of conservation laws (which is what we solve in this thesis), the boundary conditions on each cell are known as the \emph{flux}, as they describe the rate, with respect to time, that the conserved quantity flows across the boundaries of the cells. The flux is discretised and is known as the \emph{numerical flux}.

Typically, polynomial functions are used as the basis for the DG method. The main choices to make in defining the scheme are the grid which defines the cells, the order of the polynomial basis, and the choice of numerical flux. 

\cite{nodalDGBook} claim that the first discontinuous Galerkin finite element scheme was developed in \cite{reed1973} where they use the method to solve the first order differential equation
\[\sigma u + \nabla \cdot (a u) = f,\]
in a 2-dimensional spatial domain, where \(\sigma\) is a constant, \(a\) is a piecewise constant function, and \(u\) is the unknown. The literature has since expanded and includes (but is not limited to) theory on the convergence rate of the scheme, the choice of \emph{numerical flux} used (we will define this later), the choice of spatial discretisation used, and applications to a range of other problems including higher-order differential equations and to partial differential equations (PDEs) such as non-linear conservation law. The PDE which describes the evolution of the distribution of a fluid queue is a conservation law as probability must be conserved. 

In one dimension, linear conservation laws take the form 
\begin{align}
	&\cfrac{\partial u( x, t)}{\partial t} - \cfrac{\partial}{\partial x} ( c(t) u( x, t)) = 0,\,  x \in \Omega, \label{eqn: cons law}
	\\& u( x,t)= g( x,t),\, x \in \partial \Omega\tag{boundary condition}
	\\& u( x,0)=f( x).\tag{initial condition}
\end{align}
where \(\Omega\) is the region of interest and \(\partial \Omega\) its boundary. Here \(u\) is the conserved quantity, which we will refer to as \emph{mass}, but it could be any conserved quantity. 

To solve conservation laws with the DG method the spatial domain \(\Omega\) is partitioned into cells (intervals) \(\mathcal D_k = [y_k,y_{k+1}]\), \(k=0,...,K-1\). For each cell \(\calD_k\) we choose \(p_k\) linearly independent functions \(\{\phi^r_k\}_{r=1}^{p_k}\), compactly supported on \(\calD_{k}\) (i.e.~\(\phi^r_k(x)=0\) for \(x\notin\calD_{k}\)). Typically, we take \(\phi_k^r(x)\) to be a basis of polynomial functions. The functions \(\{\phi^r_k\}_{r=1}^{p_k}\) span some function space, \(W_k\) say, and further form a basis for this space. It is convenient in this work to take \(\{\phi^r_k\}_{r=1}^{p_k}\) as a basis of \emph{Lagrange interpolating polynomials} defined by the \emph{Gauss-Lobatto quadrature points} \citep{nodalDGBook} (see also Section~\ref{eqn: la}). For the sake of illustration, the reader may think of \(\{\phi^r_k\}_{r=1}^{p_k}\) as the Lagrange polynomials. On each cell \(\mathcal D_{k}\) we approximate 
\[u(x,t)\approx u_{k}(x,t)=\sum\limits_{r=1}^{p_k}a_{k}^r(t)\phi^r_k(x),\, x \in\calD_k,\] 
where \(a_{k}^r(t)\) are yet-to-be-determined time-dependent coefficients. The approximation \(u_{k}\) is referred to as the \textit{local} approximation on cell \(k\), while the \textit{global} approximation is given by \(\sum\limits_{k=0}^{K-1} u_{k}\) on the whole domain \(\Omega\). The whole approximation space is \(\bigoplus\limits_{k\in\mathbb Z} W_k\), the direct sum of the spaces \(W_k\), \(k\in\{0,...,K-1\}\).

Now, consider multiplying (\ref{eqn: cons law}) by the row vector \(\bs \phi_k(x)=()\phi_k^1(x),...,\phi_k^{p_k}(x))\) and integrating by parts over \(\calD_k\),
\begin{align}
	&\int_{x\in \calD_k}\cfrac{\partial u(x,t)}{\partial t}\bs\phi_k(x)\wrt x + \int_{x\in \calD_k} c(t) u(x,t)\cfrac{\partial}{\partial x} \bs \phi_k( x)\wrt x - c(t)[u(x,t)\bs \phi_k(x)]_{x=y_k}^{y_{k+1}} = 0.\label{eqn: hadndjj}
\end{align}
The term \(- c(t)[u(x,t)\bs \phi_k(x)]_{x=y_k}^{y_{k+1}}\) is known as the \emph{flux}, and it describes the rate at which mass flows into and out of the cell \(\calD_k\). The flux is essentially a boundary condition on the cell \(\calD_k\) and is specified by the value of the solution on the adjoining cells. When \(c(t)\) is positive (negative) then mass flows into (out of) cell \(\calD_k\) at the left-hand boundary, and out of (into) cell \(\calD_k\) at the right-hand boundary. Suppose that \(c(t)\) is positive. When \(u\) is continuous, then \(u(y_{k}^-,t)=u(y_k,t),\) where \(y^-\) is the left limit at \(y\), hence the flux into cell \(\calD_k\) in Equation (\ref{eqn: cons law}) is \(c(t)u(y_k^-,t)\bs \phi_k(y_k)\). The flux out of cell \(\calD_k\) is \(c(t)u(y_{k+1}^-,t)\bs \phi_k(y_{k+1})\). Using this, rewrite (\ref{eqn: hadndjj})
\begin{align}
	&\int_{x\in \calD_k}\cfrac{\partial u(x,t)}{\partial t}\bs\phi_k(x)\wrt x + \int_{x\in \calD_k} c(t) u(x,t)\cfrac{\partial}{\partial x} \bs \phi_k( x)\wrt x \nonumber
	\\&\qquad\qquad{}- c(t)[u(y_{k+1}^-,t)\bs \phi_k(y_{k+1}) - u(y_{k}^-,t)\bs \phi_k(y_{k})] = 0.\label{eqn: hadndjjj}
\end{align}
We can proceed analogously when \(c(t)\) is negative. 

Upon substituting the approximations \(u_k\) and \(u_{k-1}\) into (\ref{eqn: hadndjjj}) 
\begin{align}
	&\int_{x\in \calD_k}\cfrac{\partial\bs a_k(t)}{\partial t}\bs \phi_k(x)\tr{}\bs\phi_k(x)\wrt x + \int_{x\in \calD_k} c(t) \bs a_k(t)\bs\phi_k(x)\tr{}\cfrac{\partial}{\partial x} \bs \phi_k( x)\wrt x \nonumber
	\\&\qquad\qquad{}- c(t)[\bs a_k(t)\bs\phi_k(y_{k+1}^-)\tr{}\bs \phi_k(y_{k+1}) - \bs a_{k-1}(t)\bs\phi_{k-1}(y_{k}^-)\tr{}\bs \phi_k(y_{k})].\nonumber
	\\&=\cfrac{\wrt \bs a_k(t)}{\wrt t}\bs M_k + c(t) \bs a_k(t)\bs G_k - c(t)[\bs a_k(t)\bs F_{k,k} - \bs a_{k-1}(t)\bs F_{k-1,k}]\nonumber
	\\&= 0,\label{eqn: hadndj}
\end{align}
where 
\[\bs M_k := \int_{x\in\calD_k}\boldsymbol \phi_k(x)\tr{}\boldsymbol \phi_k(x)\wrt x,\quad \bs G_k := \int_{x\in\calD_k}\boldsymbol \phi_k(x)\tr{}\cfrac{\wrt}{\wrt x} \boldsymbol \phi_k(x)\wrt x,\]
\[\bs F_{k,k} := \bs\phi_k(y_{k+1}^-)\tr{}\bs \phi_k(y_{k+1}),\quad \bs F_{k-1,k} := \bs\phi_{k-1}(y_{k}^-)\tr{}\bs \phi_k(y_{k}),\]
and are known as the \emph{mass}, \emph{stiffness} and \emph{flux} matrices. Thus, the approximation has reduced the problem to a system of ordinary differential equations. One can determine the coefficients \(\bs a_k(t)\), \(k=0,...,K-1\), from the initial condition \(\bs a_k(0)\), \(k=0,...,K-1\), and integrating over time, perhaps numerically. The initial coefficients \(\bs a_k(0)\), \(k=0,...,K-1\), can be found by projecting the initial condition onto the basis of polynomial functions, \(\{\phi_k^r\}_{k,r}\). 

One subtlety is the choice of numerical flux. We arrived at the numerical flux above using a continuity argument. However, the approximate solution \(\sum_{k=0}^{K-1}u_k(x,t)\) is discontinuous at the points \({y_{k}}\). It turns out that the choice of numerical flux is not unique and there are range of values we may choose from. The numerical flux above is known as the \emph{upwind} flux -- we use it throughout this thesis and do not explore others. 

Other choices we have made to define our discontinuous Galerkin scheme are the basis functions used and the cell geometries. We refer the reader to \citep{nodalDGBook} and reference therein for more details. 

Typically, DG schemes are used to solve physical problems in 2 or 3 dimensions. The cell geometries become more complex in higher dimensions and this complicates matters somewhat, but the underlying principles and intuition is the same. In this thesis we solve problems with one spatial dimension only.

Two properties which make the DG scheme a natural choice to solve the problem present in this thesis are: high-order accuracy enabled by the use of basis functions to model the solution on each cell, and the cell-based structure of the method with cells related to each other at cell edges only and only to neighbouring cells. Finite difference methods can achieve high-order accuracy, however they are not cell-based, which makes them not a natural choice to solve the fluid-fluid queue problems in this thesis. Finite volume methods are cell-based, but do not have the same high-order accuracy as the DG method. Further, approximating flux terms with greater accuracy in the finite volume method can introduce dependencies between cells which are not neighbours and this creates difficulties when partitioning the approximate operator which we need to do for our application to fluid-fluid queues. 

\subsection{Time-integration schemes}\label{sec: time integration}
In this thesis we numerically integrate ODEs of the form
\begin{align}\label{eqn: DG ODE w BCs 3}
	\cfrac{\wrt}{\wrt t} \boldsymbol a(t)
	% 
	= \boldsymbol a(t) \bs Q
\end{align}
where \(\bs a(t)\) is a vector of coefficients and \(\bs Q\) a matrix, given an initial condition \(\bs a(0)\). To do so, we employ the strong stability preserving Runge-Kutta (SSPRK) scheme of order 4 with 5 stages \citep{sr2002}. SSPRK methods are a family of Runge-Kutta methods which, to quote Section~5.7 of \cite{nodalDGBook}, ``...can be used with advantage for problems with strong shocks and discontinuities, as they guarantee that no additional oscillations are introduced as part of the time-integration process.'' 

In our context, the SSPRK method with \(s\) stages has the form 
\begin{align*}
	&\bs v^{(0)} = \bs a(t),\\
	%
	&\bs v^{(\ell)} = \sum_{k=0}^{\ell-1} \alpha_{\ell k}\bs v^{(k)} + h\beta_{\ell k}  \bs v^{(k)}\bs Q,\, \ell = 1,\dots,s,\\
	%
	&\bs a(t+h) =\bs v^{(s)},
\end{align*}
where \(\alpha_{\ell k}\) and \(\beta_{\ell k}\) are coefficients which define the scheme and \(h\) is the \(t\)-step size. To ensure the stability-preserving property the coefficients \(\alpha_{\ell k}\) and \(\beta_{\ell k}\) are chosen to be positive so that \(\bs v^{(s)}\) is a convex combination of forward-Euler steps. Moreover, the coefficients \(\alpha_{\ell k}\) and \(\beta_{\ell k}\) are optimised so that the allowable \(t\)-step size is as large as possible. The maximum allowable \(t\)-step size is \(h_{RK}\), and we require 
\[h_{RK} \leq \min_{\ell k}\cfrac{\alpha_{\ell k}}{\beta_{\ell k}}h_E,\]
where \(h_E\) is the maximum allowable \(t\)-step size for the forward-Euler scheme. 

The \(t\)-step size needs to be chosen to be sufficiently small so that the time integration is stable. Here, for an order \(p\) DG scheme we choose the time step to be less than 
\[\max_{i\in\calS}|c_i|\min_{r\in{2,...,p+1}}\left(z_{r}-z_{r-1}\right) \cfrac{\min_{k}{\Delta}_k}{2}\]
where \(z_{r-1}\leq z_r\), \(z_1=-1\), \(z_{p+1}=1\) and \(z_r, r=2,...,p,\) are the \(p-1\) zeros of the first derivative of \(P_{p}(x)\), the order \(p\) Legendre polynomial. This follows advice from \cite{nodalDGBook}. 

\subsection{Slope limiters}\label{sec: slope limiting}
A well-know problem with DG schemes is that, in the presence of steep gradients or discontinuities, spurious oscillations in the approximate solution can occur and positivity is not guaranteed (see \cite{nodalDGBook}~Section~5.6, and \cite{koltai2011}~Section~3.3, for example). To rectify this, in some contexts, slope limiting can be used (see \cite{c99}, or \cite{nodalDGBook},~Section~5.6.2 and references therein). Slope limiting alters the DG approximation in regions where oscillations are detected to ensure the approximate solution is non-oscillatory. It does so by reducing the order of the approximation to linear and then limiting the gradient of the approximation in these regions. However, limiting does not distinguish between natural oscillations, which are a fundamental feature of the solution, and spurious oscillations, which are caused by the approximation scheme. As a result, the limiter may unnecessarily reduce the accuracy of the scheme in the presence of natural oscillations (see \citep{nodalDGBook},~Example~5.8). Furthermore, slope limiting adds an extra computational cost on top of the approximation scheme and means that the resultant approximation to certain operators are no longer linear. This is of consequence in our application to fluid-fluid queues where we approximate an operator-Riccati equation by a matrix-Riccati equation, which is only possible when the approximate operators are matrices. 

We now describe a simple slope limiter known as the \emph{Generalised MUSCL} scheme \citep{c99} (see also \citep{nodalDGBook},~Section~5.6.1). Define the \emph{minmod} function 
\begin{align*}
	m(a_1,a_2,a_3)=\begin{cases}
		s\min_{i\in\{1,2,3\}}|a_i|, & |s|=1, \\
		0, &\mbox{otherwise},
	\end{cases}
\end{align*}
where \(s=\cfrac{1}{3}\sum\limits_{i=1}^{3}\mbox{sign}(a_i)\). When all three arguments, \(a_1,\,a_2\) and \(a_3\) have the same sign, the minmod function returns the smallest of the three arguments with the correct sign, otherwise the signs of the three arguments differ and the minmod function returns zero. In the context of DG the three arguments \(a_1,\,a_2\) and \(a_3\) are the estimates of the average gradient of the approximate solution near a given cell. When the signs of \(a_1,\,a_2\) and \(a_3\) differ, this suggests an oscillation in the approximate solution. 

Now, define \(\overline u_{k}\) as the average value of the approximate solution on cell \(k\), then the slope limited solution on cell \(k\) is the linear approximant 
\begin{align*}
	\Pi_{k}^{lim}(u_k)=\overline u_{k} + (x-\overline y_k)m\left(\Pi^1_{\partial x}u_k,\cfrac{\overline u_{k+1}-\overline u_{k}}{\Delta_k},\cfrac{\overline u_{k}-\overline u_{k-1}}{\Delta_k}\right),
\end{align*}
where \(\Pi^1_{\partial x}u_k\) is the slope of the linear projection of \(u_k\) on cell \(k\) and \(\overline y_k=(y_k+y_{k+1})/2\) is the centre of the \(k\)th cell. 

In non-oscillatory regions of the solution the slope limiter is unnecessary and reduces accuracy, so we only apply the limiter to cells \(k\) where oscillations are detected. To determine which cells need limiting we compute 
\begin{align*}
	v_k^L &:= \overline{u}_{k} - m(\overline{u}_{k} - {u}_{k}^L,\overline{u}_{k}-\overline{u}_{k-1},\overline{u}_{k+1}-\overline{u}_{k}),
	\\v_k^R &:= \overline{u}_{k} + m(\overline{u}_{k} - {u}_{k}^R,\overline{u}_{k}-\overline{u}_{k-1},\overline{u}_{k+1}-\overline{u}_{k}),
\end{align*}
where \({u}_{k}^L\) and \({u}_{k}^R\) are the values of the approximate solution on cell \(k\) evaluated at the left-hand and right-hand edges of the cell. We apply the slope limiter to cell \(k\) if \(v_k^L\neq u_k^L\) or \(v_k^R\neq u_k^R\), otherwise the approximation on cell \(k\) is left unaltered. 

% Notice that the process of limiting is essentially a post-processing of the approximate solution after applying 

% In the context of approximating the first return operator of a fluid-fluid queue there is no direct equivalent of slope limiting, other than, perhaps, to re-compute the solution with constant basis function if a higher-order approximation happens to be oscillatory, but this is a post-hoc solution which would essentially require computing two solutions. 

\subsection{Slope limiters and time integration}\label{subsec: slope lim and int}
To use slope limiters within the SSPRK scheme we first project the initial condition on to \(W\), the set of polynomial basis functions, and apply the slope limiter to the projection to determine the initial coefficients \(\bs a(0)\). To find the coefficients for the approximate solution to the transient distribution at time \(t_0\) we take the (limited) initial condition \(\bs a(0)\) and evolve it forward in time until \(t_0\) via numerical integration of the differential equation~(\ref{eqn: DG ODE w BCs 3}). At each stage of the time-integration we apply the slope limiter, i.e.~the scheme above with a slope limiter is 
\begin{align*}
	&\bs v^{(0)} = \bs a(t),\\
	%
	&\bs v^{(\ell)} = \Pi^{lim}\left(\sum_{k=0}^{\ell-1} \alpha_{\ell k}\bs v^{(k)} + h\beta_{\ell k}  \bs v^{(k)}\bs Q\right),\, \ell = 1,\dots,s,\\
	%
	&\bs a(t+h) =\bs v^{(s)},
\end{align*} 
where \(\Pi^{lim}\) is the slope limiter function which determines on which cells the solution needs limiting, and if so, applies the limiter \(\Pi_k^{lim}\). 

We refer to this (vanilla) application of a limiter to the DG scheme as the DG-lim scheme. 

\paragraph{Consequences of slope limiting} We have already mentioned that the Generalised MUSCL slope limiter reduces the order to linear in regions where oscillatory solutions are detected. When slope limiting is used in conjunction with a time-integration scheme the reduction in order may not be local in time and may persist past the time when the limiter first detected oscillations. For example, consider a problem with an initial condition which introduces oscillations into the numerical approximation. If the slope limiter is applied to the initial condition then, in the region around the oscillations, the approximation to the initial condition will be linear. Even if the slope limiter is not applied (or not needed) after this point, the initial condition has still been altered, perhaps significantly, from the original approximation and this can affect the accuracy of transient approximations. This is not to say that the slope-limited-regions of the approximation will \emph{always} remain linear. When no oscillations are detected, the DG scheme can use the full power of the high-order polynomial basis to approximate the solution, and this is one of the advantages of generalised slope limiter described. Instead, we want to point out that once the limiter is applied at one time point, the approximation becomes linear which will affect the accuracy at subsequent time points. 

\subsection{Another slope limiting scheme}\label{sec: anoth pos pres}
The advantage of the approach above is that, in areas of the approximation where the solution is smooth, then the high-order accuracy of the method can be retained, while maintaining positivity where necessary. However, if the problem at hand is dominated by discontinuities and/or if there are no regions of the solution of interest which are smooth, then there is no real advantage in the approach above as the limiter will reduce the approximate solution to linear in all regions of interest. Hence, we may as well just use a linear scheme and save on some computation. 

Perhaps a better approach would be to use a linear scheme with a smaller cell width such that the computational work remains approximately the same. For example, for a DG scheme with \(2p\) basis functions on a cell, \(\mathcal D_k\), of width \(\Delta\), say, we construct block matrices of dimension \(2p\times 2p\) (i.e.~the mass, stiffness and flux matrices, \(\bs M_k\), \(\bs G_k\), and \(\bs F^{k\ell}\), respectively) and compute the approximate solution using these matrices. Alternatively, consider breaking the cell, \(\mathcal D_k\), up into \(p\) sub-cells of width \(\Delta/p\) and using a DG scheme with \(2\) basis functions on each sub-cell (i.e.~a linear basis on each cell). On each of the \(p\) sub-cells we construct block matrices of size \(2\times 2\). To approximate the solution on the original interval \(\mathcal D_k\) we form the \(p\) approximations on each sub-cell into larger matrices which are of dimension \(2p\times 2p\), and then use in the same way as before. Thus, the scheme with \(2p\) basis functions on the cell \(\calD_k\), and the scheme with a 2 basis functions on each of the \(p\) sub-cell have approximately the same computational cost (the latter scheme results in a sparser matrix). 

If we know \emph{a priori} that we will apply the slope limiter to cell \(\calD_k\), then intuitively we suspect that the latter scheme may be far more accurate; it will approximate the solution on \(\calD_k\) by a piecewise linear function with \(p\) pieces, whereas the DG scheme with \(2p\) basis functions on cell \(\calD_k\) will, after limiting, represent the solution as a single linear function on \(\calD_k\). 

We refer to this scheme as the DG-lin-lim scheme, as it is a linear DG scheme with a limiter.

\subsection{Briefly, on accuracy}
As a rough guide on accuracy we now paraphrase some known results on the accuracy of the DG method from \cite[Sections~5.5,~5.8]{nodalDGBook} for a similar kind of problem to the one under study here. For certain scalar conservation-law problems, under certain regularity conditions on the continuity and boundedness of the solution and its derivatives, on the grid used, on the flux function, and on the numerical flux, then one can expect accuracy proportional to \(\Delta^{p}\) for the DG approximation of the solution, provided the DG scheme uses a polynomial basis with \(p\) functions on each cell, a regular grid of \(\Delta = \max \Delta_k\) is used and a second-order SSPRK method is used to integrate over time \citep[Sections~5.5,~5.8, and references therein]{nodalDGBook}. Applying this result to the DG-lin-lim scheme in Section~\ref{sec: anoth pos pres}, we might expect accuracy of order \((\Delta/p)^2\) under the same regularity conditions.

% To quote Section~5.7 of \cite{nodalDGBook} SSPRK methods ``can be used with advantage for problems with strong shocks and discontinuities, as they guarantee that no additional oscillations are introduced as part of the time-integration process.''

% why the MUSCL limiter?
%   it requires no tuning and guarentees non-negativity
%   some other positivity-preserving methods require tuning and can still produce negative results if not tuned properly
%   ^ same goes for filtering

% Consider the partial differential equation 
% \[\cfrac{\partial f}{\partial t}+\cfrac{\partial c(f)}{\partial x}=0,\, x\in[a,b],\]
% where \(c\) is some function, and subject to appropriate initial, \(f(x,0)=f_0(x)\), and boundary conditions, \(f(a,t)=g(t),\) assuming \(c\geq 0\). The method first supposes that the solution is of the form \(f(x,t)\) translates the above differential equation, which is in \emph{strong form}, into a \emph{weak form} by multiplying by a test function \(\psi\), integrating with respect to \(x\) and applying integration by parts; 
% \begin{align*}
% 	% &\int_{x=a}^b\cfrac{\partial f(x)}{\partial t}\psi(x)\wrt x+\int_{x=a}^b\cfrac{\partial c(f)}{\partial x}\psi(x)\wrt x
% 	\int_{x=a}^b\cfrac{\partial f(x)}{\partial t}\psi(x)\wrt x+\int_{x=a}^b c(f)\cfrac{\wrt\psi(x)}{\wrt x}\wrt x - [c(f(x,t))]_{x=a}^b = 0. 
% \end{align*}
% The weak form 

\section{Quasi-birth-and-death processes with rational arrival process components}\label{qbd-rap intro}
One of the approximation schemes we develop in this thesis is based on a suitably defined quasi-birth-and-death process with rational-arrival-process components (QBD-RAP). QBD-RAP processes are built from matrix-exponentially distributed inter-event times. We now introduce the class of matrix exponential distributions and recount some important properties before introducing the QBD-RAP. 

\subsection{Matrix exponential distributions}
Here we recount some facts about matrix exponential distributions. See \cite{MEinAP} for a more detailed exposition. A random variable, \(Z\), is said to have a matrix exponential distribution if it has a distribution function of the form \(1-\bs \alpha e^{\bs Sx}(-\bs S)^{-1}\bs s\), where \(\bs \alpha\) is a \(1\times p\) \emph{initial vector}, \(\bs S\) a \(p\times p\) matrix, and \(\bs s\) a \(p\times 1\) \emph{closing vector}, and \(\displaystyle e^{\bs S x} := \sum_{n=0}^\infty \cfrac{\left(\bs Sx\right)^n}{n!}\) is the matrix exponential. The density function of \(Z\) is given by \(f_Z(x) = \bs\alpha e^{\bs S x}\bs s\). The only restrictions on the parameters \((\bs \alpha, \bs S, \bs s)\) are that \(\bs \alpha e^{\bs S x} \bs s\) be a valid density function, i.e.~\(\bs \alpha e^{\bs S x} \bs s\geq 0,\) for all \(x\geq 0\) and \(\lim_{x\to\infty} 1-\bs \alpha e^{\bs Sx}(-\bs S)^{-1}\bs s = 1.\) In general, in \cite{MEinAP}, there is the possibility of an \emph{atom} (a point mass) at 0, but here we do not consider this possibility. The condition that \(\bs \alpha e^{\bs Sx}\bs s\) be a valid density imposes some properties on representations \((\bs \alpha, \bs S, \bs s)\). However, in general there is no way to determine whether, a given triplet \((\bs \alpha, \bs S, \bs s)\) is a representation of a matrix exponential distribution, or not. Nonetheless, some properties of a triplet \((\bs \alpha, \bs S, \bs s)\) are known, such as the following, which is used in the characterisation of QBD-RAPs. 
\begin{thm}[Theorem 4.1.3, \cite{MEinAP}]
	The density function of a matrix exponential distribution with representation \((\bs\alpha, \bs S, \bs s)\) can be expressed in terms of real-valued constants as 
	\begin{align}
		\psi(x)&=\sum_{j=1}^{m_1} \sum_{k=1}^{p_j} c_{jk} \cfrac{x^{k-1}}{(k-1)!} e^{\mu_jx} + \sum_{j=1}^{m+2}\sum_{k=1}^{q_j} d_{jk}\cfrac{x^{k-1}}{(k-1)!}e^{\eta_jx}\cos(\sigma_jx) \nonumber 
		\\&\quad{}+ \sum_{j=1}^{m_2}\sum_{k=1}^{q_j}e_{jk}\cfrac{x^{k-1}}{(k-1)!}e^{\eta_jx}\sin(\sigma_jx), \label{eqn: spec}
	\end{align}
	for integers \(m_1,\,m_2,\,p_j,\) and \(q_j\) and some real constants \(c_{jk},\,d_{jk},\,e_{jk},\,\mu_j,\,\eta_j,\) and \(\sigma_j\). Here \(\mu_j,\, j=1,\dots,m_1\) are the real eigenvalues of \(\bs S\), while \(\eta_j+i\sigma_j, \, \eta_j-i\sigma_j,\, j=1,\dots,m_2\) denote its complex eigenvalues, which come in conjugate pairs. Thus, \(m_1+2m_2\) is the total number of eigenvalues, while the dimension of the representation is given by \(\displaystyle p=\sum_{j=1}^{m+1}p_j + 2\sum_{j=1}^{m_2}q_j\). 
\end{thm}
\begin{thm}[Theorem 4.1.4, \cite{MEinAP}]\label{thm: 4.1.4}
	Consider the non-vanishing terms of the matrix exponential density (\ref{eqn: spec}), \emph{i.e.}, the terms for which \(c_{jk}\neq 0,\,d_{jk}\neq 0,\) or \(e_{jk}\neq 0\). Among the corresponding eigenvalues \(\lambda_j\), there is a real dominating eigenvalue \(\kappa\), say. That is, \(\kappa\) is real, \(\kappa \geq Re(\lambda_j)\) for all \(j\), and the multiplicity of \(\kappa\) is at least the multiplicity of every other eigenvalue with real part \(\kappa\).
\end{thm}
\begin{cor}[Corollary 4.1.5, \cite{MEinAP}]
If \((\bs \alpha, \bs S, \bs s)\) is a representation for a matrix exponential distribution, then \(\bs S\) has a real dominating eigenvalues.
\end{cor}
\begin{thm}[Theorem 4.1.6, \cite{MEinAP}]
	Let \(Z\) be a matrix-exponentially distributed random variable with density (\ref{eqn: spec}). Then the dominant real eigenvalue \(\kappa\) of Theorem~\ref{thm: 4.1.4} is strictly negative. 
\end{thm}
We define \(dev(\bs S)\) to be the real dominating eigenvalue of \(\bs S\), that is \(dev(\bs S)=\kappa\) in Theorem~\ref{thm: 4.1.4}.

The class of matrix exponential distributions is characterised as the class of probability distributions which have a rational Laplace transform. That is, \(\displaystyle \int_{x=0}^\infty e^{-\lambda x}\bs \alpha e^{\bs S x}\bs s \wrt x\) is a ratio of two polynomial functions in \(\lambda\). Matrix exponential distributions are an extension of Phase-type distributions, where for the latter, \(\bs S\) must be a sub-generator matrix of a CTMC, \(\bs s = -\bs S\bs e\) where \(\bs e\) is a \(1\times p\) vector of ones, and \(\bs \alpha\) is a discrete probability distribution.  

A \emph{representation} of a matrix exponential distribution is a triplet \((\bs \alpha, \bs S, \bs s)\), and we write \(Z\sim ME(\bs \alpha, \bs S, \bs s)\) to denote that \(Z\) has a matrix exponential distribution with this representation. The order of the representation is the dimension of the square matrix \(\bs S\), i.e.~if \(\bs S\) is \(p\times p\), then the matrix exponential distribution is said to be of order \(p\). Representations of matrix-exponential distributions are not unique \citep{MEinAP}. A representation is called \emph{minimal} when \(\bs S\) has the smallest possible dimension. Throughout this work, we assume that the representation of any matrix exponential distribution is minimal. Let \(\bs e_i\) be a vector with a 1 in the \(i\)th position and zeros elsewhere. We assume that \(\bs s = -\bs S\bs e\), and that \((\bs e_i,\bs S,\bs s)\) for \(i=1,\dots,p\) are representations of matrix exponential distributions. It is always possible to find such a representation \cite[Theorem 4.5.17, Corollary 4.5.18]{MEinAP}. As such, we abbreviate our notation \(Z\sim ME(\bs \alpha, \bs S, \bs s)\) to \(Z\sim ME(\bs \alpha, \bs S)\). Further, given \(\bs s=-\bs S\bs e\) then observe that  \(\displaystyle \int_{x=0}^\infty e^{\bs S x} \bs s\wrt x = (-\bs S)^{-1}\bs s= \bs e\). 

For a given \(p\times p\) matrix \(\bs S\), denote by \(\mathcal{A}\subset \mathbb R^p\) the space of all possible vectors \(\bs a\) such that \((\bs a, \bs S)\) is a valid representation of a (possibility defective) matrix exponential distribution. %By CITE it is known that \(\mathcal{E}\) is a closed, bounded, convex, affine subspace of \(\mathbb R^p\). 

\subsubsection*{Concentrated matrix exponential distributions}
Recently, the class of \emph{concentrated matrix exponential distributions} (CMEs) has been investigated \citep{ert2006,hstz2016,ert2006,hht2020,mt2021}. As the name suggests, concentrated matrix exponential distributions are matrix exponential distributions with a very low coefficient of variation (variance relative to the mean). As the order, \(p\), of the representation increases, the variance of concentrated matrix exponential distributions decreases at rate approximately \(\mathcal O(1/p^2)\). For comparison, the variance of an Erlang distribution, the most concentrated Phase-type distribution, decreases at rate \(\mathcal O(1/p)\) as the order \(p\) increases \citep{as1987}. In this thesis we use CMEs to approximate the distribution of deterministic events. For a given order, concentrated matrix exponential distributions have a much lower coefficient of variation than any Phase-type of the same order, and therefore a better ability to model determinism. 

The class of concentrated matrix exponential distributions (CMEs) is found numerically in \citep{hht2020} and computationally efficient expressions for the density and moments of CMEs are provided in \citep{hht2020}. 

CMEs exist for all orders, however, in our computations we only use CMEs with odd order. The justification for considering representations of odd orders only is that the variance of CME distributions of orders \(2p\) and \(2p-1\) are relatively similar and therefore have similar abilities to represent the delta function \citep{hht2020}. Hence, if we construct an approximation with a representation of order \(2p\) we expect it to perform only marginally better than an approximation constructed with representations of order \(2p-1\). However, the computational cost of the latter is lower, so we opt for the order \(2p-1\) representation. 

From the definition of the class of CME distributions, for a given CME with odd order, \(p\), and representation \((\bs\alpha,\bs S)\), the matrix \(\bs S\) has one real eigenvalue, and \(p-1\) complex eigenvalues and all eigenvalues have the same real part \cite{hht2020}. 

For CMEs the vector function \(\bs k(t)=\cfrac{\bs \alpha e^{St}}{\bs \alpha e^{St}\bs e}\) is periodic with period \(\rho = 2\pi/\omega\) where \(\omega=min_i(|\Im(\lambda_i)|)\), \(\lambda_i\) are the eigenvalues of \(\bs{S}\) and \(\Im(z)\) is the imaginary component of a complex number \(z\). We leverage this fact to simplify some numerical integration in Chapter~\ref{sec: construction and modelling}. 


%\subsection{Rational arrival processes}
%Let \(N\) be a simple point process, with event time \(Y_0=0<Y_1<Y_2<\cdot\). Let \(\{N(t)\}_{t\geq0}\) be the right-continuous counting process associated with \(N\); \(N(t)\) returns the number of events by time \(t\). Denote by \(f_{N,n}(y_1,\dots,y_n)\) the joint density of \(Y_1, Y_2-Y_1, \dots,Y_n-Y_{n-1}\), the first \(n\) inter-arrival times. For a matrix \(\bs B\) let \(dev(\bs B)\) denote the dominant eigenvalue of \(\bs B\) (the one with maximal real part). Theorem 1.1 of Asmussen and Bladt CITE states that the point process \(N\) is a RAP if there exists matrices \(\bs S\), \(\bs D\), a row vector \(\bs\alpha\), and a column vector \(\bs s\) such that \(dev(\bs S)<1\), \(dev(\bs S+\bs D)=0\), \((\bs S+\bs D)\bs e=0,\) and 
%\[f_{N,n}(y_1,\dots,y_n) = \bs \alpha e^{\bs Sy_1} \bs D e^{\bs Sy_1} \bs D \dots e^{\bs Sy_n} \bs s.\]
%Here \(\bs s\) can be taken to be \(\bs D e\). The \(n\)th inter-arrival, \(T_n-T_{n-1}\), has a matrix exponential distribution with density \(\bs \alpha(-\bs S\bs D )^{n-1}e^{\bs S y_n}\bs s\). Denote such a process \(N \sim RAP(\bs \alpha, \bs S, \bs D)\). 
%
%Asmussen and Bladt CITE, Corollary 2.2, show that associated with the RAP is a row-vector-valued \emph{orbit} process, \(\{\bs A(t)\}_{t\geq0},\), 
%\[\bs A(t) = \cfrac{\bs \alpha\left( \prod_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs S\right)e^{\bs S t-Y_{N(t)}}}{\bs \alpha\left( \prod_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs S\right)e^{\bs S( t-Y_{N(t)})}\bs e}.\]
%Thus, \(\{\bs A(t)\}\) is a piecewise-deterministic Markov process where, in between jumps \(\{\bs A(t)\}\) according to 
%\[\bs A(t) = \cfrac{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}}{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}\bs e},\]
%where \(\bs A(Y_{N(t)}^-) = \lim_{u\to 0^+}\bs A(Y_{N(t)}-u)\). The process \(\{\bs A(t)\}\) jumps at event times of \(N\). At time \(t\) the intensity with which \(\{\bs A(t)\}\) has a jump is \(\bs A(t) \bs D\bs e\), and upon a jump at time \(t\), the new position of the orbit is \(\bs A(t) = \bs A(t^-) \bs D/\bs A(t^-) \bs D\bs e\). 
%
%RAPs are an extension of Markovian arrival processes (MAPs) to include matrix-exponential inter-arrival times. For MAPs, the vector \(\bs A(t)\) is a vector of posterior probabilities of a continuous-time Markov chain. 
%
%Intuitively, \(\bs A(t)\) encodes all of the information about the jump times of the RAP up to time \(t\). Let \(\mathcal F_{t}\) be the \(\sigma\)-algebra generated by \(N(u), u\in[0,t]\). Then \(N\mid \mathcal F_t \equiv N\mid \bs A(t) \sim ME(\bs A(t), \bs S, \bs S)\). In words, the future of the point process after time \(t\) given all of the information about the process up to and including time \(t\), is distributed as a RAP with initial vector \(\bs A(t)\). 

\subsection{QBD-RAPs}\label{sec: qbd-rap}
% As RAPs are an extension of Markovian arrival processes to include matrix-exponential inter-arrival times, QBD-RAPs are extensions of QBDs to include matrix-exponential times between level changes. 

To define a QBD-RAP we first need a Batch (Marked) rational-arrival-process (RAP). Let \(\mathscr K\subset \mathbb Z\) be a set of \emph{marks}. Let \(N\) be a point process, and \(Y_0=0<Y_1<Y_2\cdots\) be event times of \(N\). Let \(\{N(t)\}\) be the counting process associated with \(N\) such that \(N(t)\) returns the number of events by time \(t\). Associated with the \(n\)th event is a mark \(M_n\). For \(i\in\mathscr K\), let \(N_i\) be simple point processes associated with events with marks of type \(i\) only, and let \(\{N_i(t)\}_{t\geq 0}\) be the associated counting processes of events of mark \(i\). Denote by \(f_{N,n}(y_1,m_1,y_2,m_2,\dots,y_n,m_n)\) the joint density, probability mass function of the first \(n\) inter-arrival times, \(Y_1,Y_2-Y_1,\dots,Y_n-Y_{n-1}\), and the associated marks \(M_n\). From Bean and Nielsen \cite[Theorem 1]{bn2010} we have the following. 
\begin{thm}
A process \(N\) is a Marked RAP if there exist matrices \(\bs S\), \(\bs D_i,\,i\in\mathscr K\), and a row vector \(\bs \alpha\) such that \(dev(\bs S)<0\), \(dev(\bs S+\bs D)=0\), \((\bs S+\bs D)\bs e = 0\), \(\bs D = \sum_{i\in\mathscr K} \bs D_i\), and 
\begin{align}f_{N,n}(y_1,m_1,y_2,m_2,\dots,y_n,m_n) = \bs \alpha e^{\bs S y_1}\bs D_{m_1} e^{\bs S y_2} \bs D_{m_2}\dots e^{\bs S y_n} \bs D_{m_n}\bs e.\label{eqn: brap}\end{align}
Conversely, if a point process has the property (\ref{eqn: brap}) then it is a Marked RAP.
\end{thm}

Denote such a process \(N\sim BRAP(\bs \alpha, \bs S, \bs D_i,\,i\in\mathscr K).\)

Also, from \cite{bn2010}, associated with a Marked RAP is a row-vector-valued \emph{orbit} process, \(\{\bs A(t)\}_{t\geq0},\)
\[\bs A(t) = \cfrac{\bs \alpha\left( \prod\limits_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs D_{M_i}\right)e^{\bs S (t-Y_{N(t)})}}{\bs \alpha\left( \prod\limits_{i=1}^{N(t)} e^{\bs S(Y_{i}-Y_{i-1})}\bs D_{M_i}\right)e^{\bs S( t-Y_{N(t)})}\bs e}.\]
Thus, \(\{\bs A(t)\}\) is a piecewise-deterministic Markov process where, in between events \(\{\bs A(t)\}\) evolves deterministically according to 
\[\bs A(t) = \cfrac{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}}{\bs A(Y_{N(t)}^-)e^{\bs S(t-Y_{N(t)})}\bs e},\]
where \(\bs A(Y_{N(t)}^-) = \lim_{u\to 0^+}\bs A(Y_{N(t)}-u)\). The process \(\{\bs A(t)\}\) can jump at event times of \(N\) (the process may not actually jump at these times, but we may still refer to it a `jump' and typically the dynamics change at this point). At time \(t\) the intensity with which \(\{\bs A(t)\}\) has a jump is \(\bs A(t) \bs D\bs e\), i.e.~\(\mathbb P(N(t)=n,N(t+dt)=n+1)=\bs A(t) \bs D\bs e\wrt t\). Upon an event the event is associated with mark \(i\) with probability \(\bs A(t) \bs D_i\bs e/\bs A(t) \bs D\bs e\). Upon on an event at time \(t\) with mark \(i\), the new position of the orbit is \(\bs A(t) = \bs A(t^-) \bs D_i/\bs A(t^-) \bs D_i\bs e\). It is important to note that the jumps of the orbit process are \emph{linear} transformations of the orbit process immediately before the time of the jump. 

Marked RAPs are an extension of Marked Markovian arrival processes to include matrix-exponential inter-arrival times. For Marked MAPs, the vector \(\bs A(t)\) is a vector of posterior probabilities of a continuous-time Markov chain. 

Intuitively, \(\bs A(t)\) encodes all the information about the event times of the Marked RAP and associated marks up to time \(t\) that is needed to determine the future behaviour of the point process. Let \(\mathcal F_{t}\) be the \(\sigma\)-algebra generated by \(N(u), u\in[0,t]\). Then \(N\mid \mathcal F_t \equiv N\mid \bs A(t) \sim BRAP(\bs A(t), \bs S, \bs D_i,i\in\mathscr K)\). In words, the future of the point process after time \(t\) given all the information about the process up to and including time \(t\), is distributed as a Marked RAP with initial vector \(\bs A(t)\). 

Now consider a Marked RAP, \(N\sim BRAP(\bs \alpha, \bs S, \bs D_i,\,i\in\{-1,0,+1\}).\) The process \(\{(L(t),\bs A(t))\}_{t\geq0}\) formed by letting \(L(t) = N_{+1}(t) - N_{-1}(t)\) is a QBD-RAP. QBDs are QBD-RAPs where the inter-event times are of Phase-type.


\section{Kronecker products and related results}
Here we describe Kronecker products and sums and some of their properties (see \cite{MEinAP}, and also Appendix A.4, for further details). We use the results in this section to manipulate certain matrix expressions in Chapter~\ref{sec: conv}.

Let 
\[\bs A = \left[\begin{array}{ccc}a_{11} & \dots & a_{1m}\\\hdots & & \hdots \\ a_{n1}&\dots & a_{nm}\end{array}\right]
\qquad
\bs{B} = \left[\begin{array}{ccc}b_{11} & \dots & b_{1m'}\\\hdots & & \hdots \\ b_{n'1}&\dots & b_{n'm'}\end{array}\right]\]
be matrices with dimensions \(n\times m\) and \(n'\times m'\), respectively. The operator \(\otimes\) is the Kronecker product of two matrices; 
\[\bs A\otimes \bs{B} = \left[\begin{array}{ccc}a_{11}\bs{B} & \dots & a_{1m}\bs{B}\\\hdots & & \hdots \\ a_{n1}\bs{B}&\dots & a_{nm}\bs{B}\end{array}\right],\]
which is an \(nn'\times mm'\) matrix. 

Let \(\bs{C},\bs{D}\)  be matrices with dimensions \(m\times k\) and \(m'\times k'\). A property of the Kronecker Product is 
\begin{align}
	\left(\bs A\otimes \bs{B}\right)\left(\bs{C}\otimes \bs{D}\right) &= \bs A\bs{C}\otimes \bs{B}\bs{D}.\label{eqn:mpr}\tag{Mixed Product Rule}
\end{align}
\begin{proof}
	The proof follows from 
	\begin{align*}
		\left[\begin{array}{cccc}a_{i1}\bs{B} & a_{i2}\bs{B}&\dots&a_{in}\bs{B}\end{array}\right]\left[\begin{array}{c}c_{1j}\bs{D}\\c_{2j}\\\vdots\\c_{nj}\bs{D} \end{array}\right] 
		%
		&= \left(\sum_\ell a_{i\ell}c_{\ell j}\right) \bs{B}\bs{D}
		\\&= \left(\bs A\bs{C}\right)_{ij}\bs{B}\bs{D}.
	\end{align*}
\end{proof}

If \(\bs A\) and \(\bs{B}\) are invertible matrices, then 
\begin{align}\label{eqn:kron inverse}
	\left(\bs A\otimes \bs{B}\right)^{-1} = \bs A^{-1}\otimes \bs{B}^{-1}.
\end{align}

Let \(\bs A\) and \(\bs{B}\) be \(n\times n\) and \(m\times m\) matrices, respectively. The Kronecker sum of \(\bs A\) and \(\bs{B}\) is denoted by \(\oplus\) and defined as 
\[\bs A\oplus \bs{B} := \bs A\otimes \bs{I}_{m} + \bs{I}_{n}\otimes \bs{B}.\]

The exponential of a square matrix \(\bs B\) is \[e^{\bs B} := \sum_{n=0}^\infty \cfrac{1}{n!}\bs B^n.\]

A property of the Kronecker sum is 
\begin{align}\label{eqn:kron exp}
	e^{\bs A\oplus \bs{B}}= e^{\bs A}\otimes e^{\bs{B}}.
\end{align}
\begin{proof}
	First, the matrices \(\bs A\otimes \bs{I}_m\) and \(\bs{I}_n\otimes \bs{B}\) commute; from the mixed product rule their product is \(\bs A\otimes \bs{B}\). Hence, 
	\[e^{\bs A\oplus \bs{B}} = e^{\bs A\otimes \bs{I}_m}e^{\bs{I}_n\otimes \bs{B}}.\]
	We now show that \(e^{\bs A\otimes \bs{I}_m} = e^{\bs A}\otimes \bs{I}_m\) and \(e^{\bs{I}_n\otimes \bs{B}}=\bs{I}_n\otimes e^{\bs{B}}\). The latter follows from the fact that \(\bs{I}_n\otimes \bs{B}\) is a block diagonal matrix with blocks \(\bs{B}\), hence its exponential is also block diagonal with blocks equal to the exponential of \(\bs{B}\). The former follows from 
	\begin{align}
	e^{\bs A\otimes \bs{I}_m} &= \sum_{n=0}^\infty \cfrac{1}{n!}\left(\bs A\otimes \bs{I}_m\right)^n \nonumber
	\\&= \sum_{n=0}^\infty \cfrac{1}{n!}\left(\bs A^n\otimes \bs{I}_m\right) \nonumber
	\\&=\left(\sum_{n=0}^\infty \cfrac{1}{n!}\bs A^n\otimes \bs{I}_m\right) \nonumber
	\\&=e^{\bs A}\otimes \bs{I}_m.\label{eqn:09ksdjgah}
	\end{align}
	
	Therefore 
	\[e^{\bs A\oplus \bs{B}} = \left(e^{\bs A}\otimes \bs{I}_m\right)\left(\bs{I}_n\otimes e^{\bs{B}}\right),\]
	and the result follows by the mixed product rule. 
\end{proof}

\begin{lem}\label{lem: lst mpr}
	Let \(\bs{T}\) and \(\bs{C}\) be \(n\times n\), square matrices with \(\bs{C}\) diagonal and invertible; let \(\bs{S}\) be a \(p\times p\) matrix. Further, suppose \(\left[\bs{T}\otimes \bs{I} + \bs{C}\otimes \bs{S} - \lambda \bs{I}\right]\) is invertible for \(\lambda>0\). Then
\begin{align}
	&\int_{t=0}^\infty e^{-\lambda t}  e^{{\left(\bs{T}\otimes \bs{I} + \bs{C}\otimes \bs{S}\right)t}} \wrt t 
	%
	=   \int_{x=0}^\infty e^{{\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)x}}\otimes e^{\bs{S}x} \wrt x \left(\bs{C}\otimes \bs{I}\right)^{-1}.  \label{eqn:lstsimplify}\end{align}
\end{lem}
\begin{proof}
	Computing the integral on the left-hand side and then factorising the result and using the~\ref{eqn:mpr} multiple times gives
	\begin{align}
            	\int_{t=0}^\infty e^{-\lambda t} e^{\left(\bs{T}\otimes \bs{I} + \bs{C}\otimes \bs{S}\right)t} \wrt t\nonumber 
            	%
            	&= - \left[\bs{T}\otimes \bs{I} + \bs{C}\otimes \bs{S} - \lambda \bs{I}\right]^{-1}
		%
		\\&= -  \left[\bs{T}\otimes \bs{I} + \left(\bs{C}\otimes \bs{I}\right)\left(\bs{I}\otimes \bs{S}\right) - \lambda \bs{I}\right]^{-1}\nonumber
		%
		\\&= -  \left[\left(\bs{C}\otimes \bs{I}\right)\left(\left(\bs{C}\otimes \bs{I}\right)^{-1}\left(\bs{T}\otimes \bs{I} \right)+ \bs{I}\otimes \bs{S} - \left(\bs{C}\otimes \bs{I}\right)^{-1}\lambda \bs{I}\right)\right]^{-1}. \label{eqn: ref this one 12}
		%
	\end{align}
	By Equation~(\ref{eqn:kron inverse}) and since \(\bs{C}\) is invertible, (\ref{eqn: ref this one 12}) is equal to
	\begin{align}
		& - \left[\left(\bs{C}\otimes \bs{I}\right)\left(\left(\bs{C}^{-1}\otimes \bs{I}\right)\left(\bs{T}\otimes \bs{I} \right)+ \bs{I}\otimes \bs{S} - \left(\bs{C}^{-1}\otimes \bs{I}\right)\lambda \bs{I}\right)\right]^{-1}. \label{eqn: ref this one 13}
		%
	\end{align}
	{Using the~\ref{eqn:mpr} and algebraic manipulation, (\ref{eqn: ref this one 13}) is equal to }
	\begin{align}
		&- \left[\left(\bs{C}\otimes \bs{I}\right)\left(\left(\bs{C}^{-1}\bs{T}\right)\otimes \bs{I} + \bs{I}\otimes \bs{S} - \left(\bs{C}^{-1}\lambda \bs{I}\right)\otimes \bs{I}\right)\right]^{-1} \nonumber
		%
		\\&= - \left[\left(\bs{C}\otimes \bs{I}\right)\left(\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)\right)\otimes \bs{I} + \bs{I}\otimes \bs{S}\right)\right]^{-1} \nonumber
		%
		\\&= - \left[\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)\right)\otimes \bs{I} + \bs{I}\otimes \bs{S}\right]^{-1}\left(\bs{C}\otimes \bs{I}\right)^{-1} \nonumber
		\\&= - \left[\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)\right)\oplus \bs{S}\right]^{-1}\left(\bs{C}\otimes \bs{I}\right)^{-1},\label{eqn: is an integral}
	\end{align}
	by definition of the Kronecker sum.
	
	Now, for an invertible matrix \(\bs A\) we can write \(-\bs A^{-1} = \displaystyle\int_{x=0}^\infty e^{\bs Ax}\wrt x\). Therefore, (\ref{eqn: is an integral}) is 
	\begin{align*}
		-\left[\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)\right)\oplus \bs{S}\right]^{-1}\left(\bs{C}\otimes \bs{I}\right)^{-1}
		&= \int_{x=0}^\infty e^{\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)x\right)\oplus \bs{S}x}\wrt x\left(\bs{C}\otimes \bs{I}\right)^{-1}.
	\end{align*}
	{Using the rule in Equation~(\ref{eqn:kron exp}) gives }
	\begin{align*}
		&\int_{x=0}^\infty e^{\left(\bs{C}^{-1}\left(\bs{T}-\lambda \bs{I}\right)\right)x}\otimes e^{ \bs{S}x}\wrt x\left(\bs{C}\otimes \bs{I}\right)^{-1},
	\end{align*}
	which is the result.
\end{proof}

% vector space? Polynomial basis?
% me basis? semigroup? generator? 
\section{Laplace Transforms}
We have already mentioned \emph{Laplace transforms}, but we have not-yet defined them. We define them now. In Chapters~\ref{sec: conv} and~\ref{ch: global results} we work with {Laplace transforms} to prove convergence of the QBD-RAP scheme. For a measure \( \mu\), defined on the Borel sets of \([0,\infty)\), we define the Laplace transform of \(\mu\) to be
\[\widehat \mu(\lambda) = \mathcal L(\mu)(\lambda) = \int_{t=0}^\infty e^{-\lambda t}\wrt \mu,\]
where the \emph{region of convergence} is the set of values of \(\lambda\in\mathbb R\) such that the integral is finite. When \(\mu\) has a density, \(v\), then the Laplace transform is 
\[\widehat \mu(\lambda) = \widehat v(\lambda) = \mathcal L(v)(\lambda) = \int_{t=0}^\infty e^{-\lambda t}v(x)\wrt x.\]
When \(\mu\) is the probability measure associated with a random variable, \(Z\), say, then we may write 
\[\widehat \mu(\lambda)=\mathbb E[e^{-\lambda Z}],\]
and the region of convergence is at least \([0,\infty)\). 
Further, letting \(E^\lambda\) be an exponentially distributed random variable with rate \(\lambda\) and noting that \(\mathbb P(E^\lambda >t ) = e^{-\lambda t}\), then 
\[\widehat \mu(\lambda)=\mathbb P(Z<E^\lambda),\]
which gives a probabilistic interpretation of the Laplace transform. That is, the Laplace transform with parameter \(\lambda >0\) is the probability that \(Z\) occurs before \(E^\lambda\), an independent random exponential time with rate \(\lambda\), occurs. 

A convenient property of the Laplace transform which we utilise is the Convolution Theorem. 
\begin{thm}[Convolution Theorem]
	Let \(f,g: [0,\infty) \to \mathbb R\) be integrable functions, then 
	\[\mathcal L\left(\int_{u=0}^tf(u)g(t-u)\wrt u\right)(\lambda) = \mathcal L\left(f\right)\cdot \mathcal L\left(g\right).\]
\end{thm}
The Convolution Theorem states that the Laplace transform of the convolution, given by \(\displaystyle \int_{u=0}^tf(u)g(t-u)\wrt u\), is equal to the product of the Laplace transform of \(f\) and \(g\). 

The Laplace transform is unique in the sense that, if \(\mu\) and \(\nu\) are two measures on the Borel sets of \([0,\infty)\) and 
\[\widehat \mu(\lambda) = \widehat \nu(\lambda)\] 
for all \(\lambda > a\) with \(a<\infty\), then \(\mu\) and \(\nu\) are the same. In terms of functions, \(f,g: [0,\infty) \to \mathbb R\), if 
\[\mathcal L(f)(\lambda) = \mathcal L(g)(\lambda),\]
for all \(\lambda > a\) with \(a<\infty\), and \(f\) and \(g\) are continuous, then \(f(t)=g(t)\) for all \(t\geq 0\). Without knowing \(f\) and \(g\) are continuous, then we can only claim that 
\(f(t)=g(t)\) for all \(t\geq 0, t\notin \mathcal N,\) where \(\mathcal N\) is a \emph{null set} with respect to Lebesgue measure. 

%Laplace transforms of probability measures with non-negative support and where the Laplace transform variable, \(\lambda\), is real and non-negative can be given a stochastic interpretation. Let \(W\) be a random variable with distribution function \(F_W(w)= \mathbb P(W<w)\), then \(\displaystyle\int_{w=0}^\infty e^{-\lambda w} \wrt F_W(w) = \mathbb P(W < E^{\lambda})\). That is, the Laplace transform with parameter \(\lambda >0\) is the probability that \(W\) occurs before \(E^\lambda\), an independent random exponential time with rate \(\lambda\), occurs. 

\section{Convergence theorems}
We use the following convergence theorems to help us prove that the QBD-RAP scheme converges weakly to the distribution of the fluid queue. The first result we state, the Portmanteau Theorem, is a sweeping statement about convergence of measures. First, let's define the notion of weak convergence. We follow \cite{billingsleyconvergence}.

Let \(S\) be a metric space and let \(\mathcal S\) be the Borel \(\sigma\)-algebra generated by the open sub-sets of \(S\). A probability measure, \(P\), is a function which maps elements of \(\mathcal S\) (elements of \(\mathcal S\) are sets) to real numbers in the interval \([0,1]\), with \(P(S)=1\). Further, \(P\) is \emph{countably-additive}, which means that for any countable collection of \emph{disjoint} sets \(A_1, A_2,...\in\mathcal S\), then 
\[P\left(\bigcup\limits_{n=1}^\infty A_n\right) = \sum\limits_{n=1}^\infty P\left(A_n\right).\]
Define the notation \(Pf = \int_S f \wrt P\) where \(f:S\to \mathbb R\) is a function. Let \(P_1,P_2,...\), be a sequence of probability measures. For a given function \(f\), the sequence \(P_1f,P_2f,...\), is simply a sequence of real numbers. The sequence of probability measures \(P_1,P_2,....\), is said to \emph{converge weakly} to \(P\) if \(P_nf\to Pf\) for every bounded continuous real function \(f\). \cite{billingsleyconvergence} uses the notation \(P_n\Rightarrow P\) to denote this weak convergence. 

We need a few more definitions before stating The Portmanteau Theorem. A set \(A\in\mathcal S\) is said to be a \(P\)-\emph{continuity set} if \(\partial A\) (the boundary of the set \(A\)) satisfies \(P(\partial A)=0\). Define the \emph{limit inferior} and \emph{limit superior} of a sequence of real numbers \(x_1,x_2,...,\) as
\begin{align*}
	\liminf_{n\to\infty} x_n &:= \lim_{n\to \infty}	\left(\inf_{m\geq n}x_m\right)
	\intertext{and}
	\\\limsup_{n\to\infty} x_n &:= \lim_{n\to \infty}	\left(\sup_{m\geq n}x_m\right),
\end{align*}
respectively. 

\begin{thm}[Portmanteau Theorem I, Theorem~2.1 of \cite{billingsleyconvergence}]
	These five conditions are equivalent.
	\begin{enumerate}
		\item[(i)] \(P_n\Rightarrow P\). 
		\item[(ii)] \(P_nf\to Pf\) for all bounded, uniformly continuous \(f\).
		\item[(iii)] \(\limsup_n P_n F \leq P F\) for all closed [sets] \(F\).
		\item[(iv)] \(\liminf_n P_n G \leq P G\) for all open [sets] \(G\).
		\item[(v)] \(P_n A \to P A\) for all \(P\)-continuity sets \(F\).
	\end{enumerate}
\end{thm}
Some authors, such as \cite{portmanteaubook}, also include the following equivalent condition in the Portmanteau Theorem. 
\begin{thm}[Portmanteau Theorem II, Theorem~13.16 of \cite{portmanteaubook}]\label{thm: Portmanteau}
	\(\,\)
	\begin{enumerate}
		\item[(vi)] \(P_nf \to P f\) for all bounded, Lipschitz continuous \(f\).
	\end{enumerate}
\end{thm}

Recall that a stochastic process is a sequence of random variables \(\{X(t)\}_{t\in\mathcal T}\). The \emph{finite-dimensional distributions} of \(\{X(t)\}_{t\in\mathcal T}\) are the joint distributions of the random vector \((X(t_1),X(t_2),...,X(t_n))\), where \(t_1,t_2,...,t_n\in \mathcal T\) is a finite collection of times. 

Key concepts in establishing weak convergence of stochastic processes are the notions \emph{tight} and \emph{relatively compact}. Again, we follow \cite{billingsleyconvergence}. A probability measure \(P\) is said to be \emph{tight} if for each \(\varepsilon\) there exists a compact set \(K\) such that \(P(K)>1-\varepsilon\). A family of probability measures, \(\Pi\), is said to be tight if for every \(\varepsilon\) there exists a compact set \(K\) such that \(P(K)>1-\varepsilon\) for every probability measure \(P\) in \(\Pi\). The family \(\Pi\) is \emph{relatively compact} if every sequence of elements of \(\Pi\) contains a subsequence which converges weakly. 

Let \(P_1,P_2,...,\) and \(P\) be probability measures of stochastic processes. \cite{billingsleyconvergence} provides the following result.
\begin{thm}\label{thm: aofaa}
	If \(\{P_n\}\) is relatively compact and the finite-dimensional distributions of \(P_n\) converge weakly to those of \(P\), then \(P_n\Rightarrow_n P\). 
\end{thm}
Further, the condition that \(\{P_n\}\) is relatively compact can be replaced by tightness due Prohorov's Theorem \citep{billingsleyconvergence}. 
\begin{thm}
	If \(\Pi\) is tight, then it is relatively compact.
\end{thm}


% Denote by \(\mathcal M_f(E)\) the set of all finite measures on \((E,\mathcal E)\), where \(E\) is a non-empty set and \(\mathcal E\) is a \(\sigma\)-algebra. Further, denote by \(C_b(E)\) the set of continuous bounded functions on \(E\). Let \(\mu,\,\mu_1,\,\mu_2,...\in \mathcal M_f(E)\). We say that \(\{\mu_n\}_{n\in\mathbb N}\) converges weakly to \(\mu\), formally \(\mu_n\to \mu\) weakly as \(n\to\infty\), if 
% \[\int f \wrt \mu_n \to \int f\wrt \mu,\mbox{ for all }f \in C_b(E).\]
% We use part of The Portmanteau Theorem in Chapter~\ref{ch: global results}. Let \(\mathcal M_{\leq 1}(E):=\{\mu \in \mathcal M_f(E)\mid \mu(E)\leq 1\},\) the set of all sub-probability measures on \((E,\mathcal E)\). 
% \begin{thm}[(Part of) The Portmanteau Theorem, Theorem~13.16 of \cite{portmanteaubook}]
% 	Let \(E\) be a metric space and let \(\mu,\, \mu_1,\,\mu_2,... \in \mathcal M_{\leq 1}(E)\). The following are equivalent.
% 	\begin{enumerate}
% 		\item[(i)] \(\mu_n\to \mu\) weakly as \(n\to \infty\). 
% 		\item[(ii)] \(\int f\wrt \mu_n \to \int f\wrt \mu\) for all bounded, Lipschitz continuous \(f\).
% 		\\ \(\vdots\) 
% 	\end{enumerate}
% \end{thm}
% There are 8 parts to The Portmanteau Theorem in \cite{portmanteaubook}, Theorem~13.16. Here we only quote to relevant parts; the other 6 parts require us to define other concepts which are not relevant to this thesis, so they are omitted. See \cite{portmanteaubook}, Theorem~13.16 for details. 

Another tool we can use to show convergence of measures is to show that the Laplace transforms converge, as stated in the following theorem.
\begin{thm}[Extended Continuity Theorem, \cite{feller1957}, Theorem 2a]\label{thm: ext cont thm}
	For \(p=1,2,...,\) let \(U_p\) be a measure with Laplace transform \(\zeta_p\). If \(\zeta_p(\lambda)\to\zeta(\lambda)\) for \(\lambda > a\geq 0\), then \(\zeta\) is the Laplace transform of a measure \(U\) and \(U_p\to U\) [weakly].
	
	Conversely, if \(U_p\to U\)[weakly] and the sequence \(\{\zeta_p(a)\}\) is bounded, then \(\zeta_p(\lambda)\to\zeta(\lambda)\) for \(\lambda >a\). 
\end{thm}

In Chapters~\ref{sec: conv} and~\ref{ch: global results} we use the Dominated Convergence Theorem to aid our convergence arguments. In applied probability we often want to prove convergence of certain expressions. An approach which can simplify matters is to partition the expression on certain events where we have a simpler characterisation, thereby enabling us to prove convergence on each element in the partition. The original expression can be written as an integral over the partition. To establish the convergence result we initially desired, we can use the convergence of each element of the partition and the Dominated Convergence Theorem. I have taken the following from Theorem~1.13~\cite{steinreal}. \cite{steinreal} use the notation 
\[\int f = \int f \wrt x = \int f \wrt m(x),\]
where \(m\) denotes Lebesgue measure, to denote the Lebesgue integral. 
\begin{thm}[Dominated Convergence Theorem]
	Suppose \(\{f_n\}\) is a sequence of measurable functions such that \(f_n(x)\to f(x)\) almost everywhere with respect to \(x\), as \(n\) tends to infinity. If \(|f_n(x)|\leq g(x)\), where \(g\) is integrable, then 
	\[\int|f_n-f|\to 0 \mbox{ as } n \to \infty,\]
	and consequently 
	\[\int f_n\to\int f \mbox{ as } n\to \infty.\]
\end{thm}

Also in Chapters~\ref{sec: conv} and~\ref{ch: global results} we want to manipulate infinite sums or integrals and rearrange the order of summation or integration. However, things can go awry when we swap the order of integration/summation if we are not careful. The next few results give some conditions under which we have equality under before and after swapping the order of summation/integration. Once again, we follow \cite{steinreal} quoting their Theorem~2.13. If \(f\) is a function in \(\mathbb R^{d}=\mathbb R^{d_1}\times \mathbb R^{d_2}\), the \emph{slice} of \(f\) corresponding to \(y\in\mathbb R^{d_2}\) is the function \(f^y\) of the \(x\in\mathbb R^{d_1}\) variable, given by 
\[f^y(x)=f(x,y).\]
Similarly, the slice of \(f\) for a fixed \(x\in\mathbb R^{d_1}\) is \(f_x(y)=f(x,y)\). 
\begin{thm}[Fubini's Theorem]\label{Fubini}
	Suppose \(f(x,y)\) is integrable on \(\mathbb R^{d_1}\times \mathbb R^{d_2}\). Then for almost every \(y\in\mathbb R^{d_2}\):
	\begin{itemize}
		\item The slice \(f^y\) is integrable on \(\mathbb R^{d_1}\).
		\item The function defined by \(\int_{R^{d_1}} f^y(x)\wrt x\) is integrable on \(\mathbb R^{d_2}\). 
	\end{itemize}
	Moreover:
	\begin{itemize}
		\item[(iii)] \(\displaystyle\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y = \int_{\mathbb R^d}f.\)
	\end{itemize}
\end{thm}
\cite{steinreal} then state \begin{quotation}``Clearly, the [Fubini] theorem is symmetric in \(x\) and \(y\) so that we also may conclude that the slice \(f_x\) is integrable on \(\mathbb R^{d_2}\) for almost every \(x\). Moreover, \(\int_{\mathbb R^{d_2}}f_x(y)\wrt y\) is integrable and 
\[\displaystyle\int_{R^{d_1}}\left(\int_{R^{d_2}}f(x,y)\wrt y\right)\wrt x = \int_{\mathbb R^d}f.\]
In particular, Fubini's theorem states that the integral of \(f\) on \(\mathbb R^d\) can be computed by iterating lower-dimensional integrals, and that the iterations can be taken in any order
\[\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y=\displaystyle\int_{R^{d_1}}\left(\int_{R^{d_2}}f(x,y)\wrt y\right)\wrt x = \int_{\mathbb R^d}f.\mbox{''}\]
\end{quotation}
It is this last statement which is most powerful. Effectively, if either
\[\int_{R^{d_2}}\left(\int_{R^{d_1}}|f(x,y)|\wrt x\right)\wrt y<\infty,\]
or
\[\int_{R^{d_1}}\left(\int_{R^{d_2}}|f(x,y)|\wrt y\right)\wrt x<\infty,\]
then we can swap the order of integration. 

A closely related theorem which is often used alongside Fubini's Theorem is Tonelli's Theorem. Define the \emph{extended Lebesgue integral} of an extended valued (it can take the value \(+\infty\)) non-negative function \(f\) by 
\[\int f(x)\wrt x = \sup_{g}\int g(x)\wrt x.\]
\begin{thm}[Tonelli's Theorem]\label{Tonelli}
	Suppose \(f(x,y)\) is a non-negative measurable function on \(\mathbb R^{d_1}\times \mathbb R^{d_2}\). Then for almost every \(y\in\mathbb R^{d_2}\):
	\begin{itemize}
		\item The slice \(f^y\) is integrable on \(\mathbb R^{d_1}\).
		\item The function defined by \(\int_{R^{d_1}} f^y(x)\wrt x\) is integrable on \(\mathbb R^{d_2}\). 
	\end{itemize}
	Moreover:
	\begin{itemize}
		\item[(iii)] \(\displaystyle\int_{R^{d_2}}\left(\int_{R^{d_1}}f(x,y)\wrt x\right)\wrt y = \int_{\mathbb R^d}f(x,y)\wrt x\wrt y \mbox{ in the extended sense}.\)
	\end{itemize}
\end{thm}

Once again, we note that the theorem is symmetric in \(x\) and \(y\), so we can establish that we may swap the order of integration provided that \(f\) is non-negative. Tonelli's Theorem allows us to exchange the order of integration for any non-negative extended real-valued function, and includes the case where the value of the integral is infinite. In contrast, Fubini's Theorem us to exchange the order of integration for any real-valued function, provided that the integral is finite. 

Collectively, we refer to Theorems~\ref{Fubini} and~\ref{Tonelli} together as the Fubini-Tonelli Theorem, but they are otherwise known collectively as just Fubini's Theorem. Often, they are used in conjunction. Since \(|f|\) is a non-negative function, then we may use Tonelli's Theorem and compute (or bound) the integral \(\int |f|\) via computing an iterated integral. If this is found to be finite, then Fubini's Theorem applies so \(f\) is integrable, and we may evaluate \(\int f\) via an iterated integral. 

In the context of probability, the function \(f\) which we are integrating is often positive, so Tonelli's Theorem is all that is required to justify a swap of iterated integrals. 

\section{Sundry mathematical concepts}\label{eqn: la}
\subsection*{Polynomials}
The discontinuous Galerkin method involves projecting the operator equation onto a basis of functions, typically polynomials. A convenient basis with which to work is the interpolating Lagrange polynomials. The order \(p-1\) Lagrange polynomials are defined by a set of points, \(\xi_i, i=1,...,p\), (the \(\xi_i\)'s must be distinct), and are given by 
\[\ell_i(r)=\prod_{\substack{j=1\\j\neq i}}^p \cfrac{r-\xi_j}{\xi_i-\xi_j},\, i=1,...,p.\]
A convenient property of the Lagrange polynomials is 
\[l_i(\xi_j)=\begin{cases}1 & \mbox{ if } i=j, \\ 0 & \mbox{ otherwise.} \end{cases}\]

Sometimes it is more convenient to work with \emph{orthogonal} polynomials. For example, the Legendre polynomials, which can be defined recursively by 
\begin{align*}
	P_0(x)&=1,
	\\P_1(x)&=x, 
	\\(n+1)P_{n+1}(x)&=(2n+1)xP_n(x)-nP_{n-1}(x),
\end{align*}
and are orthogonal on \([-1,1]\). 

The zeros of \((1-x^2)P_n'(x)\) define the \emph{Legendre-Gauss-Lobatto} points, which are used, among other things, for numerically approximating integrals via quadrature.

\subsection*{Numerical integration}
To numerically approximate integrals in the discontinuous Galerkin schemes in this thesis we can use Gauss-Lobatto quadrature. Consider the integral of some function \(f\) over the interval \([-1,1]\). Quadrature approximates the integral by evaluating the function on a set of points, \(x_i,\, i=1,...,p\), and computing the weighted sum
\[\int_{-1}^1f(x)\wrt x \approx \sum_{i=1}^p f(x_i)w_i,\]
where \(w_i\) are weights. There are various quadrature schemes which one can use. 

For the discontinuous Galerkin method, Gauss-Lobatto quadrature is convenient as in both we evaluate the function at the end points of the interval. For Gauss-Lobatto quadrature, the nodes, \(x_i, i=1,...,p\) is the zero of \((1-x^2)P_n'(x)\) where \(P_n\) is the \(n\)th Legendre polynomial, the weights are given by 
\begin{align*}
	w_1&=1,
	\\w_i&=\cfrac{2}{n(n-1)[P_{n-1}(x_i)]^2}, \, i\neq 1, p,
	\\w_p&=1.
\end{align*}
Gauss-Lobatto quadrature is accurate for polynomials up to degree \(2p-3\). An integral over the interval \([a,b]\) can be approximated by 
\[\int_{a}^bf(x)\wrt x \approx \cfrac{b-a}{2}\sum_{i=1}^p f\left(\cfrac{b-a}{2}x_i+\cfrac{a+b}{2}\right)w_i.\]

We also use a trapezoidal integration rule at times. Consider the integral \(\int_{a}^bf(x)\wrt x\). A trapezoidal rule with \(p\) evenly spaced grid-points approximates the integral via 
\[\int_{a}^bf(x)\wrt x \approx \sum_{i=2}^p \cfrac{f(x_{i-1}+f(x_i))}{2}\Delta,\]
where \(x_i=a+(i-1)\Delta\), \(i=1,...,p\), and \(\Delta = \cfrac{b-a}{p-1}.\) 

\subsection*{Measuring the difference between distributions}
In the numerical investigations in this thesis we evaluate approximation schemes by comparing the resulting approximations they produce with a ground truth. For comparing distributions we use \(L^p\) \emph{norms} and the Kolmogorov-Smirnov distance. The \(L^p\) norm of a function \(f\) is given by 
\[\left(\int |f(x)|^p\wrt x\right)^{1/p}.\]
The Kolmogorov-Smirnov distance between two distribution functions, \(F_1\), \(F_2\) is 
\[\sup_{x}|F_1(x)-F_2(x)|.\]

\section{More on relevant literature \& the context of the thesis} 
In the process of introducing the technical concepts we have already covered some the relevant literature. Here, we provide some more context for the contributions of this thesis. The focus of this thesis is on developing numerical approximations of fluid queues such that we may approximate the operator-analytic analysis of fluid-fluid queues form \cite{bo2014}. %The analysis of fluid-fluid queues in \cite{bo2014} relies on a certain transient analysis of the driving process, which is a fluid queue.

\paragraph{Fluid-fluid queues and related models} Related to the analysis of fluid-fluid queues are the works of \cite{mz2012,lnp13,bo2013b} and \cite{bop2020}. \cite{mz2012} analyse related discrete-time multidimensional Markov additive processes and derive operator-analytic expressions for the limiting distribution. Although markedly different in their approach \cite{mz2012}, like  \cite{bo2014}, is inspired by a matrix-analytic approach. The work of \cite{lnp13} considers a fluid-fluid queue where the driving process \(\{(X(t),\varphi(t))\}\) is level-dependent, and derives computable expressions for the marginal probability distribution of the first level, and bounds for that of the second level. Their analysis is somewhat specific to the data handling model considered and only arrives at bounds for the marginal limiting distribution of the second fluid level. In contrast, the work of \cite{bo2014} treats more general models and derives operator-analytic expressions for the joint limiting distribution. Therefore, compared to \cite{lnp13}, the methods of this thesis apply to a more general class of models and enable to approximation of the joint limiting distribution. However, unlike \cite{lnp13}, the analysis of \cite{bo2014} does not consider a level-dependent driving process. %but I believe that it would be relatively straightforward to extend their results (and the results of this thesis) to the level-dependent case where the rates \(c_i\) depend on \(X(t)\) in a piecewise-constant way. 
\cite{bo2013b} analyse a model which is simple special case of the fluid-fluid queue. They consider a model where the second level process \(\{\dot Y(t)\}\) has constant rates which do not depend on \(\ddot X(t)\) (but do depend on \(\varphi(t)\)), while \(\{(\ddot X(t),\varphi(t))\}\) is an \emph{unbounded} fluid queue. They derive matrix-analytic expressions for certain Laplace transforms, with respect to the position of the first fluid \(\{X(t)\}\) relative to its starting position \(X(0)\), of the probability of certain transient events (such as first return and draining/filling times) of \(\{Y(t)\}\). %In contrast, \cite{bo2014} treats a more general model. 
The work of \cite{bop2020} treats a similar model to that of \cite{bo2013b}, except that the driving process has a regulated lower boundary at \(0\); this significantly complicates the analysis. They derive matrix-analytic expressions for the first return operator of the second fluid given a specific exponential form of the initial distribution and a certain boundary condition is met. %Once again, \cite{bo2013b} together with this thesis, treat a more general model and together, give computational methods for various performance measures of fluid-fluid queues.

Returning now to \cite{bo2014} which treats the most general form of the fluid-fluid queue. Their analysis is inspired by the analysis of classical fluid queues in \cite{bean2005} whereby matrix-analytic expressions for the fluid level are derived in terms of the generator of the driving process. The driving process of a fluid-fluid queue is a fluid queue. The analysis of \cite{bo2014} requires specific expressions about the transient distribution of the fluid queue which cannot, in general, be obtained from the existing literature.

\paragraph{Transient analysis of fluid queues} The analysis of fluid queues can be classified broadly into three approaches, matrix-analytic methods (for example, \cite{ajr2005,ar2003,ar2004,bean2005b,bean2005,bot08,bean2009,dasilva2005,latouche2018}), differential equations based approaches (such as \cite{anick1982,kk1995,blnos2022}) and analyses based on recurrence relations (for example, \cite{sericola1998,sericola1999,sericola2001}). Often, it is the limiting distribution of the fluid queue which is of interest. In the context of the analysis of fluid-fluid queues as in \cite{bo2014}, however, we require information about the transient distribution of the fluid queue. Specifically, on the event that the second fluid is non-decreasing (non-increasing), we need to know the amount of fluid to have flowed into or out of the second level process, the time (or Laplace transform of time) taken to do so, and the position of the first fluid and phase at the time when the in-out process of the second fluid reaches a given height. Moreover, we want the resulting expressions to be readily computable. None of the existing literature gives exact expressions which have all the aforementioned properties. Even if we consider a simpler model where the rates \(r_i(x)\) are constant on intervals \(\calD_{k}\), then the existing results are not satisfactory. Perhaps the closest is the work of \cite{bean2009} who compute expressions for the Laplace transform with respect to the time take for the fluid to exit an interval, \(\calD_{k}\), say, on the even that fluid exits in some phase \(j\), given it started within the interval at some point \(x_0\) and in some phase \(i\). These expressions could perhaps be used to analyse a fluid-fluid queue where the rates \(r_i(x)=r_k\) for all \(i\in\calS\), and \(x\in\calD_{k,i}\). Even with this simplified fluid-fluid queue, it is unclear how we could use these expressions to compute the position of the second fluid. 

Approximation methods which are appropriate for the application to fluid-fluid queues have been proposed. The uniformisation method of \cite{bo2013} approximates a fluid queue by a discrete-time Markov chain. \cite{bo2013} approximate the fluid queue \(\{(\dot X(t),\varphi(t))\}\) by the quasi-birth-and-death-process \(\{(L(t),\varphi(t))\}\). In both processes, \(\{\varphi(t)\}\) is the same so there is no approximation in the phase process. The level process \(\{L(t)\}\) approximates the level process \(\{\dot X(t)\}\). Specifically, \cite{bo2013} derive the level process such that the event that \(L(t)=k\) and \(\varphi(t)=i\) approximates the event \(\dot X(t)\in \calD_{k,i}\) and \(\varphi(t)=i\), where all the levels \(\{\calD_{k,i}\}_{k}\) have the same width, \(\Delta=y_{k+1}-y_k\). By considering arbitrarily small \(\Delta\), they prove that the distribution of the quasi-birth-and-death-process \(\{(L(t),\varphi(t))\}\) can be made arbitrarily close to the distribution of the fluid queue \(\{(\dot X(t),\varphi(t))\}\). Replacing the driving process of the fluid-fluid queue, \(\{(\dot X(t),\varphi(t))\}\), by the approximation \(\{(L(t),\varphi(t))\}\) and approximating the rate functions \(r_i(x)\) by constants on each interval \(\calD_k\) yields a classical fluid queue which, intuitively, approximates the fluid-fluid queue. \cite{bo2013} numerically investigate the ability of their approximation to approximate \(\bs \Psi_X\). To date, the uniformisation approximation has not been applied in the context of fluid-fluid queues, and its ability to approximate transient quantities of fluid queues (other than the aforementioned investigation of \(\bs\Psi_X\)) has not been well studied. In Chapter~\ref{sec: numerics} we investigate the numerical performance of the uniformisation approximation and show that it is effective. Of the methods considered in Chapter~\ref{sec: numerics}, the performance of the uniformisation approximation is the poorest and its rate of convergence is the slowest for the numerical experiments we conduct. %However, due to the stochastic interpretation of the uniformisation approximation as a QBD, approximations to probabilities are guaranteed to be non-negative, and we can employ probabilistic techniques to analyse the approximation (as \cite{bo2013} did). Moreover, the simplicity of the uniformisation scheme lends itself more easily to mathematical analysis. For example, when the uniformisation scheme is used in the context of approximating a fluid-fluid queue the resultant approximation is itself a fluid queue, and this stochastic interpretation could aid in the analysis of this approximation. 

As mentioned earlier, the uniformisation approximation of \cite{bo2013} is equivalent to a certain application of the {discontinuous Galerkin (DG) method}. DG approximation schemes have found great success for approximating solutions of partial differential equations, however, to my knowledge, they have not been applied to fluid queues, or fluid-fluid queue to date (except for our paper \cite{blnos2022}, of which I am a co-author and the relevant parts of that work are included in this thesis). %The DG scheme has the advantage that its rate of convergence is rapid for smooth problems, however, it can produce oscillatory approximations and result in negative probabilities.

%In Chapter~\ref{sec: construction and modelling} we attempt to derive a new approximation scheme which converges faster than the uniformisation scheme and still has a stochastic interpretation so that estimates of probability are guaranteed to be positive.


% Even if we assume \(r_i(x)\) is piecewise constant on intervals \(\calD_k\), say, then we still need expressions for the amount of time spent in each phase while the fluid level remains in \(\calD_k\). Analyses, such as \cite{bean2009}, derive expressions for the time spent in the intervals \(\calD_k\), but do not track how much time is spent in each phase. Moreover, 



% For example, Ramaswami CITE, analysed fluid queues by mapping them to a quasi-birth-and-death process (QBD), after which they applied known matrix-analytic methods for QBDs to compute quantities of interest. Anick Mitra Sondhi CITE, analysed fluid queues using a more direct differential equation-based method. Since Ramaswami's CITE initial work, there has been significant developments in the analysis of fluid queues CITE and related algorithms CITE. 

% The analysis of fluid-fluid queues of \cite{bo2014} is inspired by the matrix-analytic approach to fluid queues. In particular, the approach taken by Bean CITE THEIR FIRST HITTING TIME PAPER AND LATOUCHE AND DA SILVA SOARES. The approach requires 

% ...